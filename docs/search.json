[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matteo Ventura",
    "section": "",
    "text": "Researcher in Statistics\n\nUniveristy of Brescia"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Application of Gaussian Graphical Models for investigating the structure of soil arthropods communities, and for studying the impact of various factors on the edaphic biodiversity of agroecosystems, in order to provide useful tools to support the decisions in the management and protection of biodiversity.\n\n\n\nImprovement and application of models within the so-called CUB (Combination of discrete Uniform and shifted Binomial random variables) class of Models, which allows for the measurement of two latent traits: feeling and uncertainty. These models have broad applications, including in the fields of measuring visitor experience, sports analytics, and sensory analysis. \n\n\n\n\n2024 PhD in Analytics for Economics and Management\n\nThesis title: Advances in Mixture Models for Ordinal Data: Theoretical Insights and Model-based Clustering\n\n\nUniversity of Brescia, Italy\n\n\nSupervisor: Prof. Paola Zuccolotto\n\n\nCo-supervisor: Prof. Julien Jacuqes\n\n\n\n\n2024 Visiting PhD student at the ERIC Laboratory\n\nUniversity Lumière Lyon 2, France\n\n\nSupervisor: Prof. Julien Jacques"
  },
  {
    "objectID": "Favorites.html",
    "href": "Favorites.html",
    "title": "Publications",
    "section": "",
    "text": "Lasagna\nPizza\nPasta"
  },
  {
    "objectID": "Favorites.html#favorite-food",
    "href": "Favorites.html#favorite-food",
    "title": "Publications",
    "section": "",
    "text": "Lasagna\nPizza\nPasta"
  },
  {
    "objectID": "Favorites.html#favorite-subjects",
    "href": "Favorites.html#favorite-subjects",
    "title": "Publications",
    "section": "Favorite subjects",
    "text": "Favorite subjects\n\nStatistics\nMatematics\nMachine learning"
  },
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Matteo Ventura",
    "section": "",
    "text": "Researcher in Statistics\n\nUniveristy of Brescia"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Published Articles\n\nSimonetto A., Ventura M. (2025) Distance Measures for Unweighted Undirected Networks: a comparison study. Australian and New Zealand Journal of Statistics\nVentura, M., Macis, A., Manisera, M., & Zuccolotto, P. (2024). On the equivalence of two mixture models for rating data. Advances in Statistical Analysis, 1(25), 70–86.\nManisera, M., Migliorati, M., Ventura, M., & Zuccolotto, P. (2024). A mixture model for the analysis of categorical variables measured on five-point semantic differential scales. Austrian Journal of Statistics, 53(3), 70–86.\nVentura, M., & Ricciardi, R. (2022). Exploring financial microblogs: Analysis of users’ trading profiles with multivariate statistical methods. Statistica e Applicazioni, 20(1), 61–78.\nGhiglieno, I., Simonetto, A., Sperandio, G., Ventura, M., Gatti, F., Donna, P., Tonni, M., Valenti, L., & Gilioli, G. (2021). Impact of environmental conditions and management on soil arthropods communities in vineyard ecosystems. Sustainability, 13(21), 11999.\n\n\n\nConference Proceedings\n\nVentura, M., Jacques, J., & Zuccolotto, P. (2025). Clustering multivariate rating data within the CUB framework. In A. Pollice & P. Mariani (Eds.), IES 2025 - Statistical Methods for Evaluation and Quality. (pp. 637-642). Cleup sc. ISBN: 9788854958494.\nVentura, M., Jacques, J., & Zuccolotto, P. (2025). Clustering multivariate rating data within the CUB framework. In A. Pollice & P. Mariani (Eds.), 52nd Scientific Meeting of the Italian Statistical Society – Bari, 17-20 June 2024. (pp. 637-642). Springer. ISBN: 9783031644467.\nSimonetto, A., Ventura, M., & Gilioli, G. (2022). An explorative analysis of different distance metrics to compare unweighted undirected networks. In A. Balzanella, M. Bini, C. Cavicchia, & R. Verde (Eds.), 51st Scientific Meeting of the Italian Statistical Society – Caserta, 22-24 June 2022 (pp. 1522-1527). Pearson. ISBN: 9788891932310.\nSimonetto, A., Ventura, M., Ghiglieno, I., Gatti, F., & Gilioli, G. (2021). Le risposte della biodiversità edafica alla gestione degli agroecosistemi: un approccio basato sull’ecological network anlaysis. In A. Elia & G. Conversa (Eds.), Agriculture, Environment and Health XIII National Congress on Biodiversity – Foggia, 7-9 September 2021 (p. 236). DAFNE, University of Foggia. ISBN: 9788874271016.\nGhiglieno, I., Simonetto, A., Sperandio, G., Ventura, M., Gatti, F., Donna, P., Tonni, M., Valenti, L., & Gilioli, G. (2021). Impatto delle condizioni ambientali e della gestione del suolo sugli artropodi edafici dell’ecosistema vigneto. In A. Elia & G. Conversa (Eds.), Agriculture, Environment and Health XIII National Congress on Biodiversity – Foggia, 7-9 September 2021 (p. 123). DAFNE, University of Foggia. ISBN: 9788874271016.\n\n\n\nSubmitted Articles\n\nVentura M., Jacques J., Zuccolotto P., Model-based Clustering of Multivariate Rating Data accounting for Feeling and Uncertainty\nVentura M., Macis A., Manisera M., Zuccolotto P., Measuring Synesthetic Experience of Museum Visitors using Multi-point Semantic Differential Scales\nCarloni E., Codini A. P., Ventura M., Does country of origin extend to sustainable products? The case of sustainable Made in Italy"
  },
  {
    "objectID": "publications.html#favorite-food",
    "href": "publications.html#favorite-food",
    "title": "Publications",
    "section": "",
    "text": "Lasagna\nPizza\nPasta"
  },
  {
    "objectID": "publications.html#favorite-subjects",
    "href": "publications.html#favorite-subjects",
    "title": "Publications",
    "section": "Favorite subjects",
    "text": "Favorite subjects\n\nStatistics\nMatematics\nMachine learning"
  },
  {
    "objectID": "publications.html#published-papers",
    "href": "publications.html#published-papers",
    "title": "Publications",
    "section": "Published papers",
    "text": "Published papers\n\nManisera M., Migliorati M., Ventura M., Zuccolotto P. (2024)  A Mixture Model for the Analysis of Categorical Variables Measured on Five-point Semantic Differential Scales  Austrian Journal of Statistics. 53(3), 70–86\nVentura M., Ricciardi R. (2022)  Exploring Financial Microblogs: Analysis of Users’ Trading Profiles with Multivariate Statistical Methods Statistica e Applicazioni. 20(1), 61–78\nGhiglieno I., Simonetto A., Sperandio G., Ventura M., Gatti F., Donna P., Tonni M., Valenti L., Gilioli G. (2021) Impact of Environmental Conditions and Management on Soil Arthropods Communities in Vineyard Ecosystems  Sustainability. 13(21), 11999"
  },
  {
    "objectID": "publications.html#submitted",
    "href": "publications.html#submitted",
    "title": "\nPublications\n",
    "section": "Submitted",
    "text": "Submitted\n\nVentura M., Jacques J., Zuccolotto P.  Model-based Clustering of Multivariate Rating Data accounting for Feeling and Uncertainty\nVentura M., Macis A., Manisera M., Zuccolotto P.  On the Equivalence of two Mixture Models for Rating Data\nVentura M., Macis A., Manisera M., Zuccolotto P.  Measuring synesthetic experience of museum visitors using multi-point semantic differential scales\nSimonetto A., Ventura M.  Distance Measures for Unweighted Undirected Networks: a comparison study"
  },
  {
    "objectID": "publications.html#submitted-papers",
    "href": "publications.html#submitted-papers",
    "title": "Publications",
    "section": "",
    "text": "Ventura M., Jacques J., Zuccolotto P.  Model-based Clustering of Multivariate Rating Data accounting for Feeling and Uncertainty\nVentura M., Macis A., Manisera M., Zuccolotto P.  On the Equivalence of two Mixture Models for Rating Data\nVentura M., Macis A., Manisera M., Zuccolotto P.  Measuring Synesthetic Experience of mMuseum Visitors using Multi-point Semantic Differential Scales\nSimonetto A., Ventura M.  Distance Measures for Unweighted Undirected Networks: a comparison study"
  },
  {
    "objectID": "publications.html#conference-proceedings",
    "href": "publications.html#conference-proceedings",
    "title": "Publications",
    "section": "",
    "text": "Ventura M., Jacques J., Zuccolotto P. (2024)  Clustering Multivariate Rating Data within the CUB Framework  52nd Scientific Meeting of the Italian Statistical Society – Bari, 17–20 giugno 2022\nSimonetto A., Ventura M., Gilioli G. (2022)  An Explorative analysis of Different Distance Metrics to Compare Unweighted Undirected Networks   51st Scientific Meeting of the Italian Statistical Society – Caserta, 22–24 giugno 2022 (pp. 1522-1527). Pearson. ISBN: 9788891932310.\nSimonetto A., Ventura M., Ghiglieno I., Gatti F., Gilioli G. (2021) Le Risposte della Biodiversità Edafica alla Gestione degli Agroecosistemi: un Approccio Basato sull’Ecological Network Anlaysis   Agriculture, Environment and Health XIII National Congress on Biodiversity – Foggia, 7–9 September 2021 (p. 236). DAFNE, University of Foggia. ISBN: 9788874271016.\nGhiglieno, I., Simonetto, A., Sperandio G., Ventura M., Gatti F., Donna P., Tonni M., Valenti L., Gilioli G. (2021)  Impatto delle Condizioni Ambientali e della Gestione del Suolo sugli Artropodi Edafici dell’Ecosistema Vigneto   Agriculture, Environment and Health XIII National Congress on Biodiversity – Foggia, 7–9 September 2021 (p. 123). DAFNE, University of Foggia. ISBN: 9788874271016."
  },
  {
    "objectID": "publications.html#submitted-articles",
    "href": "publications.html#submitted-articles",
    "title": "Publications",
    "section": "",
    "text": "Ventura M., Jacques J., Zuccolotto P.  Model-based Clustering of Multivariate Rating Data accounting for Feeling and Uncertainty\nVentura M., Macis A., Manisera M., Zuccolotto P.  Measuring Synesthetic Experience of Museum Visitors using Multi-point Semantic Differential Scales\nSimonetto A., Ventura M.  Distance Measures for Unweighted Undirected Networks: a comparison study"
  },
  {
    "objectID": "publications.html#published-articles",
    "href": "publications.html#published-articles",
    "title": "Publications",
    "section": "",
    "text": "Ventura M., Macis A., Manisera M., Zuccolotto P. (2024) On the Equivalence of two Mixture Models for Rating Data, Advances in Statistical Analysis. 1 –25, 70–86\nManisera M., Migliorati M., Ventura M., Zuccolotto P. (2024) A Mixture Model for the Analysis of Categorical Variables Measured on Five-point Semantic Differential Scales, Austrian Journal of Statistics. 53(3), 70–86\nVentura M., Ricciardi R. (2022) Exploring Financial Microblogs: Analysis of Users’ Trading Profiles with Multivariate Statistical Methods, Statistica e Applicazioni. 20(1), 61–78\nGhiglieno I., Simonetto A., Sperandio G., Ventura M., Gatti F., Donna P., Tonni M., Valenti L., Gilioli G. (2021) Impact of Environmental Conditions and Management on Soil Arthropods Communities in Vineyard Ecosystems, Sustainability. 13(21), 11999"
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About",
    "section": "",
    "text": "Application of Gaussian Graphical Models for investigating the structure of soil arthropods communities, and for studying the impact of various factors on the edaphic biodiversity of agroecosystems, in order to provide useful tools to support the decisions in the management and protection of biodiversity.\n\n\n\nImprovement and application of models within the so-called CUB (Combination of discrete Uniform and shifted Binomial random variables) class of Models, which allows for the measurement of two latent traits: feeling and uncertainty. These models have broad applications, including in the fields of measuring visitor experience, sports analytics, and sensory analysis."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "2024 PhD in Analytics for Economics and Management\n\nThesis title: Advances in Mixture Models for Ordinal Data: Theoretical Insights and Model-based Clustering\n\n\nUniversity of Brescia, Italy\n\n\nSupervisor: Prof. Paola Zuccolotto\n\n\nCo-supervisor: Prof. Julien Jacuqes\n\n\n\n\n2024 Visiting PhD student at the ERIC Laboratory\n\nUniversity Lumière Lyon 2, France\n\n\nSupervisor: Prof. Julien Jacques"
  },
  {
    "objectID": "main.html#ph.d-student-in-statistics",
    "href": "main.html#ph.d-student-in-statistics",
    "title": "Matteo Ventura",
    "section": "",
    "text": "Univeristy of Brescia"
  },
  {
    "objectID": "contacts.html",
    "href": "contacts.html",
    "title": "Matteo Ventura",
    "section": "",
    "text": "Matteo Ventura\nDepartment of Economics and Management University of Brescia Contrada S. Chiara 50, Brescia, 25122, Italy\nEmail: matteo.ventura@unibs.it"
  },
  {
    "objectID": "contacts.html#matteo-ventura-ph.d.",
    "href": "contacts.html#matteo-ventura-ph.d.",
    "title": "Matteo Ventura",
    "section": "",
    "text": "Department of Economics and Management University of Brescia Contrada S. Chiara 50, Brescia, 25122, Italy\nEmail: matteo.ventura@unibs.it"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Courses",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nOrdinal Data Analysis in R\n\n\nJun 9, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures.html#submitted-articles",
    "href": "lectures.html#submitted-articles",
    "title": "Lectures",
    "section": "",
    "text": "Ventura M., Jacques J., Zuccolotto P.  Model-based Clustering of Multivariate Rating Data accounting for Feeling and Uncertainty\nVentura M., Macis A., Manisera M., Zuccolotto P.  On the Equivalence of two Mixture Models for Rating Data\nVentura M., Macis A., Manisera M., Zuccolotto P.  Measuring Synesthetic Experience of Museum Visitors using Multi-point Semantic Differential Scales\nSimonetto A., Ventura M.  Distance Measures for Unweighted Undirected Networks: a comparison study"
  },
  {
    "objectID": "lectures.html#published-articles",
    "href": "lectures.html#published-articles",
    "title": "Lectures",
    "section": "",
    "text": "Manisera M., Migliorati M., Ventura M., Zuccolotto P. (2024)  A Mixture Model for the Analysis of Categorical Variables Measured on Five-point Semantic Differential Scales  Austrian Journal of Statistics. 53(3), 70–86\nVentura M., Ricciardi R. (2022)  Exploring Financial Microblogs: Analysis of Users’ Trading Profiles with Multivariate Statistical Methods Statistica e Applicazioni. 20(1), 61–78\nGhiglieno I., Simonetto A., Sperandio G., Ventura M., Gatti F., Donna P., Tonni M., Valenti L., Gilioli G. (2021) Impact of Environmental Conditions and Management on Soil Arthropods Communities in Vineyard Ecosystems  Sustainability. 13(21), 11999"
  },
  {
    "objectID": "lectures.html#conference-proceedings",
    "href": "lectures.html#conference-proceedings",
    "title": "Lectures",
    "section": "",
    "text": "Ventura M., Jacques J., Zuccolotto P. (2024)  Clustering Multivariate Rating Data within the CUB Framework  52nd Scientific Meeting of the Italian Statistical Society – Bari, 17–20 giugno 2022\nSimonetto A., Ventura M., Gilioli G. (2022)  An Explorative analysis of Different Distance Metrics to Compare Unweighted Undirected Networks   51st Scientific Meeting of the Italian Statistical Society – Caserta, 22–24 giugno 2022 (pp. 1522-1527). Pearson. ISBN: 9788891932310.\nSimonetto A., Ventura M., Ghiglieno I., Gatti F., Gilioli G. (2021) Le Risposte della Biodiversità Edafica alla Gestione degli Agroecosistemi: un Approccio Basato sull’Ecological Network Anlaysis   Agriculture, Environment and Health XIII National Congress on Biodiversity – Foggia, 7–9 September 2021 (p. 236). DAFNE, University of Foggia. ISBN: 9788874271016.\nGhiglieno, I., Simonetto, A., Sperandio G., Ventura M., Gatti F., Donna P., Tonni M., Valenti L., Gilioli G. (2021)  Impatto delle Condizioni Ambientali e della Gestione del Suolo sugli Artropodi Edafici dell’Ecosistema Vigneto   Agriculture, Environment and Health XIII National Congress on Biodiversity – Foggia, 7–9 September 2021 (p. 123). DAFNE, University of Foggia. ISBN: 9788874271016.\n\n\ntitle: “Untitled”"
  },
  {
    "objectID": "lectures.html#the-role-of-measurement-in-science",
    "href": "lectures.html#the-role-of-measurement-in-science",
    "title": "Ordinal Data Analysis in R: Measuring Human Perceptions from Surveys",
    "section": "1.1 The Role of Measurement in Science",
    "text": "1.1 The Role of Measurement in Science\nMeasurement is a fundamental activity in science, indeed we acquire knowledge about the world around us by observing it, and we usually quantify to give a sense to what we observe. Therefore, measurement is essential in a wide range of research contexts.\nThere exist several situations in which scientists come up with measurement problems, even though they are not interested primary in measurement. For instance:\n\nA health psychologist needs a measurement scale which doesn’t seem to exist. The study depends on a tool that can clearly distinguish between what individuals want to happen and what they expect to happen when visiting a physician. However, the review of previous research reveals that existing scales often blur this distinction, unintentionally mixing the two concepts. None of the available instruments capture the separation in the specific way her study requires. While the psychologist could create a few items that appear to address the difference between wants and expectations, she/he is concerned that these improvised questions may lack the reliability and validity necessary to serve as accurate measures.\nAn epidemiologist is conducting secondary analyses on data from a national health survey. They wish to investigate the link between perceived psychological stress and health status. Unfortunately, the survey did not include a validated stress measure. While it may be possible to construct one using existing items, a poorly constructed scale could lead to misleading conclusions.\nA marketing team is struggling to design a campaign for a new line of high-end infant toys. Focus groups suggest that parents are heavily influenced by a toy’s perceived educational value. The team hypothesizes that parents with strong educational and career aspirations for their children are more likely to be interested in the product. To test this idea across a broad, geographically diverse sample, the team needs a way to reliably measure parental aspirations. Something that additional focus groups can’t easily provide.\n\nDespite coming from different disciplines, these researchers share a common understanding: using arbitrary or poorly designed measurement tools increases the risk of collecting inaccurate data. As a result, developing their own carefully constructed measurement instruments appears to be the most reliable solution.\nHistorically, measurement problems were well-known in natural sciences such as physics and astronomy, even concerning figures like Isaac Newton. However, among social scientists, a debate arose regarding the measurability of psychological variables. While physical attributes like mass and length seem to possess an intrinsic mathematical structure similar to positive real numbers, the measurement of psychological variables was considered impossible by the Commission of the British Association for the Advancement of Science. The primary reason was the difficulty in objectively ordering or summing sensory perceptions, as well illustrated by the question: how can one establish that a sensation of “a little warm” plus another similar sensation equals “twice as warm”?\n\nMeasurement classification\nThe americal psychologist Stevens (1946) disagreed with this perspective. He contended that the rigid requirement of “strict additivity,” as seen in measurements of length or mass, was not essential for measuring sensations. He pointed out that individuals could make reasonably consistent ratio judgments regarding the loudness of sounds. For instance, they could determine if one sound was twice as loud as another.\nStevens further argued that this “ratio” characteristic enabled the data derived from such measurements to be mathematically analyzed. He is known for categorizing measurements into nominal, ordinal, interval, and ratio scales. In his view, judgments about sound “loudness” belonged to the ratio scale.\nDespite the classification proposed by Stevens has been criticized by several authors and new classifications has been proposed, it is the most commonly accepted and used internationally.\nStevens identified four properties for describing the scales of measurement:\n\nIdentity: each value has a unique meaning.\nMagnitude: the values of the variable have an ordered relationship to one another, so there is a specific order to the variables.\nEqual intervals: the data points along the scale are equally spaced, so the difference between data points one and two, is the same as data points three and four.\nA minimum value of zero: the scale has a true zero point.\n\nAs previously said, Stevens identified four scales of measurement, that is how variables are defined and categorised:\n\nNominal scale of measurement: This scale has certain characteristics, but doesn’t have any form of numerical meaning. The data can be placed into categories but can’t be multiplied, divided, added or subtracted from one another. It’s also not possible to measure the difference between data points. It defines only the identity property of data.\nExamples: Gender, Etnicity, Eye colour…\nOrdinal scale of measurement: It defines data that is placed in a specific order. While each value is ranked, there’s no information that specifies what differentiates the categories from each other. These values can’t be added to or subtracted from.\nExamples: satisfaction data points in a survey, where ‘one = happy, two = neutral and three = unhappy.’\nInterval scale of measurement: The interval scale contains properties of nominal and ordered data, but the difference between data points can be quantified. This type of data shows both the order of the variables and the exact differences between the variables. They can be added to or subtracted from each other, but not multiplied or divided (For example, 40 degrees is not 20 degrees multiplied by two.).\nIn this scale of measurement the zero is just a convention and not absolute, it is an existing value of the variable itself.\nRatio scale of measurement: This scale include properties from all four scales of measurement. The data is nominal and defined by an identity, can be classified in order, contains intervals and can be broken down into exact value. Weight, height and distance are all examples of ratio variables. Data in the ratio scale can be added, subtracted, divided and multiplied. Ratio scales also differ from interval scales in that the scale has a ‘true zero’. The number zero means that the data has no value point.\nAn example of this is height or weight, as someone cannot be zero centimetres tall or weigh zero kilos."
  },
  {
    "objectID": "lectures.html#scales-and-questionnaires-development",
    "href": "lectures.html#scales-and-questionnaires-development",
    "title": "Ordinal Data Analysis in R: Measuring Human Perceptions from Surveys",
    "section": "1.2 Scales and Questionnaires development",
    "text": "1.2 Scales and Questionnaires development\nMeasurement is a crucial aspect in various scientific disciplines, with each field developing specialized methods and tools to measure its specific objects of study. In the behavioral and social sciences, the branch dedicated to measurement is known as psychometrics. This subfield focuses on the assessment of psychological and social phenomena, which are typically measured through the use of questionnaires."
  },
  {
    "objectID": "courses/course1.html",
    "href": "courses/course1.html",
    "title": "Corso di Statistica",
    "section": "",
    "text": "Contenuto completo del corso qui…\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOrdinal Data Analysis in R: Measuring Human Perceptions from Surveys\n\n\n\n\n\n\n\n\n\n\n\n\nMatteo Ventura\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/Ordinal_data_an_R.html",
    "href": "courses/Ordinal_data_an_R.html",
    "title": "Ordinal Data Analysis in R",
    "section": "",
    "text": "Surveys are key tools for measuring human perceptions, capturing latent traits through structured responses. Among the data they generate, ordinal and rating data are particularly important yet often less studied, requiring specialized statistical techniques. Ordinal data appears frequently in real-world applications, such as customer satisfaction surveys, psychological assessments, and medical research, making its correct analysis crucial for obtaining reliable insights. This short course provides instructor-led, hands-on training in the analysis of ordinal data. It begins with an overview of survey design and the validation of results, focusing on building effective surveys and ensuring the reliability of the data obtained. The course then covers the most commonly used statistical models for analyzing ordinal data, with an emphasis on discovering latent patterns and traits. Both theoretical foundations and practical applications will be explored, using real-world case studies from domains such as marketing, social sciences, tourism and culture.\nA common approach to analyzing ordinal data is to treat it as numerical, but this can lead to a loss of statistical power. In this course, participants will learn how to apply specialized methods designed for ordinal data, allowing them to draw more effective and reliable conclusions.\nBy the end of the course, participants will have both theoretical knowledge and practical skills to analyze ordinal data in research and professional settings. Specifically, they will be able to:"
  },
  {
    "objectID": "courses/Ordinal_data_an_R.html#the-role-of-measurement-in-science",
    "href": "courses/Ordinal_data_an_R.html#the-role-of-measurement-in-science",
    "title": "Ordinal Data Analysis in R",
    "section": "1.1 The Role of Measurement in Science",
    "text": "1.1 The Role of Measurement in Science\nMeasurement is a fundamental activity in science, indeed we acquire knowledge about the world around us by observing it, and we usually quantify to give a sense to what we observe. Therefore, measurement is essential in a wide range of research contexts.\nThere exist several situations in which scientists come up with measurement problems, even though they are not interested primary in measurement. For instance:\n\nA health psychologist needs a measurement scale which doesn’t seem to exist. The study depends on a tool that can clearly distinguish between what individuals want to happen and what they expect to happen when visiting a physician. However, the review of previous research reveals that existing scales often blur this distinction, unintentionally mixing the two concepts. None of the available instruments capture the separation in the specific way her study requires. While the psychologist could create a few items that appear to address the difference between wants and expectations, she/he is concerned that these improvised questions may lack the reliability and validity necessary to serve as accurate measures.\nAn epidemiologist is conducting secondary analyses on data from a national health survey. They wish to investigate the link between perceived psychological stress and health status. Unfortunately, the survey did not include a validated stress measure. While it may be possible to construct one using existing items, a poorly constructed scale could lead to misleading conclusions.\nA marketing team is struggling to design a campaign for a new line of high-end infant toys. Focus groups suggest that parents are heavily influenced by a toy’s perceived educational value. The team hypothesizes that parents with strong educational and career aspirations for their children are more likely to be interested in the product. To test this idea across a broad, geographically diverse sample, the team needs a way to reliably measure parental aspirations. Something that additional focus groups can’t easily provide.\n\nDespite coming from different disciplines, these researchers share a common understanding: using arbitrary or poorly designed measurement tools increases the risk of collecting inaccurate data. As a result, developing their own carefully constructed measurement instruments appears to be the most reliable solution.\nHistorically, measurement problems were well-known in natural sciences such as physics and astronomy, even concerning figures like Isaac Newton. However, among social scientists, a debate arose regarding the measurability of psychological variables. While physical attributes like mass and length seem to possess an intrinsic mathematical structure similar to positive real numbers, the measurement of psychological variables was considered impossible by the Commission of the British Association for the Advancement of Science. The primary reason was the difficulty in objectively ordering or summing sensory perceptions, as well illustrated by the question: how can one establish that a sensation of “a little warm” plus another similar sensation equals “twice as warm”?\n\nMeasurement classification\nThe americal psychologist Stevens (1946) disagreed with this perspective. He contended that the rigid requirement of “strict additivity,” as seen in measurements of length or mass, was not essential for measuring sensations. He pointed out that individuals could make reasonably consistent ratio judgments regarding the loudness of sounds. For instance, they could determine if one sound was twice as loud as another.\nStevens further argued that this “ratio” characteristic enabled the data derived from such measurements to be mathematically analyzed. He is known for categorizing measurements into nominal, ordinal, interval, and ratio scales. In his view, judgments about sound “loudness” belonged to the ratio scale.\nDespite the classification proposed by Stevens has been criticized by several authors and new classifications has been proposed, it is the most commonly accepted and used internationally.\nStevens identified four properties for describing the scales of measurement:\n\nIdentity: each value has a unique meaning.\nMagnitude: the values of the variable have an ordered relationship to one another, so there is a specific order to the variables.\nEqual intervals: the data points along the scale are equally spaced, so the difference between data points one and two, is the same as data points three and four.\nA minimum value of zero: the scale has a true zero point.\n\nAs previously said, Stevens identified four scales of measurement, that is how variables are defined and categorised:\n\nNominal scale of measurement: This scale has certain characteristics, but doesn’t have any form of numerical meaning. The data can be placed into categories but can’t be multiplied, divided, added or subtracted from one another. It’s also not possible to measure the difference between data points. It defines only the identity property of data.\nExamples: Gender, Etnicity, Eye colour…\nOrdinal scale of measurement: It defines data that is placed in a specific order. While each value is ranked, there’s no information that specifies what differentiates the categories from each other. These values can’t be added to or subtracted from.\nExamples: satisfaction data points in a survey, where ‘one = happy, two = neutral and three = unhappy.’\nInterval scale of measurement: The interval scale contains properties of nominal and ordered data, but the difference between data points can be quantified. This type of data shows both the order of the variables and the exact differences between the variables. They can be added to or subtracted from each other, but not multiplied or divided (For example, 40 degrees is not 20 degrees multiplied by two.).\nIn this scale of measurement the zero is just a convention and not absolute, it is an existing value of the variable itself.\nRatio scale of measurement: This scale include properties from all four scales of measurement. The data is nominal and defined by an identity, can be classified in order, contains intervals and can be broken down into exact value. Weight, height and distance are all examples of ratio variables. Data in the ratio scale can be added, subtracted, divided and multiplied. Ratio scales also differ from interval scales in that the scale has a ‘true zero’. The number zero means that the data has no value point.\nAn example of this is height or weight, as someone cannot be zero centimetres tall or weigh zero kilos."
  },
  {
    "objectID": "courses/Ordinal_data_an_R.html#scales-and-questionnaires-development",
    "href": "courses/Ordinal_data_an_R.html#scales-and-questionnaires-development",
    "title": "Ordinal Data Analysis in R",
    "section": "1.2 Scales and Questionnaires development",
    "text": "1.2 Scales and Questionnaires development\nMeasurement plays a vital role across scientific disciplines, with each field creating specialized methods and tools tailored to its unique subjects of study. In the behavioral and social sciences, the area devoted to measurement is called psychometrics. This subfield concentrates on evaluating psychological and social constructs, which are most often assessed using questionnaires. Theaching how to build effective questionnaires would require a specific course, but this is out of the scope of this course. The following are some practical guidelines that researchers can use to develop measurement scales and questionnaires.\n\nDetermine Clearly What You Want to Measure\nResearchers often discover their initial ideas about what they want to measure are vague, which can lead to costly changes later. Key questions include whether the scale should be theory-based or explore new directions, its level of specificity, and which aspects of the phenomenon to emphasize.\n\nDefine the theory: Basing scale development on relevant substantive theories is essential for clearly defining the construct being measured, particularly when dealing with abstract or non-observable phenomena. A theoretical basis helps establish the construct’s boundaries, reducing the risk of the scale extending into unrelated areas. In the absence of an existing theory, developers should create a conceptual framework of their own—beginning with a precise definition and linking the new construct to related, established ones.\nDetermine the level of specificity: In psychometric scale development, it’s important to consider how general or specific the measurement should be. This decision affects how well the scale works in predicting or relating to other variables. For example, if you’re interested in general attitudes about personal control, a broad scale scale works well. But if you’re studying beliefs about controlling a specific health issue, a focused scale is more appropriate.\nDefine which aspects are enphasised: Scale developers must clearly distinguish the target construct from related ones. Scales can be broad (e.g., general anxiety) or narrow (e.g., test anxiety). Including items outside the intended focus can lead to confusion or inaccurate measurement. For example, in health contexts, physical symptoms caused by an illness might be mistaken for psychological symptoms (like depression), leading to misleading results. Therefore, item selection should match the specific research purpose and avoid overlap with unrelated constructs. \n\n\n\n\n\n\nGenerate an Item Pool\nWhen developing a psychometric scale, items should be carefully selected or created to match the specific construct you aim to measure. That means you need a clear idea of what the scale is supposed to do, and every item on the scale should reflect that goal.\nImagine the construct (like anxiety, motivation, or trust) as something hidden or latent, which can’t be observed directly. The items on your scale are the visible signs or behaviors that reflect this hidden thing. So, each item acts like a small “test” of how much of that construct a person has. If your items truly measure the construct, then someone with a high level of the trait should tend to score higher on all of them.\nWhen constructing the item pool, it is important to consider the following aspects:\n\nThe latent construct A good scale includes multiple items to improve reliability, but every single item must still be strongly connected to the latent construct. You should think broadly and creatively when writing items to make sure they cover all the different ways the construct can be expressed—but without straying into measuring something else.\nA construct is a single, unified idea (like “attitudes toward punishing drug abusers”) that can be thought of as causing how someone responds to related items. A category, on the other hand, is just a grouping of different constructs (like “attitudes” in general, or “barriers to compliance”).\nJust because several items relate to the same category doesn’t mean they measure the same underlying construct. For instance, “Barriers to compliance” is a category that can include many distinct things (fear of symptoms, cost concerns, distance to treatment, etc.). Each of these could be a separate construct with its own latent variable, so a scale that mixes these up wouldn’t truly be unidimensional (i.e., measuring just one thing).\nRedundancy is crucial for reliability: multiple items allow common content to summate while idiosyncrasies cancel out. However, avoid superficial redundancy (e.g., minor wording changes, identical grammatical structures) which can inflate reliability estimates. Useful redundancy involves expressing the same core idea differently. Overly specific or redundant items within a broader scale can create subclusters (e.g., multiple specific anxiety items in a general emotion scale), potentially undermining unidimensionality and biasing the scale. This is less of a problem if the items match the scale’s intended specificity.\nThe number of items Start with more items than planned for the final scale (e.g., 3-4 times as many) to allow for careful selection and ensure good internal consistency. An initial pool 50% larger might suffice if items are hard to generate or fewer are needed for reliability. If the pool is too large, eliminate items based on criteria like lack of clarity or relevance.\nThe wording Including both positively worded items (indicating the presence of the construct) and negatively worded items (indicating its absence or low levels) is a common strategy to reduce acquiescence bias—the tendency of respondents to agree with statements regardless of their content. However, reversing the wording can sometimes confuse participants, particularly in general population or community samples, and this confusion may reduce the scale’s reliability.\n\n\n\n\n\n\n\n\nReversing the wording of items (also known as reversed polarity) can confuse respondents, especially if the items are complex or abstract, ot if the respondents have lower reading comprehension or aren’t used to taking surveys.\nThis confusion can lead to inconsistent or inaccurate responses, which lowers the reliability of the scale (i.e., how consistently it measures the construct).\n\n\n\n\n\n\n\n\n\n\n\nDetermine the Format for Measurement\nDefining the measurement format is a critical step in designing data collection instruments like questionnaires and scales. This decision, ideally made concurrently with item generation, impacts data quality, variability, instrument sensitivity, and ultimately, research conclusions.\n\n\n\nMost scale items consist of two parts: a stem and a series of response options. A kew aspect of the scale items is the number of response options. A desiderable quality of a measurement scale is variability. One way to increase opportunities for variability is to include lots of scale items. Another way is to provide numerous respose options within each item, especially with fewer items.\nIn this view, continuous formats (e.g., thermometer scales) offer many gradations, and so increase the opportunities for variability. However, too many options can exceed respondents’ ability to meaningfully discriminate, leading to “false precision” and increased error variance. Researchers must balance the need for variability with respondents’ cognitive limitations.\nAnother issue the investigator has to concern with, is whether the number of options should be even or odd. This choice depends on the type of question. the type of response option, and the objectives of the investigator.\nAn odd number of categories usually allows to express neutrality, while an even number of categories forces a choice from the respondent. The choice depends on whether allowing neutrality is desirable or should be avoided.\nThere exist several ways to present items that are commonly used:\n\nLikert Scale: It is one of the most common item formats. The Liker Scale presents declarative statements with response options indicating degree of agreement (e.g., strongly disagree to strongly agree), and it is useful for measuring opinions, beliefs, attitudes.\nThe statements should generally be moderately strong, since it is better to lead the respondent to not give responses near the center of the scale from the “average” respondent to maximize variance and discrimination.\nOptions should represent roughly equal intervals. Likert Scales with 5, 6, 7 categories are commonly used.\nSemantic Differential Scale: It is used in reference ot one or more stimuli (e.g. a group of people) followed by pairs of opposite adjectives (e.g., honest/dishonest) separated by several response lines/spaces. Respondents mark the space reflecting their evaluation. The adjectives can be bipolar (friendly/hostile) or unipolar (friendly/not friendly). To measure an underlying variable, multiple related adjective pairs can be used (e.g., honesty).\nVisual Analog Scale (VAS): Presents a continuous line between two descriptors and the respondents mark a point on the line. Therefore, it is clear that this scale allows continuous scoring but it has to be noted that interpretation can be subjective, and comparisons across individuals may be difficult. An advantage of this type of scale is that it is shghly sensitive, so it useful for detecting subtle changes within individuals over time; moreover they may reduce reduce bias from recalling previous discrete responses.\n\n\n\nBinary Options: Offer two choices (e.g., agree/disagree, yes/no, check if applies). This type of option is simple for respondents but yields to minimal variability per item, therefore more items are required for obtaining an adequate scale variance. However, the ease of response may allow for more items to be administered.\n\n\n\n\nExperts’ review\nExpert review plays a key role in strengthening content validity during scale development. By drawing on their subject-matter expertise, reviewers help ensure that the items meaningfully represent the construct.\nExperts are typically asked to assess how well each item reflects the construct definition, providing feedback that can confirm or refine the conceptual framework. They also evaluate the clarity and precision of item wording, offering suggestions to reduce ambiguity. In addition, experts may highlight important aspects of the construct that have been overlooked.\nHowever, it’s important to note that content experts may not be familiar with psychometric principles. For instance, they might recommend eliminating seemingly redundant items, not realizing that some redundancy is intentional and necessary for reliability. While expert input is highly valuable, final decisions should rest with the scale developer, who must balance expert judgment with methodological rigor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubsequent Steps in Scales Development\nFollowing the initial design of the questionnaire, including the selection and construction of appropriate scales, the next crucial phase involves preparing for validation and data collection. This includes strategically incorporating additional items aimed at facilitating later validation efforts, such as those designed to detect response biases or to assess the questionnaire’s construct validity by measuring theoretically related concepts.\nSubsequently, the questionnaire is administered to a development sample. It’s essential that this sample is sufficiently large and representative of the target population to ensure stable results and minimize concerns about subject variance.\nOnce the data is collected, a thorough evaluation of the individual items is undertaken. This involves examining their intercorrelations to ensure they are measuring a common underlying construct, addressing any negatively correlated items through techniques like reverse scoring, and assessing the correlation of each item with the overall scale. Furthermore, the variance and means of the items are analyzed to ensure they discriminate effectively among respondents. Factor analysis is employed to confirm the dimensionality of the scale, and reliability, often measured by Cronbach’s alpha, is calculated to assess the internal consistency of the items.\nFinally, the length of the scale is optimized. This involves balancing the need for brevity to reduce respondent burden with the desire for higher reliability, which is generally associated with longer scales. Weak items that negatively impact reliability are considered for removal, and techniques like splitting the development sample for cross-validation can be used to ensure the stability of the optimized scale in new samples.\nRANKINGS"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html",
    "title": "Ordinal Data Analysis in R",
    "section": "",
    "text": "Surveys are key tools for measuring human perceptions, capturing latent traits through structured responses. Among the data they generate, ordinal and rating data are particularly important yet often less studied, requiring specialized statistical techniques. Ordinal data appears frequently in real-world applications, such as customer satisfaction surveys, psychological assessments, and medical research, making its correct analysis crucial for obtaining reliable insights. This short course provides instructor-led, hands-on training in the analysis of ordinal data. It begins with an overview of survey design and the validation of results, focusing on building effective surveys and ensuring the reliability of the data obtained. The course then covers the most commonly used statistical models for analyzing ordinal data, with an emphasis on discovering latent patterns and traits. Both theoretical foundations and practical applications will be explored, using real-world case studies from domains such as marketing, social sciences, tourism and culture.\nA common approach to analyzing ordinal data is to treat it as numerical, but this can lead to a loss of statistical power. In this course, participants will learn how to apply specialized methods designed for ordinal data, allowing them to draw more effective and reliable conclusions.\nBy the end of the course, participants will have both theoretical knowledge and practical skills to analyze ordinal data in research and professional settings. Specifically, they will be able to:"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#the-role-of-measurement-in-science",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#the-role-of-measurement-in-science",
    "title": "Ordinal Data Analysis in R",
    "section": "1.1 The Role of Measurement in Science",
    "text": "1.1 The Role of Measurement in Science\nMeasurement is a fundamental activity in science, indeed we acquire knowledge about the world around us by observing it, and we usually quantify to give a sense to what we observe. Therefore, measurement is essential in a wide range of research contexts.\nThere exist several situations in which scientists come up with measurement problems, even though they are not interested primary in measurement. For instance:\n\nA health psychologist needs a measurement scale which doesn’t seem to exist. The study depends on a tool that can clearly distinguish between what individuals want to happen and what they expect to happen when visiting a physician. However, the review of previous research reveals that existing scales often blur this distinction, unintentionally mixing the two concepts. None of the available instruments capture the separation in the specific way her study requires. While the psychologist could create a few items that appear to address the difference between wants and expectations, she/he is concerned that these improvised questions may lack the reliability and validity necessary to serve as accurate measures.\nAn epidemiologist is conducting secondary analyses on data from a national health survey. They wish to investigate the link between perceived psychological stress and health status. Unfortunately, the survey did not include a validated stress measure. While it may be possible to construct one using existing items, a poorly constructed scale could lead to misleading conclusions.\nA marketing team is struggling to design a campaign for a new line of high-end infant toys. Focus groups suggest that parents are heavily influenced by a toy’s perceived educational value. The team hypothesizes that parents with strong educational and career aspirations for their children are more likely to be interested in the product. To test this idea across a broad, geographically diverse sample, the team needs a way to reliably measure parental aspirations. Something that additional focus groups can’t easily provide.\n\nDespite coming from different disciplines, these researchers share a common understanding: using arbitrary or poorly designed measurement tools increases the risk of collecting inaccurate data. As a result, developing their own carefully constructed measurement instruments appears to be the most reliable solution.\nHistorically, measurement problems were well-known in natural sciences such as physics and astronomy, even concerning figures like Isaac Newton. However, among social scientists, a debate arose regarding the measurability of psychological variables. While physical attributes like mass and length seem to possess an intrinsic mathematical structure similar to positive real numbers, the measurement of psychological variables was considered impossible by the Commission of the British Association for the Advancement of Science. The primary reason was the difficulty in objectively ordering or summing sensory perceptions, as well illustrated by the question: how can one establish that a sensation of “a little warm” plus another similar sensation equals “twice as warm”?\n\nMeasurement classification\nThe americal psychologist Stevens (1946) disagreed with this perspective. He contended that the rigid requirement of “strict additivity,” as seen in measurements of length or mass, was not essential for measuring sensations. He pointed out that individuals could make reasonably consistent ratio judgments regarding the loudness of sounds. For instance, they could determine if one sound was twice as loud as another.\nStevens further argued that this “ratio” characteristic enabled the data derived from such measurements to be mathematically analyzed. He is known for categorizing measurements into nominal, ordinal, interval, and ratio scales. In his view, judgments about sound “loudness” belonged to the ratio scale.\nDespite the classification proposed by Stevens has been criticized by several authors and new classifications has been proposed, it is the most commonly accepted and used internationally.\nStevens identified four properties for describing the scales of measurement:\n\nIdentity: each value has a unique meaning.\nMagnitude: the values of the variable have an ordered relationship to one another, so there is a specific order to the variables.\nEqual intervals: the data points along the scale are equally spaced, so the difference between data points one and two, is the same as data points three and four.\nA minimum value of zero: the scale has a true zero point.\n\nAs previously said, Stevens identified four scales of measurement, that is how variables are defined and categorised:\n\nNominal scale of measurement: This scale has certain characteristics, but doesn’t have any form of numerical meaning. The data can be placed into categories but can’t be multiplied, divided, added or subtracted from one another. It’s also not possible to measure the difference between data points. It defines only the identity property of data.\nExamples: Gender, Etnicity, Eye colour…\nOrdinal scale of measurement: It defines data that is placed in a specific order. While each value is ranked, there’s no information that specifies what differentiates the categories from each other. These values can’t be added to or subtracted from.\nExamples: satisfaction data points in a survey, where ‘one = happy, two = neutral and three = unhappy.’\nInterval scale of measurement: The interval scale contains properties of nominal and ordered data, but the difference between data points can be quantified. This type of data shows both the order of the variables and the exact differences between the variables. They can be added to or subtracted from each other, but not multiplied or divided (For example, 40 degrees is not 20 degrees multiplied by two.).\nIn this scale of measurement the zero is just a convention and not absolute, it is an existing value of the variable itself.\nRatio scale of measurement: This scale include properties from all four scales of measurement. The data is nominal and defined by an identity, can be classified in order, contains intervals and can be broken down into exact value. Weight, height and distance are all examples of ratio variables. Data in the ratio scale can be added, subtracted, divided and multiplied. Ratio scales also differ from interval scales in that the scale has a ‘true zero’. The number zero means that the data has no value point.\nAn example of this is height or weight, as someone cannot be zero centimetres tall or weigh zero kilos."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#scales-and-questionnaires-development",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#scales-and-questionnaires-development",
    "title": "Ordinal Data Analysis in R",
    "section": "1.2 Scales and Questionnaires development",
    "text": "1.2 Scales and Questionnaires development\nMeasurement plays a vital role across scientific disciplines, with each field creating specialized methods and tools tailored to its unique subjects of study. In the behavioral and social sciences, the area devoted to measurement is called psychometrics. This subfield concentrates on evaluating psychological and social constructs, which are most often assessed using questionnaires. Theaching how to build effective questionnaires would require a specific course, but this is out of the scope of this course. The following are some practical guidelines that researchers can use to develop measurement scales and questionnaires.\n\nDetermine Clearly What You Want to Measure\nResearchers often discover their initial ideas about what they want to measure are vague, which can lead to costly changes later. Key questions include whether the scale should be theory-based or explore new directions, its level of specificity, and which aspects of the phenomenon to emphasize.\n\nDefine the theory: Basing scale development on relevant substantive theories is essential for clearly defining the construct being measured, particularly when dealing with abstract or non-observable phenomena. A theoretical basis helps establish the construct’s boundaries, reducing the risk of the scale extending into unrelated areas. In the absence of an existing theory, developers should create a conceptual framework of their own—beginning with a precise definition and linking the new construct to related, established ones.\nDetermine the level of specificity: In psychometric scale development, it’s important to consider how general or specific the measurement should be. This decision affects how well the scale works in predicting or relating to other variables. For example, if you’re interested in general attitudes about personal control, a broad scale scale works well. But if you’re studying beliefs about controlling a specific health issue, a focused scale is more appropriate.\nDefine which aspects are enphasised: Scale developers must clearly distinguish the target construct from related ones. Scales can be broad (e.g., general anxiety) or narrow (e.g., test anxiety). Including items outside the intended focus can lead to confusion or inaccurate measurement. For example, in health contexts, physical symptoms caused by an illness might be mistaken for psychological symptoms (like depression), leading to misleading results. Therefore, item selection should match the specific research purpose and avoid overlap with unrelated constructs. \n\n\n\n\n\n\nGenerate an Item Pool\nWhen developing a psychometric scale, items should be carefully selected or created to match the specific construct you aim to measure. That means you need a clear idea of what the scale is supposed to do, and every item on the scale should reflect that goal.\nImagine the construct (like anxiety, motivation, or trust) as something hidden or latent, which can’t be observed directly. The items on your scale are the visible signs or behaviors that reflect this hidden thing. So, each item acts like a small “test” of how much of that construct a person has. If your items truly measure the construct, then someone with a high level of the trait should tend to score higher on all of them.\nWhen constructing the item pool, it is important to consider the following aspects:\n\nThe latent construct A good scale includes multiple items to improve reliability, but every single item must still be strongly connected to the latent construct. You should think broadly and creatively when writing items to make sure they cover all the different ways the construct can be expressed—but without straying into measuring something else.\nA construct is a single, unified idea (like “attitudes toward punishing drug abusers”) that can be thought of as causing how someone responds to related items. A category, on the other hand, is just a grouping of different constructs (like “attitudes” in general, or “barriers to compliance”).\nJust because several items relate to the same category doesn’t mean they measure the same underlying construct. For instance, “Barriers to compliance” is a category that can include many distinct things (fear of symptoms, cost concerns, distance to treatment, etc.). Each of these could be a separate construct with its own latent variable, so a scale that mixes these up wouldn’t truly be unidimensional (i.e., measuring just one thing).\nRedundancy is crucial for reliability: multiple items allow common content to summate while idiosyncrasies cancel out. However, avoid superficial redundancy (e.g., minor wording changes, identical grammatical structures) which can inflate reliability estimates. Useful redundancy involves expressing the same core idea differently. Overly specific or redundant items within a broader scale can create subclusters (e.g., multiple specific anxiety items in a general emotion scale), potentially undermining unidimensionality and biasing the scale. This is less of a problem if the items match the scale’s intended specificity.\nThe number of items Start with more items than planned for the final scale (e.g., 3-4 times as many) to allow for careful selection and ensure good internal consistency. An initial pool 50% larger might suffice if items are hard to generate or fewer are needed for reliability. If the pool is too large, eliminate items based on criteria like lack of clarity or relevance.\nThe wording Including both positively worded items (indicating the presence of the construct) and negatively worded items (indicating its absence or low levels) is a common strategy to reduce acquiescence bias—the tendency of respondents to agree with statements regardless of their content. However, reversing the wording can sometimes confuse participants, particularly in general population or community samples, and this confusion may reduce the scale’s reliability.\n\n\n\n\n\n\n\n\nReversing the wording of items (also known as reversed polarity) can confuse respondents, especially if the items are complex or abstract, ot if the respondents have lower reading comprehension or aren’t used to taking surveys.\nThis confusion can lead to inconsistent or inaccurate responses, which lowers the reliability of the scale (i.e., how consistently it measures the construct).\n\n\n\n\n\n\n\n\n\n\n\nDetermine the Format for Measurement\nDefining the measurement format is a critical step in designing data collection instruments like questionnaires and scales. This decision, ideally made concurrently with item generation, impacts data quality, variability, instrument sensitivity, and ultimately, research conclusions.\n\n\n\nMost scale items consist of two parts: a stem and a series of response options. A kew aspect of the scale items is the number of response options. A desiderable quality of a measurement scale is variability. One way to increase opportunities for variability is to include lots of scale items. Another way is to provide numerous respose options within each item, especially with fewer items.\nIn this view, continuous formats (e.g., thermometer scales) offer many gradations, and so increase the opportunities for variability. However, too many options can exceed respondents’ ability to meaningfully discriminate, leading to “false precision” and increased error variance. Researchers must balance the need for variability with respondents’ cognitive limitations.\nAnother issue the investigator has to concern with, is whether the number of options should be even or odd. This choice depends on the type of question. the type of response option, and the objectives of the investigator.\nAn odd number of categories usually allows to express neutrality, while an even number of categories forces a choice from the respondent. The choice depends on whether allowing neutrality is desirable or should be avoided.\nThere exist several ways to present items that are commonly used:\n\nLikert scales: are widely used psychometric tools designed to measure attitudes, opinions, and perceptions by assessing the degree of agreement or disagreement with a statement. These scales typically present a statement (called a Likert item) followed by an ordered series of response options, generally consisting of five or seven points. However, scales with four, nine, or ten points can also be employed.\n\nResponse anchors are the labels that define each point on the scale (for example, “Strongly disagree,” “Disagree,” “Neutral,” “Agree,” “Strongly agree”). Scales with an odd number of points often include a neutral midpoint, while scales with an even number of points force the respondent to express a direction (agreement or disagreement).\nLikert scales are extensively applied in surveys to assess employee engagement, customer satisfaction, product feedback, and clinical evaluations.\n\n\n\n\n\n\n\nAlthough Likert scale data is often numerically coded to facilitate analysis, it’s essential to remember their ordinal nature and approach the calculation of means with caution.\n\n\n\n\n\n\nSemantic Differential scales: are assessment tools used to measure attitudes and opinions toward an object, person, event, or idea through pairs of bipolar adjectives. Developed by psychologist Charles E. Osgood, these scales present a concept followed by several rows of opposite adjective pairs placed at the extremes of a continuum, typically with five to seven intermediate points. Respondents evaluate the concept on each adjectival scale by selecting the point that best represents their attitude. Examples of bipolar adjective pairs include “Good - Bad,” “Happy - Sad,” “Strong - Weak,” and “Pleasant - Unpleasant.” These scales are commonly used in market research, branding, and customer satisfaction assessments to understand perceptions and associations.\n\n\nSemantic differential scales explore the connotative meaning of a concept, revealing the emotional and evaluative dimensions of attitudes, unlike Likert scales which primarily focus on the degree of agreement.\n\n\n\nRankings: represent data where items are ordered according to a specific criterion or preference. Respondents arrange items in a sequence based on their preference, importance, or another defined attribute. Examples include ranking favorite movies, product features by importance, or job candidates. Ranking data indicates relative order but not the magnitude of difference between positions. The difference between the first and second positions might be substantial, while the difference between lower positions might be negligible.\n\n\n\nVisual Analog Scale (VAS): Presents a continuous line between two descriptors and the respondents mark a point on the line. Therefore, it is clear that this scale allows continuous scoring but it has to be noted that interpretation can be subjective, and comparisons across individuals may be difficult. An advantage of this type of scale is that it is shghly sensitive, so it useful for detecting subtle changes within individuals over time; moreover they may reduce reduce bias from recalling previous discrete responses.\n\n\n\nBinary Options: Offer two choices (e.g., agree/disagree, yes/no, check if applies). This type of option is simple for respondents but yields to minimal variability per item, therefore more items are required for obtaining an adequate scale variance. However, the ease of response may allow for more items to be administered.\n\n\n\n\nExperts’ review\nExpert review plays a key role in strengthening content validity during scale development. By drawing on their subject-matter expertise, reviewers help ensure that the items meaningfully represent the construct.\nExperts are typically asked to assess how well each item reflects the construct definition, providing feedback that can confirm or refine the conceptual framework. They also evaluate the clarity and precision of item wording, offering suggestions to reduce ambiguity. In addition, experts may highlight important aspects of the construct that have been overlooked.\nHowever, it’s important to note that content experts may not be familiar with psychometric principles. For instance, they might recommend eliminating seemingly redundant items, not realizing that some redundancy is intentional and necessary for reliability. While expert input is highly valuable, final decisions should rest with the scale developer, who must balance expert judgment with methodological rigor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubsequent Steps in Scales Development\nFollowing the initial design of the questionnaire, including the selection and construction of appropriate scales, the next crucial phase involves preparing for validation and data collection. This includes strategically incorporating additional items aimed at facilitating later validation efforts, such as those designed to detect response biases or to assess the questionnaire’s construct validity by measuring theoretically related concepts.\nSubsequently, the questionnaire is administered to a development sample. It’s essential that this sample is sufficiently large and representative of the target population to ensure stable results and minimize concerns about subject variance.\nOnce the data is collected, a thorough evaluation of the individual items is undertaken. This involves examining their intercorrelations to ensure they are measuring a common underlying construct, addressing any negatively correlated items through techniques like reverse scoring, and assessing the correlation of each item with the overall scale. Furthermore, the variance and means of the items are analyzed to ensure they discriminate effectively among respondents. Factor analysis is employed to confirm the dimensionality of the scale, and reliability, often measured by Cronbach’s alpha, is calculated to assess the internal consistency of the items.\nFinally, the length of the scale is optimized. This involves balancing the need for brevity to reduce respondent burden with the desire for higher reliability, which is generally associated with longer scales. Weak items that negatively impact reliability are considered for removal, and techniques like splitting the development sample for cross-validation can be used to ensure the stability of the optimized scale in new samples."
  },
  {
    "objectID": "courses/nome_del_nuovo_corso/index.html",
    "href": "courses/nome_del_nuovo_corso/index.html",
    "title": "Titolo del Nuovo Corso",
    "section": "",
    "text": "Contenuto del Nuovo Corso\n…"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#principles-for-visualizing-ordinal-data",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#principles-for-visualizing-ordinal-data",
    "title": "Ordinal Data Analysis in R",
    "section": "1.3 Principles for Visualizing Ordinal Data",
    "text": "1.3 Principles for Visualizing Ordinal Data\nThe most important principle in visualizing ordinal data is to always represent ordinal categories in their natural, ordered sequence in any visual representation. In bar charts, bars should be arranged along the axis based on the logical order of the ordinal scale (e.g., from “Low” to “High”). For stacked and divergent bar charts, the segments representing ordinal categories should also follow this intrinsic order within each bar.\n\n\nCode\nlibrary(ggplot2)\n\n# Create sample data\nsatisfaction &lt;- data.frame(\n  level = factor(c(\"Very Dissatisfied\", \"Dissatisfied\", \"Neutral\", \"Satisfied\", \"Very Satisfied\"),\n                 levels = c(\"Very Dissatisfied\", \"Dissatisfied\", \"Neutral\", \"Satisfied\", \"Very Satisfied\")),\n  count = c(15, 23, 30, 45, 27)\n)\n\n# Create bar chart with ordered categories\nggplot(satisfaction, aes(x = level, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(title = \"Customer Satisfaction Levels\",\n       x = \"Satisfaction Level\",\n       y = \"Number of Responses\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe choice of chart should align with the research question and the specific aspect of ordinal data being investigated. Not all chart types are equally effective for representing ordered categorical data.\n\nBar Charts\nRepresent each ordinal category with a bar, whose height or length corresponds to the frequency or count of that category. Fundamentally, the bars must be arranged in the logical order of the ordinal variable (e.g., from lowest to highest category). They can be vertical or horizontal; horizontal orientation is often preferred for readability of long category labels.\nBar charts provide a clear and easily understandable visualization of the distribution of a single ordinal variable, highlighting the frequency of each ordered category.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create sample Likert scale data for one survey question\nlikert_data &lt;- data.frame(\n  response_category = factor(\n    c(\"Strongly Disagree\", \"Disagree\", \n      \"Neutral\", \"Agree\", \"Strongly Agree\"),\n    levels = c(\"Strongly Disagree\", \"Disagree\", \n               \"Neutral\", \"Agree\", \"Strongly Agree\")\n  ),\n  frequency = c(15, 27, 43, 85, 30)\n)\n\n# Create horizontal bar chart with properly ordered categories\nggplot(likert_data, aes(x = response_category,\n                        y = frequency, \n                        fill = response_category)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\n    \"Strongly Disagree\" = \"#d7191c\",\n    \"Disagree\" = \"#fdae61\",\n    \"Neutral\" = \"#ffffbf\",\n    \"Agree\" = \"#abd9e9\",\n    \"Strongly Agree\" = \"#2c7bb6\"\n  )) +\n  coord_flip() +  # Horizontal orientation for better label readability\n  theme_minimal() +\n  labs(\n    title = \"Responses to: 'The new software \n    interface is intuitive to use'\",\n    subtitle = \"Distribution of 200 employee responses\",\n    x = \"\",\n    y = \"Number of Responses\"\n  ) +\n  theme(\n    legend.position = \"none\",  # Remove legend as colors are self-explanatory\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank()  # Remove horizontal grid lines\n  )\n\n\n\n\n\n\n\nStacked Bar Charts\nShow multiple ordinal categories within a single bar, with each segment representing a different category stacked on top of another. They are useful for comparing the distribution of ordinal data across different groups or conditions. They can be displayed as counts or as percentages (where each bar totals 100%).\nStacked bar charts allow comparison of both total amounts within each group and the proportion of each ordinal category within those groups, providing insights into how distributions differ between categories.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Create sample semantic differential scale data\n# This represents evaluations of three different smartphones on five dimensions\nsemantic_data &lt;- data.frame(\n  product = rep(c(\"Smartphone A\", \"Smartphone B\", \"Smartphone C\"), each = 5),\n  dimension = rep(c(\"Ineffective - Effective\",\n                    \"Complicated - Simple\",\n                    \"Unreliable - Reliable\",\n                    \"Outdated - Innovative\",\n                    \"Unattractive - Attractive\"), 3),\n  rating_1 = c(5, 8, 3, 10, 7,       # Smartphone A\n               2, 5, 2, 3, 4,        # Smartphone B\n               8, 6, 4, 5, 3),       # Smartphone C\n  rating_2 = c(10, 12, 15, 8, 13,    # Smartphone A\n               8, 10, 7, 9, 11,      # Smartphone B\n               12, 8, 9, 10, 7),     # Smartphone C\n  rating_3 = c(25, 20, 22, 18, 20,   # Smartphone A\n               15, 20, 18, 13, 15,   # Smartphone B\n               20, 22, 18, 15, 20),  # Smartphone C\n  rating_4 = c(35, 30, 32, 40, 35,   # Smartphone A\n               45, 35, 40, 38, 35,   # Smartphone B\n               30, 35, 38, 40, 35),  # Smartphone C\n  rating_5 = c(25, 30, 28, 24, 25,   # Smartphone A\n               30, 30, 33, 37, 35,   # Smartphone B\n               30, 29, 31, 30, 35)   # Smartphone C\n)\n\n# Reshape data for ggplot\nsemantic_long &lt;- semantic_data %&gt;%\n  pivot_longer(cols = starts_with(\"rating_\"),\n               names_to = \"rating_level\",\n               values_to = \"count\") %&gt;%\n  mutate(\n    rating_number = as.numeric(substr(rating_level, 8, 8)),\n    rating_label = factor(\n      case_when(\n        rating_number == 1 ~ \"1 (Negative)\",\n        rating_number == 2 ~ \"2\",\n        rating_number == 3 ~ \"3 (Neutral)\",\n        rating_number == 4 ~ \"4\",\n        rating_number == 5 ~ \"5 (Positive)\"\n      ),\n      levels = c(\"1 (Negative)\", \"2\", \"3 (Neutral)\", \"4\", \"5 (Positive)\")\n    )\n  )\n\n# Calculate percentages for each product-dimension combination\nsemantic_pct &lt;- semantic_long %&gt;%\n  group_by(product, dimension) %&gt;%\n  mutate(\n    percentage = count / sum(count) * 100,\n    total = sum(count)\n  ) %&gt;%\n  ungroup()\n\n# Create stacked bar chart\nggplot(semantic_pct, aes(x = dimension, y = percentage, fill = rating_label)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ product) +\n  scale_fill_manual(values = c(\"1 (Negative)\" = \"#d7191c\",\n                               \"2\" = \"#fdae61\",\n                               \"3 (Neutral)\" = \"#ffffbf\",\n                               \"4\" = \"#a6d96a\",\n                               \"5 (Positive)\" = \"#1a9641\")) +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Smartphone Evaluations using Semantic Differential Scales\",\n       subtitle = \"Distribution of ratings across five dimensions\",\n       x = \"\",\n       y = \"Percentage of Responses\",\n       fill = \"Rating\") +\n  theme(\n    legend.position = \"bottom\",\n    strip.background = element_rect(fill = \"lightblue\", color = NA),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\nThis visualization effectively reveals patterns such as which smartphone is perceived as more innovative, which has the most consistent ratings across dimensions, and where the greatest differences between products exist. These insights would be difficult to discern from tables of raw data. The stacked bar format is particularly effective for semantic differential scales because it shows the full distribution of responses, not just averages, allowing you to see whether opinions are polarized or consistent across respondents.\n\n\nDivergent Stacked Bar Charts\nSpecifically designed to visualize ordinal data with a neutral central category or bipolar responses, such as Likert scales and semantic differentials. Segments representing responses on one side of the neutral point extend in one direction, while segments representing responses on the other side extend in the opposite direction from a central baseline. They effectively illustrate the balance between positive and negative responses and the distribution of opinions.\nDivergent stacked bar charts are the recommended visualization for Likert-type scales as they clearly show the proportion of responses in each category and the overall tendency of agreement or disagreement.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Create sample Likert scale data\nlikert_data &lt;- data.frame(\n  question = c(\"The product is easy to use\",\n               \"The customer service was helpful\",\n               \"I would recommend this product to others\"),\n  strongly_disagree = c(5, 10, 8),\n  disagree = c(15, 18, 12),\n  neutral = c(20, 25, 15),\n  agree = c(40, 30, 35),\n  strongly_agree = c(20, 17, 30)\n)\n\n# Reshape data for ggplot\nlikert_long &lt;- likert_data %&gt;%\n  pivot_longer(cols = -question,\n               names_to = \"response\",\n               values_to = \"count\") %&gt;%\n  mutate(response = factor(response,\n                           levels = c(\"strongly_disagree\", \"disagree\", \"neutral\",\n                                     \"agree\", \"strongly_agree\")),\n         response_type = ifelse(response %in% c(\"strongly_disagree\", \"disagree\"),\n                               \"negative\",\n                               ifelse(response == \"neutral\", \"neutral\", \"positive\")),\n         # Negative values for disagreement\n         plot_value = ifelse(response_type == \"negative\", -count, count),\n         # For neutral, split half to each side (as per original code)\n         plot_value = ifelse(response_type == \"neutral\", count/2, plot_value))\n\n# Create divergent stacked bar chart\nggplot(likert_long, aes(x = question, y = plot_value, fill = response)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_fill_manual(values = c(\"strongly_disagree\" = \"darkred\",\n                               \"disagree\" = \"lightcoral\",\n                               \"neutral\" = \"lightgrey\",\n                               \"agree\" = \"lightblue\",\n                               \"strongly_agree\" = \"darkblue\")) +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Responses to Product Survey\",\n       x = \"\",\n       y = \"Count\",\n       fill = \"Response\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nOther Possible Visualizations\nDepending on the specific analytical objective, these alternative visualizations can provide valuable perspectives on ordinal data, particularly when exploring relationships between variables or tracking changes in rankings.\nMosaic plots show the relationship between two or more categorical variables, including ordinal ones, using tiled rectangles whose area is proportional to the frequency of each combination of categories.\n\n\nCode\n# install.packages(\"ggmosaic\")\nlibrary(ggplot2)\nlibrary(ggmosaic)\nlibrary(dplyr)\n\n# Create sample data for education level (ordinal) \n#and job satisfaction (ordinal)\nset.seed(123)\nn &lt;- 500\n\neducation_levels &lt;- c(\"High School\", \"Associate's\", \"Bachelor's\",\n                      \"Master's\", \"Doctorate\")\nsatisfaction_levels &lt;- c(\"Very Dissatisfied\", \"Dissatisfied\",\n                         \"Neutral\", \"Satisfied\", \"Very Satisfied\")\n\n# Create sample data with a pattern \n#(higher education tends to correlate with higher satisfaction)\nmosaic_data &lt;- data.frame(\n  education = factor(sample(education_levels, n, replace = TRUE,\n                            prob = c(0.3, 0.25, 0.25, 0.15, 0.05)),\n                     levels = education_levels),\n  satisfaction = factor(NA, levels = satisfaction_levels)\n)\n\n# Generate satisfaction levels with \n#some correlation to education\nfor (i in 1:n) {\n  # Higher education levels tend to have higher satisfaction probabilities\n  edu_level &lt;- which(education_levels == mosaic_data$education[i])\n\n  # Adjust probabilities based on education level\n  probs &lt;- c(0.25, 0.25, 0.2, 0.2, 0.1)  # Base probabilities\n\n  # Shift probabilities based on education level\n  shift &lt;- (edu_level - 3) * 0.05  # Shift factor based on education\n\n  # Adjust probabilities (higher education gets \n  #more weight for higher satisfaction)\n  adjusted_probs &lt;- probs + c(-0.1, -0.05, 0, 0.05, 0.1) * edu_level\n\n  # Ensure probabilities are valid\n  adjusted_probs &lt;- pmax(adjusted_probs, 0.01)\n  adjusted_probs &lt;- adjusted_probs / sum(adjusted_probs)\n\n  mosaic_data$satisfaction[i] &lt;- sample(satisfaction_levels, 1, prob = adjusted_probs)\n}\n\n# Create mosaic plot\nggplot(data = mosaic_data) +\n  geom_mosaic(aes(x = product(education), \n                  fill = satisfaction)) +\n  scale_fill_brewer(palette = \"RdYlGn\", direction = 1) +\n  labs(title = \"Relationship Between \n       Education Level and Job Satisfaction\",\n       subtitle = \"Mosaic plot showing the \n       distribution of satisfaction within \n       each education level\",\n       x = \"Education Level\",\n       y = \"Job Satisfaction\",\n       fill = \"Satisfaction Level\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n         legend.position = \"bottom\")\n\n\n\n\n\nThis mosaic plot visualizes the relationship between two ordinal variables: education level and job satisfaction. The width of each column represents the proportion of respondents with that education level in the overall sample. Within each education level column, the height of each colored section represents the proportion of respondents reporting that satisfaction level.\nLine charts (bump charts) visualize the change in rank of different items over time or between categories, emphasizing movement in relative positions.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Create sample data for product rankings over time\nrankings &lt;- data.frame(\n  product = rep(c(\"Product A\", \"Product B\", \n                  \"Product C\", \"Product D\", \"Product E\"), 4),\n  quarter = rep(c(\"Q1 2024\", \"Q2 2024\", \n                  \"Q3 2024\", \"Q4 2024\"), each = 5),\n  rank = c(1, 2, 3, 4, 5,       # Q1 rankings\n           1, 3, 2, 5, 4,       # Q2 rankings\n           2, 1, 3, 5, 4,       # Q3 rankings\n           3, 1, 2, 4, 5)       # Q4 rankings\n)\n\n# Create bump chart\nggplot(rankings, aes(x = quarter, y = rank, \n                     group = product, color = product)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 4) +\n  scale_y_reverse(breaks = 1:5) +  # Reverse Y-axis so \n                                   # rank 1 is at the top\n  theme_minimal() +\n  labs(title = \"Product Rankings by Quarter\",\n       subtitle = \"Showing changes in ranking \n       position over time\",\n       x = \"Quarter\",\n       y = \"Rank (Lower is Better)\",\n       color = \"Product\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#other-possible-visualizations",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#other-possible-visualizations",
    "title": "Ordinal Data Analysis in R",
    "section": "Other Possible Visualizations",
    "text": "Other Possible Visualizations\nDepending on the specific analytical objective, these alternative visualizations can provide valuable perspectives on ordinal data, particularly when exploring relationships between variables or tracking changes in rankings.\nMosaic plots show the relationship between two or more categorical variables, including ordinal ones, using tiled rectangles whose area is proportional to the frequency of each combination of categories.\n\n\nCode\n# install.packages(\"ggmosaic\")\nlibrary(ggplot2)\nlibrary(ggmosaic)\nlibrary(dplyr)\n\n# Create sample data for education level (ordinal) and job satisfaction (ordinal)\nset.seed(123)\nn &lt;- 500\n\neducation_levels &lt;- c(\"High School\", \"Associate's\", \"Bachelor's\", \"Master's\", \"Doctorate\")\nsatisfaction_levels &lt;- c(\"Very Dissatisfied\", \"Dissatisfied\", \"Neutral\", \"Satisfied\", \"Very Satisfied\")\n\n# Create sample data with a pattern (higher education tends to correlate with higher satisfaction)\nmosaic_data &lt;- data.frame(\n  education = factor(sample(education_levels, n, replace = TRUE, \n                           prob = c(0.3, 0.25, 0.25, 0.15, 0.05)),\n                    levels = education_levels),\n  satisfaction = factor(NA, levels = satisfaction_levels)\n)\n\n# Generate satisfaction levels with some correlation to education\nfor (i in 1:n) {\n  # Higher education levels tend to have higher satisfaction probabilities\n  edu_level &lt;- which(education_levels == mosaic_data$education[i])\n  \n  # Adjust probabilities based on education level\n  probs &lt;- c(0.25, 0.25, 0.2, 0.2, 0.1)  # Base probabilities\n  \n  # Shift probabilities based on education level\n  shift &lt;- (edu_level - 3) * 0.05  # Shift factor based on education\n  \n  # Adjust probabilities (higher education gets more weight for higher satisfaction)\n  adjusted_probs &lt;- probs + c(-0.1, -0.05, 0, 0.05, 0.1) * edu_level\n  \n  # Ensure probabilities are valid\n  adjusted_probs &lt;- pmax(adjusted_probs, 0.01)\n  adjusted_probs &lt;- adjusted_probs / sum(adjusted_probs)\n  \n  mosaic_data$satisfaction[i] &lt;- sample(satisfaction_levels, 1, prob = adjusted_probs)\n}\n\n# Create mosaic plot\nggplot(data = mosaic_data) +\n  geom_mosaic(aes(x = product(education), fill = satisfaction)) +\n  scale_fill_brewer(palette = \"RdYlGn\", direction = 1) +\n  labs(title = \"Relationship Between Education Level and Job Satisfaction\",\n       subtitle = \"Mosaic plot showing the distribution of satisfaction within each education level\",\n       x = \"Education Level\",\n       y = \"Job Satisfaction\",\n       fill = \"Satisfaction Level\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\n\nThis mosaic plot visualizes the relationship between two ordinal variables: education level and job satisfaction. The width of each column represents the proportion of respondents with that education level in the overall sample. Within each education level column, the height of each colored section represents the proportion of respondents reporting that satisfaction level.\nLine charts (bump charts) visualize the change in rank of different items over time or between categories, emphasizing movement in relative positions.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Create sample data for product rankings over time\nrankings &lt;- data.frame(\n  product = rep(c(\"Product A\", \"Product B\", \"Product C\", \"Product D\", \"Product E\"), 4),\n  quarter = rep(c(\"Q1 2024\", \"Q2 2024\", \"Q3 2024\", \"Q4 2024\"), each = 5),\n  rank = c(1, 2, 3, 4, 5,       # Q1 rankings\n           1, 3, 2, 5, 4,       # Q2 rankings\n           2, 1, 3, 5, 4,       # Q3 rankings\n           3, 1, 2, 4, 5)       # Q4 rankings\n)\n\n# Create bump chart\nggplot(rankings, aes(x = quarter, y = rank, group = product, color = product)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 4) +\n  scale_y_reverse(breaks = 1:5) +  # Reverse Y-axis so rank 1 is at the top\n  theme_minimal() +\n  labs(title = \"Product Rankings by Quarter\",\n       subtitle = \"Showing changes in ranking position over time\",\n       x = \"Quarter\",\n       y = \"Rank (Lower is Better)\",\n       color = \"Product\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#visualizing-ordinal-data",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#visualizing-ordinal-data",
    "title": "Ordinal Data Analysis in R",
    "section": "1.3 Visualizing Ordinal Data",
    "text": "1.3 Visualizing Ordinal Data\nThe most important principle in visualizing ordinal data is to always represent ordinal categories in their natural, ordered sequence in any visual representation. In bar charts, bars should be arranged along the axis based on the logical order of the ordinal scale (e.g., from “Low” to “High”). For stacked and divergent bar charts, the segments representing ordinal categories should also follow this intrinsic order within each bar.\n\n\nCode\nlibrary(ggplot2)\n\n# Create sample data\nsatisfaction &lt;- data.frame(\n  level = factor(c(\"Very Dissatisfied\", \"Dissatisfied\", \"Neutral\", \"Satisfied\", \"Very Satisfied\"),\n                 levels = c(\"Very Dissatisfied\", \"Dissatisfied\", \"Neutral\", \"Satisfied\", \"Very Satisfied\")),\n  count = c(15, 23, 30, 45, 27)\n)\n\n# Create bar chart with ordered categories\nggplot(satisfaction, aes(x = level, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(title = \"Customer Satisfaction Levels\",\n       x = \"Satisfaction Level\",\n       y = \"Number of Responses\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nThe choice of chart should align with the research question and the specific aspect of ordinal data being investigated. Not all chart types are equally effective for representing ordered categorical data.\n\nBar Charts\nRepresent each ordinal category with a bar, whose height or length corresponds to the frequency or count of that category. Fundamentally, the bars must be arranged in the logical order of the ordinal variable (e.g., from lowest to highest category). They can be vertical or horizontal; horizontal orientation is often preferred for readability of long category labels.\nBar charts provide a clear and easily understandable visualization of the distribution of a single ordinal variable, highlighting the frequency of each ordered category.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create sample Likert scale data for one survey question\nlikert_data &lt;- data.frame(\n  response_category = factor(\n    c(\"Strongly Disagree\", \"Disagree\", \n      \"Neutral\", \"Agree\", \"Strongly Agree\"),\n    levels = c(\"Strongly Disagree\", \"Disagree\", \n               \"Neutral\", \"Agree\", \"Strongly Agree\")\n  ),\n  frequency = c(15, 27, 43, 85, 30)\n)\n\n# Create horizontal bar chart with properly ordered categories\nggplot(likert_data, aes(x = response_category,\n                        y = frequency, \n                        fill = response_category)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\n    \"Strongly Disagree\" = \"#d7191c\",\n    \"Disagree\" = \"#fdae61\",\n    \"Neutral\" = \"#ffffbf\",\n    \"Agree\" = \"#abd9e9\",\n    \"Strongly Agree\" = \"#2c7bb6\"\n  )) +\n  coord_flip() +  # Horizontal orientation for better label readability\n  theme_minimal() +\n  labs(\n    title = \"Responses to: 'The new software \n    interface is intuitive to use'\",\n    subtitle = \"Distribution of 200 employee responses\",\n    x = \"\",\n    y = \"Number of Responses\"\n  ) +\n  theme(\n    legend.position = \"none\",  # Remove legend as colors are self-explanatory\n    axis.text.y = element_text(size = 12),\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.major.y = element_blank()  # Remove horizontal grid lines\n  )\n\n\n\n\n\n\n\nStacked Bar Charts\nShow multiple ordinal categories within a single bar, with each segment representing a different category stacked on top of another. They are useful for comparing the distribution of ordinal data across different groups or conditions. They can be displayed as counts or as percentages (where each bar totals 100%).\nStacked bar charts allow comparison of both total amounts within each group and the proportion of each ordinal category within those groups, providing insights into how distributions differ between categories.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Create sample semantic differential scale data\n# This represents evaluations of three different smartphones on five dimensions\nsemantic_data &lt;- data.frame(\n  product = rep(c(\"Smartphone A\", \"Smartphone B\", \"Smartphone C\"), each = 5),\n  dimension = rep(c(\"Ineffective - Effective\",\n                    \"Complicated - Simple\",\n                    \"Unreliable - Reliable\",\n                    \"Outdated - Innovative\",\n                    \"Unattractive - Attractive\"), 3),\n  rating_1 = c(5, 8, 3, 10, 7,       # Smartphone A\n               2, 5, 2, 3, 4,        # Smartphone B\n               8, 6, 4, 5, 3),       # Smartphone C\n  rating_2 = c(10, 12, 15, 8, 13,    # Smartphone A\n               8, 10, 7, 9, 11,      # Smartphone B\n               12, 8, 9, 10, 7),     # Smartphone C\n  rating_3 = c(25, 20, 22, 18, 20,   # Smartphone A\n               15, 20, 18, 13, 15,   # Smartphone B\n               20, 22, 18, 15, 20),  # Smartphone C\n  rating_4 = c(35, 30, 32, 40, 35,   # Smartphone A\n               45, 35, 40, 38, 35,   # Smartphone B\n               30, 35, 38, 40, 35),  # Smartphone C\n  rating_5 = c(25, 30, 28, 24, 25,   # Smartphone A\n               30, 30, 33, 37, 35,   # Smartphone B\n               30, 29, 31, 30, 35)   # Smartphone C\n)\n\n# Reshape data for ggplot\nsemantic_long &lt;- semantic_data %&gt;%\n  pivot_longer(cols = starts_with(\"rating_\"),\n               names_to = \"rating_level\",\n               values_to = \"count\") %&gt;%\n  mutate(\n    rating_number = as.numeric(substr(rating_level, 8, 8)),\n    rating_label = factor(\n      case_when(\n        rating_number == 1 ~ \"1 (Negative)\",\n        rating_number == 2 ~ \"2\",\n        rating_number == 3 ~ \"3 (Neutral)\",\n        rating_number == 4 ~ \"4\",\n        rating_number == 5 ~ \"5 (Positive)\"\n      ),\n      levels = c(\"1 (Negative)\", \"2\", \"3 (Neutral)\", \"4\", \"5 (Positive)\")\n    )\n  )\n\n# Calculate percentages for each product-dimension combination\nsemantic_pct &lt;- semantic_long %&gt;%\n  group_by(product, dimension) %&gt;%\n  mutate(\n    percentage = count / sum(count) * 100,\n    total = sum(count)\n  ) %&gt;%\n  ungroup()\n\n# Create stacked bar chart\nggplot(semantic_pct, aes(x = dimension, y = percentage, fill = rating_label)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ product) +\n  scale_fill_manual(values = c(\"1 (Negative)\" = \"#d7191c\",\n                               \"2\" = \"#fdae61\",\n                               \"3 (Neutral)\" = \"#ffffbf\",\n                               \"4\" = \"#a6d96a\",\n                               \"5 (Positive)\" = \"#1a9641\")) +\n  coord_flip() +\n  theme_minimal() +\n  labs(title = \"Smartphone Evaluations using Semantic Differential Scales\",\n       subtitle = \"Distribution of ratings across five dimensions\",\n       x = \"\",\n       y = \"Percentage of Responses\",\n       fill = \"Rating\") +\n  theme(\n    legend.position = \"bottom\",\n    strip.background = element_rect(fill = \"lightblue\", color = NA),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\nThis visualization effectively reveals patterns such as which smartphone is perceived as more innovative, which has the most consistent ratings across dimensions, and where the greatest differences between products exist. These insights would be difficult to discern from tables of raw data. The stacked bar format is particularly effective for semantic differential scales because it shows the full distribution of responses, not just averages, allowing you to see whether opinions are polarized or consistent across respondents.\n\n\nDivergent Stacked Bar Charts\nSpecifically designed to visualize ordinal data with a neutral central category or bipolar responses, such as Likert scales and semantic differentials. Segments representing responses on one side of the neutral point extend in one direction, while segments representing responses on the other side extend in the opposite direction from a central baseline. They effectively illustrate the balance between positive and negative responses and the distribution of opinions.\nDivergent stacked bar charts are the recommended visualization for Likert-type scales as they clearly show the proportion of responses in each category and the overall tendency of agreement or disagreement.\n\n\nCode\n# Install and load required package\n# install.packages(\"HH\")\nlibrary(HH)\n\n# Frequency data for three items (rows) on a 5-point Likert scale\nlikert_data &lt;- data.frame(\n  \"Easy\" = c(5, 15, 20, 40, 20),\n  \"Helpful\" = c(10, 18, 25, 30, 17),\n  \"Recommend\" = c(8, 12, 15, 35, 30)\n)\n\n# Set column names (Likert scale labels)\nrownames(likert_data) &lt;- c(\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\")\nlikert_data &lt;- t(likert_data)  # Transpose: items as columns, scale points as rows\n\n# Create the Likert plot\nlikert_plot &lt;- likert(likert_data,\n                      main = \"Customer Feedback on Product Experience\",\n                      xlab = \"Percentage of Responses\",\n                      ylab = NULL,\n                      positive.order = TRUE,\n                      reference = 0)\n\n# Display the plot\nprint(likert_plot)\n\n\n\n\n\n\n\nOther Possible Visualizations\nDepending on the specific analytical objective, these alternative visualizations can provide valuable perspectives on ordinal data, particularly when exploring relationships between variables or tracking changes in rankings.\nMosaic plots show the relationship between two or more categorical variables, including ordinal ones, using tiled rectangles whose area is proportional to the frequency of each combination of categories.\n\n\nCode\n# install.packages(\"ggmosaic\")\nlibrary(ggplot2)\nlibrary(ggmosaic)\nlibrary(dplyr)\n\n# Create sample data for education level (ordinal) \n#and job satisfaction (ordinal)\nset.seed(123)\nn &lt;- 500\n\neducation_levels &lt;- c(\"High School\", \"Associate's\", \"Bachelor's\",\n                      \"Master's\", \"Doctorate\")\nsatisfaction_levels &lt;- c(\"Very Dissatisfied\", \"Dissatisfied\",\n                         \"Neutral\", \"Satisfied\", \"Very Satisfied\")\n\n# Create sample data with a pattern \n#(higher education tends to correlate with higher satisfaction)\nmosaic_data &lt;- data.frame(\n  education = factor(sample(education_levels, n, replace = TRUE,\n                            prob = c(0.3, 0.25, 0.25, 0.15, 0.05)),\n                     levels = education_levels),\n  satisfaction = factor(NA, levels = satisfaction_levels)\n)\n\n# Generate satisfaction levels with \n#some correlation to education\nfor (i in 1:n) {\n  # Higher education levels tend to have higher satisfaction probabilities\n  edu_level &lt;- which(education_levels == mosaic_data$education[i])\n\n  # Adjust probabilities based on education level\n  probs &lt;- c(0.25, 0.25, 0.2, 0.2, 0.1)  # Base probabilities\n\n  # Shift probabilities based on education level\n  shift &lt;- (edu_level - 3) * 0.05  # Shift factor based on education\n\n  # Adjust probabilities (higher education gets \n  #more weight for higher satisfaction)\n  adjusted_probs &lt;- probs + c(-0.1, -0.05, 0, 0.05, 0.1) * edu_level\n\n  # Ensure probabilities are valid\n  adjusted_probs &lt;- pmax(adjusted_probs, 0.01)\n  adjusted_probs &lt;- adjusted_probs / sum(adjusted_probs)\n\n  mosaic_data$satisfaction[i] &lt;- sample(satisfaction_levels, 1, prob = adjusted_probs)\n}\n\n# Create mosaic plot\nggplot(data = mosaic_data) +\n  geom_mosaic(aes(x = product(education), \n                  fill = satisfaction)) +\n  scale_fill_brewer(palette = \"RdYlGn\", direction = 1) +\n  labs(title = \"Relationship Between \n       Education Level and Job Satisfaction\",\n       subtitle = \"Mosaic plot showing the \n       distribution of satisfaction within \n       each education level\",\n       x = \"Education Level\",\n       y = \"Job Satisfaction\",\n       fill = \"Satisfaction Level\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n         legend.position = \"bottom\")\n\n\n\n\n\nThis mosaic plot visualizes the relationship between two ordinal variables: education level and job satisfaction. The width of each column represents the proportion of respondents with that education level in the overall sample. Within each education level column, the height of each colored section represents the proportion of respondents reporting that satisfaction level.\nLine charts (bump charts) visualize the change in rank of different items over time or between categories, emphasizing movement in relative positions.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Create sample data for product rankings over time\nrankings &lt;- data.frame(\n  product = rep(c(\"Product A\", \"Product B\", \n                  \"Product C\", \"Product D\", \"Product E\"), 4),\n  quarter = rep(c(\"Q1 2024\", \"Q2 2024\", \n                  \"Q3 2024\", \"Q4 2024\"), each = 5),\n  rank = c(1, 2, 3, 4, 5,       # Q1 rankings\n           1, 3, 2, 5, 4,       # Q2 rankings\n           2, 1, 3, 5, 4,       # Q3 rankings\n           3, 1, 2, 4, 5)       # Q4 rankings\n)\n\n# Create bump chart\nggplot(rankings, aes(x = quarter, y = rank, \n                     group = product, color = product)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 4) +\n  scale_y_reverse(breaks = 1:5) +  # Reverse Y-axis so \n                                   # rank 1 is at the top\n  theme_minimal() +\n  labs(title = \"Product Rankings by Quarter\",\n       subtitle = \"Showing changes in ranking \n       position over time\",\n       x = \"Quarter\",\n       y = \"Rank (Lower is Better)\",\n       color = \"Product\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nReferences\n\nDeVellis, R. F., & Thorpe, C. T. (2021). Scale development: Theory and applications. Sage publications."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#cumulative-logit-models",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#cumulative-logit-models",
    "title": "Ordinal Data Analysis in R",
    "section": "2.1 Cumulative Logit Models",
    "text": "2.1 Cumulative Logit Models\nLet \\(R\\) be an ordinal variable with \\(m\\) categories, with realizations \\(r\\), and let \\(\\pi_1 , \\dots, \\pi_m\\) be the probabilities of observing each category. For \\(m\\) outcome categories the cumulative logits are defined as:\n\\[\n\\text{logit} [P(R \\leq r) ]= \\log \\frac{P(R\\leq r)}{1−P(R \\leq r)}\n\\] ::: {#cumulative-logit} \\[\n\\sum_{i=1}^n x_i = \\bar{x} \\cdot n\n\\] ::: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is equivalent to applying the binary logit to the conditional probability of observing response r, given that the outcome is either in category r or r+1.\n\n\nIn this model, the effects of the variables are described by local odds ratios rather than the cumulative odds ratios used in the cumulative logit model.\nTo achieve model parsimony, we can assume that the explanatory variables have similar effects for each logit, allowing us to use a single parameter vector γ instead of m−1 parameter vectors:\n\n\n\nContinuation-Ratio Logits The continuation-ratio logits are particularly useful when the outcome of the response variable is determined by a sequential mechanism, where an observation must potentially occur in category r before it can occur in a higher category. This is relevant, for example, in modeling survival through different age periods.\nContinuation-ratio logits can be defined in two ways depending on the direction of the sequential mechanism. If the mechanism is increasing, the logits are:\n\n\n\n\n\n\n\nThese two formulations are not equivalent, and the most appropriate one should be chosen based on the direction of the sequential mechanism characterizing the ordinal variable.\nModels with covariates can also be defined for continuation-ratio logits, allowing for both specific and equal effects for each logit."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#limitations-of-most-commonly-used-models",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#limitations-of-most-commonly-used-models",
    "title": "Ordinal Data Analysis in R",
    "section": "2.1 Limitations of most commonly used models",
    "text": "2.1 Limitations of most commonly used models\n\nLimitations of Linear Regression:\nLinear regression is designed for dependent variables that are continuous and can take any value within a range (or at least have a large number of distinct, equally-spaced values). Applying linear regression to ordinal data involves treating the ordered categories as if they were numerical scores with equal intervals between them.\n\nIgnores Non-Interval Nature: The primary issue is that linear regression assumes that the difference between category 1 and 2 is the same as the difference between category 2 and 3, and so on. For ordinal data, this is often not true. The “distance” between “Very Dissatisfied” and “Dissatisfied” might not be the same in the minds of respondents as the distance between “Satisfied” and “Very Satisfied.” By assigning numerical scores (e.g., 1, 2, 3, 4, 5) and running linear regression, we impose an arbitrary interval structure that the data doesn’t necessarily possess. This can lead to inaccurate estimates of the effects of predictors.\nViolation of Assumptions: Linear regression assumes the dependent variable is continuous and errors are normally distributed with constant variance. For an ordinal variable with a limited number of categories, these assumptions are violated. The predicted values from a linear model can also fall outside the valid range of the ordinal scale (e.g., predicting a satisfaction level of 0.5 or 5.8 on a 1-5 scale).\nMisleading Interpretation: Interpreting coefficients in linear regression involves saying that a one-unit increase in a predictor is associated with a certain change in the mean score of the ordinal variable. This interpretation is based on the problematic assumption of equal intervals and might not accurately reflect the underlying process generating the ordinal response.\n\n\n\nLimitations of Binary Logistic Regression:\nBinary logistic regression is suitable for dependent variables with exactly two outcomes (e.g., Yes/No, Success/Failure). To use it with an ordinal variable, you have to collapse the multiple ordered categories into just two.\n\nLoss of Information: The biggest drawback is the loss of valuable information about the granularity and ordering of the original categories. Forcing a 5-point scale into a binary outcome (e.g., “Satisfied/Very Satisfied” vs. “Dissatisfied/Neutral/Very Dissatisfied”) discards the nuances within the original categories. A model that can distinguish between “Dissatisfied” and “Very Dissatisfied” will likely be more informative than one that groups them.\nArbitrary Threshold: The choice of where to split the ordinal scale into two groups is often arbitrary. Different researchers might choose different cut-off points, and this arbitrary choice can significantly influence the results and conclusions drawn from the analysis. The effect of a predictor might appear different depending on how the dichotomization is performed.\nReduced Statistical Power: By reducing the number of outcomes, you potentially reduce the variability captured by the dependent variable, which can lead to a loss of statistical power to detect significant effects of your predictors compared to a model that utilizes the full ordinal scale.\n\n\n\nLimitations of Multinomial Logistic Regression:\nMultinomial (or polytomous) logistic regression is designed for dependent variables with three or more categories that have no natural order (e.g., choice of car color: red, blue, green). While it can handle multiple categories, its fundamental structure doesn’t account for ranking.\n\nIgnores the Order: Multinomial logistic regression models the probability of being in each category relative to a chosen baseline category. It estimates a separate set of coefficients for each category comparison (e.g., Category 2 vs. Category 1, Category 3 vs. Category 1, etc.). It treats the categories as distinct nominal outcomes, completely ignoring the fact that category 3 falls between category 2 and category 4 in a meaningful way.\nDifficult Interpretation (in terms of Order): The coefficients in a multinomial logit model are interpreted in terms of the change in log-odds of being in a specific category versus the baseline category for a one-unit change in a predictor. While technically correct, relating these separate category-specific effects back to the overall ordered nature of the dependent variable can be cumbersome and less intuitive than the single cumulative odds ratio provided by the cumulative logit model (when the proportional odds assumption holds). It doesn’t directly answer questions like “how does this predictor affect the likelihood of being in a higher category?”"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#modeling-cumulative-probabilities",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#modeling-cumulative-probabilities",
    "title": "Ordinal Data Analysis in R",
    "section": "2.2 Modeling Cumulative Probabilities",
    "text": "2.2 Modeling Cumulative Probabilities\nThe primary methodology for modeling ordinal data revolves around the cumulative probabilities associated with the ordered categories. This approach respects the inherent order of the data, ensuring that these probabilities monotonically increase as we move up the ordinal scale.\n\n\nTo more appropriately handle ordinality, the cumulative probabilities approach modifies logistic regression by applying transformations that consider the order of the categories. A common transformation is the logit transformation applied to the cumulative probabilities, which enhances the model’s ability to capture the ordered nature of the data. Other transformations, such as probit or log-log, can also be used depending on the specific data characteristics and analytical requirements.\n\n\n\n\n\n\nDefinition: Given an ordinal variable \\(R\\) with \\(m\\) ordered categories, let’s denote these categories as \\(r_1, r_2, \\dots, r_m\\), where \\(r_1\\) is the “lowest” category and \\(r_m\\) is the “highest”. The categories have a meaningful order: \\(r_1 \\leq r_2, \\leq \\dots, \\leq r_m\\).\nA cumulative probability for a specific category \\(r_j\\) is the probability that the observed response \\(R\\) falls into category \\(r_j\\), or any category below it. Mathematically, this is expressed as \\(P(R \\leq r_j)\\).\nWe can define \\(m-1\\) such cumulative probabilities, corresponding to the thresholds between the categories:\n\nFor the first category \\(r_1\\):\n\\(P(R \\leq r_1) = P(R = r_1)\\) is simply the probability of being in the lowest category.\nFor the first category \\(r_2\\):\n\\(P(R \\leq r_2) = P(R = r_1) + P(R = r_2)\\) is the probability of being in the second category or any category below it (which is just the first category).\nFor the third category \\(r_3\\):\n\\(P(R \\leq r_3) = P(R = r_1) + P(R = r_2) + P(R = r_3)\\) is the probability of being in the third category or any category below it.\n…and so on, up to the \\((m-1)\\)-th category \\(r_{(m-1)}\\):\n\\(P(R \\leq r_{(m-1)}) = P(R = r_1) + P(R = r_2) + P(R = r_3) + \\dots + P(R = r_{(m-1)})\\) is the probability of being in the second-highest category or any category below it.\nFor the last category \\(r_m\\) \\(r_{(m-1)}\\):\n\\(P(R \\leq r_m) = P(R = r_1) + P(R = r_2) + P(R = r_3) + \\dots + P(R = r_{(m-1)})+ P(R = r_m)=1\\), which is the cumulative probability is always 1 because it includes all possible outcomes. Since it carries no information about the differences between categories, it is not included in the modeling process; we only model the first \\(m-1\\) cumulative probabilities.\n\nThe use of cumulative probabilities is a clever way to turn the ordinal modeling problem into a series of binary comparisons, while respecting the order. Each cumulative probability \\(P(R \\leq r_j)\\) inherently creates a binary split at the threshold \\(r_j\\): - Outcome 1: the response is in category \\(r_j\\) or lower \\((R\\leq r_j)\\); - Outcome 2: The response is in category higher than \\(r_j\\) \\((R &gt; r_j)\\).\nBy modeling the probability of this binary outcome for each threshold \\(j=1,\\dots,m−1\\), we capture the transitions between categories along the ordered scale. The cumulative logit model then applies the logit transformation to these cumulative probabilities, allowing them to be related to a linear combination of predictors.\nNumerical Example: Let’s consider a simple ordinal variable, “Product Satisfaction,” with \\(m = 4\\) ordered categories:\n\n\\(r_1\\): Very Dissatisfied (VD)\n\\(r_2\\): Dissatisfied (D)\n\\(r_3\\): Satisfied (S)\n\\(r_4\\): Very Satisfied (VS)\n\nSuppose, for a particular group of individuals, the probabilities of being in each specific category are:\n\n\\(P(R=VD)=P(R=r_1)=0.10\\)\n\\(P(R=D)=P(R=r_2)=0.20\\)\n\\(P(R=S)=P(R=r_3)=0.40\\)\n\\(P(R=VS)=P(R=r_4)=0.30\\)\n\nNow, let’s calculate the cumulative probabilities:\n\nCumulative Probability for \\(r_1\\) (VD)  \\(P(R \\leq r_1) = P(R = VD) = 0.10\\)  This represents the probability of being in the “Very Dissatisfied” category or below (just VD). The implied binary split is \\(\\{VD\\}\\) vs \\(\\{D, S, VS\\}\\).\nCumulative Probability for \\(r_2\\) (D)  \\(P(R \\leq r_2) = P(R = VD) + P(R = D) = 0.10 + 0.20 = 0.30\\)  This represents the probability of being in the “Dissatisfied” category or below (VD or D). The implied binary split is \\(\\{VD, D\\}\\) vs \\(\\{S, VS\\}\\).\nCumulative Probability for \\(r_3\\) (S)  \\(P(R \\leq r_3) = P(R = VD) + P(R = D) + P(R = S) = 0.10 + 0.20 + 0.40 = 0.70\\)  This represents the probability of being in the “Satisfied” category or below (VD, D, or S). The implied binary split is \\(\\{VD, D, S\\}\\) vs \\(\\{VS\\}\\).\nCumulative Probability for \\(r_4\\) (VS)  \\(P(R \\leq r_4) = P(R = VD) + P(R = D) + P(R = S) + P(R = VS) = 0.10 + 0.20 + 0.40 + 0.3 = 1\\) As expected, the cumulative probability for the highest category is 1. We do not model this.\n\nSo, for this 4-category variable, the cumulative logit model will focus on modeling the relationships between predictors and the first \\(m−1=4−1=3\\) cumulative probabilities. Each of these represents a different threshold or cut-point on the ordered scale."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#the-cumulative-logit",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#the-cumulative-logit",
    "title": "Ordinal Data Analysis in R",
    "section": "2.3 The Cumulative Logit",
    "text": "2.3 The Cumulative Logit\nTo be able to model the relationship between the cumulative probabilities and the explanatory variables, a function is needed and, as in the binary logistic regression, we use the logit to model the probability of success. In the case of ordinal data, we apply the logit not to the probability of a single category, but to the cumulative probabilities.\nThe cumulative logit transformation for the \\(j\\)-th threshold (where \\(j\\) goes from \\(1\\) to \\(m−1\\)) is defined as the natural logarithm of the cumulative odds:\n\\[\n\\text{logit} [ P (R \\leq r_j)] = \\log \\Bigg( \\frac{P(R \\leq r_j)}{1-P(R \\leq r_j)}\\Bigg)\n\\tag{1}\\]\nSince \\(1-P(R \\leq r_j)\\) is the probability that the outcome \\(R\\) is greather than the category \\(r_j\\). So, the cumulative logit can be rewritten as:\n\\[\n\\text{logit} [ P (R \\leq r_j)] = \\log \\Bigg( \\frac{P(R \\leq r_j)}{P(R &gt; r_j)}\\Bigg)\n\\tag{2}\\]\nThis expression represents the natural logarithm of the odds of being in category \\(r_j\\) or any category below it, versus being in any category above \\(r_j\\). This transformation, as in the case of the binary logistic regreesion, maps probabilities (which are between 0 and 1) onto the entire real number line \\((-\\infty, + \\infty)\\). This allows us to equate the logit of this cumulative probability to a linear combination of our predictors, which can take any real value."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#the-cumulative-logit-with-proportional-odds-assumption",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#the-cumulative-logit-with-proportional-odds-assumption",
    "title": "Ordinal Data Analysis in R",
    "section": "2.3 The Cumulative Logit with Proportional Odds Assumption",
    "text": "2.3 The Cumulative Logit with Proportional Odds Assumption\nTo be able to model the relationship between the cumulative probabilities and the explanatory variables, a function is needed and, as in the binary logistic regression, we use the logit to model the probability of success. In the case of ordinal data, we apply the logit not to the probability of a single category, but to the cumulative probabilities.\nThe cumulative logit transformation for the \\(j\\)-th threshold (where \\(j\\) goes from \\(1\\) to \\(m−1\\)) is defined as the natural logarithm of the cumulative odds:\n\\[\n\\text{logit} [ P (R \\leq r_j)] = \\log \\Bigg( \\frac{P(R \\leq r_j)}{1-P(R \\leq r_j)}\\Bigg)\n\\tag{1}\\]\nSince \\(1-P(R \\leq r_j)\\) is the probability that the outcome \\(R\\) is greather than the category \\(r_j\\). So, the cumulative logit can be rewritten as:\n\\[\n\\text{logit} [ P (R \\leq r_j)] = \\log \\Bigg( \\frac{P(R \\leq r_j)}{P(R &gt; r_j)}\\Bigg)\n\\tag{2}\\]\nThis expression represents the natural logarithm of the odds of being in category \\(r_j\\) or any category below it, versus being in any category above \\(r_j\\). This transformation, as in the case of the binary logistic regression, maps probabilities (which are between 0 and 1) onto the entire real number line \\((-\\infty, + \\infty)\\). This allows us to equate the logit of this cumulative probability to a linear combination of our predictors, which can take any real value.\nThe resulting statistical model, and the most common one for ordinal data with a cumulative logit link, is known as the Cumulative Logit Proportional Odds Model. Its basic structure assumes that the cumulative logit for each threshold is a linear function of the predictor variables:\n\\[\n\\text{logit}[P(R\\leq r_j | X_1, \\dots, X_k)] = \\alpha_j + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_kX_k\n\\tag{3}\\]\nThis equation is estimated simultaneously for each of the \\(m-1\\) cumulative thresholds. in this equation:\n\n\\(P(R \\leq r_j | \\boldsymbol{X})\\) is the cumulative probability of being in category \\(r_j\\) or lower, conditional on the values of the predictor variables \\(X_1 ,X_2, \\dots, X_k\\).\n\\(\\alpha_j\\) are the intercepts of the model. A crucial aspect of the cumulative logit model is that there is a different intercept for each of the \\(m-1\\) cumulative logits we are modeling.\n\\(\\alpha_1\\) is the intercept for \\(\\text{logit}[P(R\\leq r_1)]\\), \\(\\alpha_2\\) is the intercept for \\(\\text{logit}[P(R\\leq r_2)]\\), and \\(\\alpha_{(m-1)}\\) is the intercept for \\(\\text{logit}[P(R\\leq r_{(m-1)})]\\).\nThese intercepts represent the baseline cumulative log-odds for their respective threshold when all the predictor variables X are zero.\nIt is necessary that these intercepts are ordered: \\(\\alpha_1 \\leq \\alpha_2 \\leq \\dots \\leq \\alpha_{(m-1)}\\).\nThis ensures that the resulting cumulative probabilities are non-decreasing as j increases.\n\\(\\beta_1, \\beta_2, \\dots, \\beta_k\\) are the coefficients associated to each predictor variables \\(X_1\\), \\(X_2\\), \\(\\dots, X_k\\).\nSince we are operating under proportional odds assumption, in this model these is only one set of \\(\\beta\\) coefficients that applies across all \\(m-1\\) cumulative logit equations.\nThis implies that the effect of each predictor on the cumulative log-odds is the same.\n\nTo demonstrate the proportional odds assumption, let’s consider the cumulative odds for two distinct sets of predictor values: \\(\\boldsymbol{X}^{(1)} = (X_1^{(1)}, \\dots, X_k^{(1)})\\) and \\(\\boldsymbol{X}^{(2)} = (X_1^{(2)}, \\dots, X_k^{(2)})\\).\nThe cumulative odds at threshold \\(r_j\\) for the first set of predictors \\(\\boldsymbol{X}^{(1)}\\) are: \\[\n\\text{Odds}(R \\leq r_j | \\boldsymbol{X}^{(1)}) =\\exp \\Bigg( \\alpha_j + \\sum_{i=1}^{k} \\beta_iX_i^{(1)} \\Bigg)\n\\] Similarly, for the second set of predictors \\(\\boldsymbol{X}^{(2)}\\):\n\\[\n\\text{Odds}(R \\leq r_j | \\boldsymbol{X}^{(2)}) = \\exp \\Bigg( \\alpha_j + \\sum_{i=1}^{k} \\beta_iX_i^{(2)} \\Bigg)\n\\] The Odds Ratio (OR) comparing the cumulative odds at \\(\\boldsymbol{X}^{(2)}\\) to those at \\(\\boldsymbol{X}^{(1)}\\) for the event \\(R \\leq r_j\\) is: \\[\n\\text{OR}_j = \\frac{\\text{Odds}(R \\leq r_j | \\boldsymbol{X}^{(2)})}{\\text{Odds}(R \\leq r_j | \\boldsymbol{X}^{(1)})}\n\\] Substituting the exponential expressions for the odds: \\[\n\\text{OR}_j = \\frac{\\exp \\Big( \\alpha_j + \\sum_{i=1}^{k} \\beta_iX_i^{(2)} \\Big)}{\\exp \\Big( \\alpha_j + \\sum_{i=1}^{k} \\beta_iX_i^{(1)} \\Big)}\n\\] Using the property \\(e^a / e^b = e^{a-b}\\), we can simplify the expression: \\[\n\\text{OR}_j = \\exp \\Bigg( \\Big( \\alpha_j + \\sum_{i=1}^{k} \\beta_iX_i^{(2)} \\Big) - \\Big( \\alpha_j + \\sum_{i=1}^{k} \\beta_iX_i^{(1)} \\Big) \\Bigg)\n\\]\nCrucially, the intercept term \\(\\alpha_j\\) cancels out:\n\\[\n\\text{OR}_j = \\exp \\Bigg( \\sum_{i=1}^{k} \\beta_iX_i^{(2)} - \\sum_{i=1}^{k} \\beta_iX_i^{(1)} \\Bigg)\n\\]\nThis can be further condensed:\n\\[\n\\text{OR}_j = \\exp \\Bigg( \\sum_{i=1}^{k} \\beta_i(X_i^{(2)} - X_i^{(1)}) \\Bigg)\n\\]\nAs evident from this final formula, the Odds Ratio (\\(\\text{OR}_j\\)) does not contain the subscript \\(j\\). This signifies that the odds ratio associated with a change in the predictor variables is constant across all \\(m-1\\) cumulative thresholds. This inherent consistency in the effect of predictors across the ordinal categories is precisely what defines the proportional odds assumption. It means that while the baseline odds change for each category, the multiplicative effect of the predictors on these odds remains the same.\nGraphically, this translates to parallelism on the log-odds scale. If you were to plot the cumulative log-odds for different levels of a predictor, you would see a set of parallel lines. For example, imagine an ordinal outcome with four categories (e.g., ‘Low’, ‘Medium’, ‘High’, ‘Very High’). There would be three cumulative probabilities: \\(P(R\\leq Low)\\), \\(P(R \\leq Medium)\\), and \\(P(R \\leq High)\\). If the Proportional Odds assumption holds for a continuous predictor, then the log-odds for each of these cumulative probabilities would change by the same amount for a one-unit increase in that predictor.\nYou would visualize three separate curves, one for each cumulative probability, plotted against the predictor on the x-axis, with the y-axis representing the logit of these probabilities. If the Proportional Odds assumption holds, these three curves would run parallel to each other across the entire range of the predictor. They would be shifted vertically relative to each other (due to the different \\(\\alpha_j\\), but their slopes (the \\(\\beta\\) coefficients) would be identical.\n\n\n\n\n\nThe Proportional Odds assumption is useful for two main reason:\n\nParsimony and Simplicity: This is a major advantage. If the assumption holds, the \\(\\beta\\) coefficients are estimated for each predictor, regardless of the number of categories \\(m\\). Fewer parameters make the model easier to estimate, interpret, and potentially more stable, especially with smaller sample sizes.\nClear and Consistent Interpretation: A single Odds Ratio per predictor provides a clear, concise summary of its effect across the entire ordinal scale.\n\n\n\nLatent Variable Motivation\nTo better understand this model we can think about an underlying, unobserved latent continuous variable, called \\(R^*\\). Let’s imagine that the ordinal response \\(R\\) that we observe is actually a categorized version of the continuous unobserved variable \\(R^*\\). We can assume that \\(R^*\\) is linearly related to our predictors plus some error, similar to a standard linear regression model:\n\\[\nR^* = \\beta_0^* + \\beta_1^*X_1 + \\dots + \\beta_k^*X_k + \\epsilon\n\\]\nWe can set some fixed thresholds, \\(\\gamma_0 = -\\infty, \\gamma_1\\), \\(\\gamma_2\\), \\(\\dots, \\gamma_{(m-1)}\\), \\(\\gamma_m = +\\infty\\) on the \\(R^*\\) scale, such that if \\(R^*\\) falls between two thresholds, the observed ordinal variable \\(R\\) falls into the corresponding category. The thresholds are set such that \\(\\gamma_0 &lt; \\gamma_1\\) &lt; \\(\\gamma_2\\) &lt; \\(\\dots &lt; \\gamma_{(m-1)}\\) \\(&lt;\\gamma_m\\).\nThe ordinal category is determined by \\(R^*\\) based on the thresholds:\n\n\\(R = r_1\\) if \\(R^* \\leq \\gamma_1\\)\n\\(R = r_2\\) if \\(\\gamma_1 &lt; R^* \\leq \\gamma_2\\)\n\\(R = r_j\\) if \\(\\gamma_{(j-1)} &lt; R^* \\leq \\gamma_j\\)\n\\(R = r_m\\) if \\(R^* &gt; \\gamma_{(m-1)}\\)\n\n\n\n\n\n\nIf we assume that the error \\(\\epsilon\\) follows a standard logistic distribution, it can be shown that this structure leads directly to the form of the cumulative logit model:\n\\[\n\\text{logit}[P \\leq r_j | \\boldsymbol{X})] = (\\gamma_j - \\beta_0^*) - \\beta_1^*X_1 - \\dots - \\beta_k^*X_k,\n\\]\nwhich clearly matches the cumulative logit model form given in Equation 3 if we define the model intercept \\(\\alpha_j = \\gamma_j - \\beta_0^*\\) and the model’s coefficients \\(\\beta_i = -\\beta_i^*\\).\nIn this latent variable framework, the Proportional Odds assumption means two things:\n\nThe thresholds on the \\(R^*\\) scale are fixed and do not depend on the predictor variables \\(X\\).\nThe effect of each predictor \\(X_i\\)(represented by is simply to shift the entire distribution of the latent variable \\(R^*\\) along the continuous scale. This shift is the same magnitude regardless of where the fixed thresholds \\(\\gamma_j\\) are located.\n\nThis parallel shif” of the latent distribution is the reason why the odds ratios for cumulative probabilities are proportional across all thresholds.\n\n\nTesting the Proportional Odds Assumption\nViolating the Proportional Odds assumption can lead to inaccurate conclusions about the effects of the predictors. The most robust and commonly recommended way to test the Proportional Odds assumption with is by using a Likelihood Ratio Test to compare a constrained model (the PO model) with a more flexible, unconstrained model (where the PO assumption is relaxed for a specific predictor).\nThe clm() function from the ordinal package allows you to relax the PO assumption for specific predictors using the nominal = ~ predictor argument. This creates a “partial proportional odds” model (also known as a generalized ordinal logit model). We then compare this more complex model (where the assumption is relaxed) to our original, simpler proportional odds model using anova(), which performs a Likelihood Ratio Test.\n\nNull Hypothesis (\\(H_0\\)): The Proportional Odds assumption holds for the specified predictor (i.e., its coefficient is constant across all thresholds).\nAlternative Hypothesis (\\(h_1\\)): The Proportional Odds assumption does not hold for the specified predictor (i.e., its coefficient varies across thresholds).\n\nImagine we have an ordinal outcome Response (e.g., ‘Low’, ‘Medium’, ‘High’) and a continuous predictor Experience.\n\n\nCode\nlibrary(ordinal) \n\n\nWarning: il pacchetto 'ordinal' è stato creato con R versione 4.4.3\n\n\n\nCaricamento pacchetto: 'ordinal'\n\n\nIl seguente oggetto è mascherato da 'package:dplyr':\n\n    slice\n\n\nCode\nlibrary(ggplot2) \nlibrary(tidyr)   \n\nset.seed(456)\ndata &lt;- data.frame(\n  Response = factor(sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE),\n                    levels = c(\"Low\", \"Medium\", \"High\"), ordered = TRUE),\n  Experience = rnorm(100, 5, 2)\n)\n\n# Fit a standard Proportional Odds model\npo_model_hypo &lt;- clm(Response ~ Experience, data = data)\nsummary(po_model_hypo)\n\n\nformula: Response ~ Experience\ndata:    data\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  100  -109.55 225.10 3(0)  1.49e-10 4.6e+02\n\nCoefficients:\n           Estimate Std. Error z value Pr(&gt;|z|)\nExperience -0.08062    0.10394  -0.776    0.438\n\nThreshold coefficients:\n            Estimate Std. Error z value\nLow|Medium   -1.0554     0.5491  -1.922\nMedium|High   0.3228     0.5390   0.599\n\n\nCode\n# Fit a model where the PO assumption is relaxed for 'Experience'\nnon_po_model_hypo &lt;- clm(Response ~ Experience, nominal = ~ Experience, data = data)\nsummary(non_po_model_hypo) \n\n\nformula: Response ~ Experience\nnominal: ~Experience\ndata:    data\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  100  -109.55 227.09 3(0)  1.87e-09 7.0e+02\n\nCoefficients: (1 not defined because of singularities)\n           Estimate Std. Error z value Pr(&gt;|z|)\nExperience       NA         NA      NA       NA\n\nThreshold coefficients:\n                        Estimate Std. Error z value\nLow|Medium.(Intercept)  -1.03724    0.64650  -1.604\nMedium|High.(Intercept)  0.30635    0.62133   0.493\nLow|Medium.Experience    0.07697    0.12457   0.618\nMedium|High.Experience   0.08416    0.12352   0.681\n\n\nCode\n# Compare the models using anova()\nanova(po_model_hypo, non_po_model_hypo)\n\n\n\n\n  \n\n\n\nInterpretation of the results\npo_model_hypo is the Proportional Odds model, where the effect of Experience is assumed to be constant across all thresholds. non_po_model_hypo is the generalized ordinal logit model (or partial proportional odds model), where the effect of Experience is allowed to vary across the thresholds.\nThe AIC is 225.10 for po_model_hypo and 227.09 for non_po_model_hypo, this already suggest that the simpler model should be preferred.\nLR.stat (Likelihood Ratio Statistic) is the test statistic for comparing the two nested models. It’s calculated as \\(2 \\cdot (logLik_{unconstrained} −logLik_{constrained})\\).\nIn this example, LR.stat = 0.0028. This indicates that the unconstrained model doesn’t provide a much better fit than the constrained model.\nPr(&gt;Chisq) (p-value) is the p-value associated with the LR.stat.\nBased on this Likelihood Ratio Test, there is no statistically significant evidence to suggest that the Proportional Odds assumption is violated for the Experience predictor in the data. The effect of Experience on the log-odds of cumulative probabilities does not appear to vary significantly across the different thresholds.\n\n\nGraphical Inspection\nWhile statistically rigorous, formal tests can be overly sensitive, especially in large datasets. A statistically significant p-value might indicate a violation that is numerically very small and practically insignificant. This is where graphical inspection becomes invaluable.\nGraphical inspection provides a visual assessment of whether the coefficients for a given predictor truly remain constant across thresholds. This involves fitting a more flexible model and then plotting the estimated coefficients for each predictor across the different thresholds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nViolation of the Proportional Odds Assumption\nWhen the Proportional Odds assumption is violated for a predictor, the effect of that predictor on the cumulative log-odds is not constant across thresholds. Graphically, this means the lines for different cumulative probabilities would not be parallel. Their slopes would differ, meaning the \\(\\beta\\) coefficients are not the same for each threshold.\n\n\n\n\n\nWhen the formal tests and especially the graphical inspection indicate a statistically and practically significant violation, it is not possible to proceed with the standard Proportional Odds model, as the coefficient estimates would be biased and their interpretation misleading.\nThere are two main alternative models to handle this problem:\n\nGeneralized Ordinal Logit Models (or Partial Proportional Odds Models): are a flexible extension of the proportional odds model. They allow the coefficients of specific predictor variables to vary across the cumulative logit equations (i.e., across the thresholds), while still forcing other predictors (those that do satisfy the Proportional Odds assumption) to have constant effects. When only a subset of coefficients is allowed to vary, it’s specifically called a Partial Proportional Odds (PPO) model. Instead of estimating a single \\(\\beta_i\\) for a predictor \\(X_i\\), a PPO model estimates a separate \\(\\beta_{ij}\\) for each cumulative logit \\(r_j\\).\nThe advantages are:\n\n\nFlexibility: It directly addresses the violation by allowing coefficients to differ where necessary.\nParsimony (relative to Multinomial): It retains some of the efficiency of the ordinal model. If only a few predictors violate the Proportional Odds assumption, it is possible to estimate fewer parameters than a full multinomial logit model, making it more parsimonious and potentially more stable.\nMaintains Ordinality: Crucially, it still respects the inherent ordering of the outcome categories. This means that the interpretations are still about “moving up or down” the ordered scale, but the strength of that effect can differ at various points along the scale.\nInterpretation: While more complex than the Proportional Odds model, the interpretation of varying coefficients provides a richer understanding. You might find that a predictor has a strong effect in distinguishing “Low” from “Medium/High” but a much weaker effect in distinguishing “Medium” from “High”.\n\n\nMultinomial Logit Model: A multinomial (or polytomous) logit model treats the outcome categories as purely nominal (unordered), even if they are inherently ordinal. It fits a separate binary logistic regression model for each category, comparing it to a chosen reference category. Advantages are:\n\n\nNo PO Assumption: It makes no assumption about the effects of predictors being constant across categories; therefore, it automatically handles any PO violation.\nMaximum Flexibility: It is the most flexible approach for categorical outcomes, as it allows for completely different effects for each category comparison.\n\nHowever, thre are some disadvantages: - Loss of Ordinal Information: This can lead to less precise estimates and interpretations that don’t fully reflect the nature of your outcome. - Increased Complexity and Reduced Parsimony: \\((m−1)\\cdot k\\) coefficients (where \\(k\\) is the number of predictors), which is more than a Proportional Odds model (\\(k\\) predictors) or even a PPO model. This increased number of parameters can lead to: - Larger standard errors: Less statistical power. - Difficulty in interpretation: Interpreting multiple sets of coefficients and odds ratios can be cumbersome. - Overfitting: Especially with smaller sample sizes, estimating too many parameters can lead to models that fit the current data well but generalize poorly.\n\n\n\n\n\n\nIt is crucial to recognize that a statistically significant p-value from a formal test, such as the Likelihood Ratio Test, does not always signify a practically meaningful violation of the Proportional Odds assumption, especially in large datasets where even trivial deviations can be flagged. Therefore, graphical inspection becomes invaluable: if the estimated coefficients for a predictor across thresholds are very similar and their confidence intervals largely overlap (indicating near-parallelism), then despite a statistical rejection, the practical implication of the violation might be minimal, allowing one to retain the more parsimonious standard PO model. Conversely, if coefficients vary widely with little to no overlap in their confidence intervals, indicating both statistical and practical significance, then adopting a more flexible model like a Partial Proportional Odds model or, as a last resort, a Multinomial Logit model, becomes necessary to accurately reflect the data."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#coefficients-interpretation",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#coefficients-interpretation",
    "title": "Ordinal Data Analysis in R",
    "section": "2.4 Coefficients interpretation",
    "text": "2.4 Coefficients interpretation\nConsider the effect of changing a single predictor, \\(X_i\\), by one unit, while holding all other predictors constant. The change in the cumulative log-odds for a one-unit increase in \\(X_i\\) is simply the coefficient \\(\\beta_i\\).\nSo, \\(\\beta_i\\) is the change in the cumulative log-odds for a one-unit increase in \\(X_i\\), holding other predictors constant.\nTo get the Odds Ratio, we exponentiate the coefficient: \\(OR_i = \\text{exp}(\\beta_i)\\).\nThis \\(OR_i\\) is the multiplicative change in the cumulative odds for a one-unit increase in \\(X_i\\).\nFor a one-unit increase in the predictor \\(X_i\\), while holding all other predictors constant, the odds of being in category \\(r_j\\) or any category below it, versus being in a category above \\(r_j\\), are multiplied by \\(\\text{exp}(\\beta_i)\\).\nNote Due to the Proportional Odds assumption, this multiplicative effect, \\(\\text{exp}(\\beta_i)\\), is the same for all \\(m-1\\) thresholds.\n\nIf \\(\\beta_i &gt; 0\\) (and thus \\(\\text{exp}(\\beta_i)&gt;1\\)): A one-unit increase in \\(X_i\\) increases the cumulative log-odds. This means it increases the odds of being in category \\(r_j\\) or below.\nTherefore, a positive \\(\\beta_i\\) indicates that higher values of \\(X_i\\) are associated with a greater likelihood of being in the lower (or earlier) categories of the ordinal variable \\(R\\). Equivalently, it’s associated with a lower likelihood of being in the higher categories.\nIf \\(\\beta_i &lt;0\\) (and thus \\(\\text{exp}(\\beta_i)&lt;1\\)): A one-unit increase in \\(X_i\\) decreases the cumulative log-odds. This means it decreases the odds of being in category \\(r_j\\) or below.\nTherefore, a negative \\(\\beta_i\\) indicates that higher values of \\(X_i\\) are associated with a greater likelihood of being in the higher (or later) categories of the ordinal variable Y. Equivalently, it’s associated with a lower likelihood of being in the lower categories.\n\n\nConnection with the Latent Variable interpretation\nIn the latent variable model \\(R^* = \\beta_0^* + \\beta_1^*X_1 + \\dots + \\beta_k^*X_k + \\epsilon\\), a positive \\(\\beta_i^*\\) means that increasing \\(X_i\\) increases the value of the latent variable \\(R^*\\). Since higher values of \\(R^*\\) correspond to higher ordinal categories, \\(\\beta_i&gt;0\\) implies a shift towards higher categories. As we saw in the derivation, the cumulative logit model coefficient \\(\\beta_I\\) (for \\(P(Y \\leq r_j)\\)) is typically \\(-\\beta_i^*\\). So, if \\(\\beta_i^*&gt;0\\), then \\(\\beta_i&lt;0\\) in the cumulative logit model, which is associated with higher ordinal categories. This confirms the consistency between the two interpretations, although the sign convention can be tricky.\nTo avoid confusion, it is generally easiest and most standard to interpret the results directly from the estimated coefficients (\\(\\beta_i\\)) and Odds Ratios (\\(\\text{exp}(\\beta_i)\\)) from the cumulative logit model output:\n\n\\(\\text{exp}(\\beta_i)&gt;1\\): Higher values of \\(X_i\\) are associated with increased odds of being in a lower category (or equivalently, decreased odds of being in a higher category). The shift is towards the beginning of the ordered scale.\n\\(\\text{exp}(\\beta_i)&lt;1\\): Higher values of \\(X_i\\) are associated with decreased odds of being in a lower category (or equivalently, increased odds of being in a higher category). The shift is towards the end of the ordered scale.\n\n\nIncluding and Interpreting Qualitative Predictors\nCategorical predictors (nominal or ordinal, when used as predictors) cannot be entered directly into the model as single numbers. Instead, we use dummy variables. For a categorical predictor with \\(m\\) categories, \\(m−1\\) dummy variables will be created. One category is designated as the “reference category”. This category does not get its own dummy variable; its effect is absorbed into the intercept (\\(\\alpha_j\\)). All other categories are then compared to this reference category.\nChoice Considerations:\n\nClinical/Logical Baseline: a naturally occurring baseline (e.g., “No exposure,” “Placebo group,” “Male gender” if female is the group of interest).\nLargest Category: often chosen for statistical stability, as there’s more data for comparison.\nFirst or Last Category: convenient for software defaults. Category of Primary Interest: if you want to compare all other groups to a specific group. Example: Gender Predictor (Male, Female)\n\nLet’s assume “Female” is the reference category, and the ordinal dependent variable is “Self-Rated Health” (1=Poor to 5=Excellent). We create one dummy variable for “Male”.\n\\(X_{\\text{male}}=1\\) if Gender = Male\n\\(X_{\\text{male}}=0\\) if Gender = Female\nThe proportional odds model incorporating Gender would look like:\n\\[\\text{logit}[P(R\\leq r_j∣Gender,Other Predictors)] = \\alpha_j + \\beta_{male}X_{male} + \\beta_{OthPred}X_{OthPred} \\]\nThe coefficient for a dummy variable represents the difference in the cumulative log-odds between the category represented by the dummy variable and the reference category, holding all other predictors constant.\nThe Odds Ratio for a dummy variable is \\(e^{\\beta_{\\text{dummy}}}\\). This Odds Ratio represents the ratio of the odds of being in a lower outcome category for the dummy variable group compared to the reference group, holding other predictors constant.\nExample: Gender Predictor (Male, Female) for Self-Rated Health\nLet’s continue with “Self-Rated Health” (1=Poor to 5=Excellent). Assume “Female” is the reference category.\nIn our example, keeping Female as the reference category, if the estimated \\(\\beta_{male}\\) is 0.4, the Odds Ratio for Male would be \\(e^{0.4}\\approx 1.49\\).\nInterpretation:\n\nCoefficient (\\(\\beta_{male}=0.4\\)): Males have, on average, 0.4 units higher cumulative log-odds of being in a lower self-rated health category (e.g., Poor, Fair, Good, Very Good) compared to females, holding other predictors constant.\nOdds Ratio (\\(OR_{male}=1.49\\)): The odds of a male reporting “Poor or Fair or Good or Very Good Health” vs. “Excellent Health” are 1.49 times the odds for a female, holding all other predictors constant. Similarly, the odds of a male reporting “Poor or Fair or Good Health” vs. “Very Good or Excellent Health” are 1.49 times the odds for a female, and so on, across all cumulative splits.\n\nIn simpler terms, males have a higher odds of being in the less healthy (lower) categories of self-rated health compared to females. This effect is assumed to be consistent across the entire range of health categories.\n\n\n\nInference on Parameters\nOnce the model is fitted using Maximum Likelihood Estimation, it is necessary to understand the statistical significance and precision of the parameter estimates.\n\nSignificance Tests for Individual Predictors\nTo determine if an individual predictor variable has a statistically significant effect on the ordinal outcome, most statistical software packages provide a Wald Test for each predictor. The Wald test calculates a z-statistic for each coefficient: \\(Z= \\frac{Estimate}{Std Error}\\). This z-statistic is then squared to get \\(\\chi^2\\) statistic with 1 degree of freedom: \\(Wald\\chi^2 = Z^2\\).\nA p-value is then calculated based on this \\(\\chi^2\\) statistic. The Null Hypothesis (\\(H_0\\)) is that the coefficient for this predictor is zero (i.e., the predictor has no effect on the cumulative log-odds). The Alternative Hypothesis (\\(H_1\\)) is that the coefficient for this predictor is not zero (i.e., the predictor has a significant effect).\nIf the p-value \\((Pr(&gt;|z|))\\) is small (typically less than 0.05), the null hypothesis is rejected. This suggests that the predictor has a statistically significant effect on the ordinal outcome. If the p-value is large, we fail to reject the null hypothesis. There’s insufficient evidence to conclude that the predictor has a significant effect.\n\n\nConfidence Intervals for the coefficients and the Odds Ratios\nWhile p-values give about statistical significance, confidence intervals (CIs) provide a range of plausible values for the true population parameter, giving information about the precision of our estimates.\nInterpretation for coefficients (\\(\\beta\\)): A 95% CI for a coefficient means that if we were to repeat the study many times, 95% of the calculated CIs would contain the true population coefficient. If a CI for \\(\\beta\\) does not include 0, then the coefficient is statistically significant at the corresponding alpha level (e.g., 0.05 for a 95% CI).\nInterpretation for Odds Ratios (\\(exp^\\beta\\))): A 95% CI for an OR means that we are 95% confident that the true population OR lies within this range. If a CI for an OR does not include 1, then the effect is statistically significant.\nIf the CI is entirely above 1, the predictor significantly increases the odds of a lower outcome.\nIf the CI is entirely below 1, the predictor significantly decreases the odds of a lower outcome (i.e., increases the odds of a higher outcome).\n\n\nEvaluating Model Fit and Performance\nThere are several statistics to evaluate the Goodness of Fit of a Proportional Odds Model.\nIn linear regression, the \\(R^2\\) is commonly used to describe the proportion of variance in the dependent variable explained by the independent variables. However, for ordinal logistic regression (and other generalized linear models), the concept of explained variance is more complex, and the traditional \\(R^2\\) is not appropriate because the model relies on a non-linear link function.\nInstead, Pseudo R-squared measures are commonly used and they attempt to provide an analogous quantification of how well the model fits the data, or how much variance it “explains”, relative to a null model.\nThey typically compare the log-likelihood of the fitted model (\\(L_{model}\\)) to the log-likelihood of a null (intercept-only) model (\\(L_{null}\\)).\nCommon types of Pseudo R-squared include: * McFadden’s \\(R^2\\): \\(1−\\Big(\\frac{L_{model}}{L_{null}}\\Big)\\)\n\nCox & Snell’s \\(R^2\\): \\(1−\\Big(\\frac{L_{model}}{L_{null}}\\Big)^{\\frac{2}{n}}\\)\n\n\n\n\n\n\n\nDo NOT compare directly to linear regression \\(R^2\\): Pseudo R-squared values are typically much lower than \\(R^2\\) values from linear regression models, even for models that fit the data very well. A McFadden’s \\(R^2\\) of 0.20 might be considered very good in an ordinal logistic regression context, whereas a linear \\(R^2\\) of 0.20 would often be considered weak.\n\n\n\nAnother widely used statistics is the Likelihood Ratio Test, which is used for comparing two nested models: the likelihood of a simpler model (the null hypothesis model) to the likelihood of a more complex model (the alternative hypothesis model) that contains all the parameters of the simpler model plus some additional parameters. If the more complex model significantly improves the likelihood, it suggests that the additional parameters are meaningful.\nLet \\(L_{restricted}\\) be the maximum likelihood of the simpler (nested) model, and L \\(L_{full}\\) be the maximum likelihood of the more complex (full) model. The likelihood ratio test statistic (\\(\\Lambda\\)) is:\n\\[\n\\Lambda = - 2 \\cdot \\log \\Bigg(\\frac{L_{restricted}}{L_{full}}\\Bigg)\n\\]\nWhich simplifies to (due to properties of logarithms):\n\\[\n\\Lambda = -2 \\cdot [\\log(L_{restricted})-\\log(L_{full})]\n\\]\nThe Null Hypothesis (\\(H_0\\)) of this test is that the additional parameters in the full model do not significantly improve the fit, so the simpler model is enough. The Alternative Hypothesis (\\(H_1\\)) is that the more complex model provides a significantly better fit. Under the null hypothesis, \\(\\Lambda\\) asymptotically follows a chi-squared distribution with degrees of freedom equal to the difference in the number of parameters between the two models.\nA large \\(\\Lambda\\) value (and small p-value) indicates that the more complex model fits the data significantly better than the simpler model, leading to rejection of \\(H_0\\).\nInformation criteria provide a way to balance model fit with model complexity. They are particularly useful for comparing non-nested models or when there are multiple competing models. Lower values generally indicate a better model. The goal is to find a model that explains the data well without being overly complex.\n\nAIC = \\(-2 \\log(L_{model})+2k\\)\nwhere\n\n\\(L_{model}\\) is the maximum likelihood of the fitted model\n\\(k\\) is the is the number of parameters in the model,including the intercept and any cut-points in ordinal regression\n\nBIC = \\(-2 \\log(L_{model})+k\\log(n)\\)\nwhere\n\n\\(L_{model}\\) is the maximum likelihood of the fitted model\n\\(k\\) is the is the number of parameters in the model,including the intercept and any cut-points in ordinal regression\n\\(n\\) is the sample size\n\n\nAIC tends to favor more complex models and is generally better for prediction accuracy. BIC tends to favor simpler, more parsimonious models and is often preferred for model selection when the goal is to identify the “true” underlying model."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#c",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#c",
    "title": "Ordinal Data Analysis in R",
    "section": "2.5 C",
    "text": "2.5 C"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#case-study-analyzing-wine-ratings",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#case-study-analyzing-wine-ratings",
    "title": "Ordinal Data Analysis in R",
    "section": "2.5 Case study: Analyzing Wine Ratings",
    "text": "2.5 Case study: Analyzing Wine Ratings\nIn this paragraph there is practical case study using the wine dataset from the ordinal package.\nThe primary aim of this study is to understand the factors that influence wine quality ratings given by various judges. Specifically, we want to investigate how the serving temperature of the wine (temp), the judge’s prior experience (contact), the storage temperature of the bottle (bottle), and the identity of the judge himself/herself, relate to the rating assigned to the wine. This will help us determine if certain conditions or judges consistently lead to higher or lower ratings.\nThis dataset comes from an experiment designed to study preferences for red wine served at different temperatures.\nLet’s start by loading the dataset and getting a panoramic view of its variables.\n\n\nWarning: il pacchetto 'performance' è stato creato con R versione 4.4.3\n\n\nThe data set has 72 observations and 6 variables:\n\nresponse: An integer variable acting as a unique identifier for each tasting observation (from 1 to 72). This is not typically used as a predictor or outcome but helps in data management.\nrating: It’s an ordered factor with 5 levels, representing the quality rating assigned to the wine by a judge. The levels are “1” (likely worst) &lt; “2” &lt; “3” &lt; “4” &lt; “5” (likely best).\ntemp: A qualitative predictor (factor) with 2 levels: “cold”: The wine was served cold. “warm”: The wine was served warm.\ncontact: A quantitative (discrete) predictor. This is the number of times a judge has participated in previous wine experiments.\nbottle: A qualitative (nominal/categorical) predictor. This describes the pre-serving storage temperature of the bottle. It’s a factor with levels ‘cold’, ‘room’, and ‘warm’.\njudge: A qualitative (nominal/categorical) predictor. This identifies the specific individual taster. There are 9 distinct judges (A through I).\n\n\n2.5.1 Model Fitting and Interpretation\nWe start by fitting a full Proportional Odds model, assuming that the effect of each predictor is constant across all thresholds.\n\n\nCode\nmodel&lt;- clm(rating ~ temp + contact + bottle + judge, data = wine)\nsummary(model)\n\n\nformula: rating ~ temp + contact + bottle + judge\ndata:    wine\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  72   -69.53 177.05 6(0)  1.17e-07 1.6e+02\n\nCoefficients: (2 not defined because of singularities)\n           Estimate Std. Error z value Pr(&gt;|z|)    \ntempwarm     4.3748     1.1348   3.855 0.000116 ***\ncontactyes   1.5853     0.9280   1.708 0.087604 .  \nbottle2      1.3207     0.9958   1.326 0.184747    \nbottle3      1.2578     1.3834   0.909 0.363254    \nbottle4      0.8900     1.3695   0.650 0.515776    \nbottle5     -0.7118     0.9501  -0.749 0.453719    \nbottle6          NA         NA      NA       NA    \nbottle7      0.4455     0.9404   0.474 0.635686    \nbottle8          NA         NA      NA       NA    \njudge2      -3.3293     1.0729  -3.103 0.001915 ** \njudge3      -1.0286     1.0039  -1.025 0.305524    \njudge4      -2.5754     1.0560  -2.439 0.014733 *  \njudge5      -2.0651     1.0181  -2.028 0.042526 *  \njudge6      -1.7975     0.9977  -1.802 0.071620 .  \njudge7      -5.3696     1.1733  -4.577 4.73e-06 ***\njudge8      -2.8523     0.9969  -2.861 0.004222 ** \njudge9      -3.2805     1.0165  -3.227 0.001249 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2   -3.641      1.072  -3.396\n2|3   -0.129      1.021  -0.126\n3|4    2.911      1.070   2.720\n4|5    4.954      1.144   4.329"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#beyond-the-proportional-odds-logit-model",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#beyond-the-proportional-odds-logit-model",
    "title": "Ordinal Data Analysis in R",
    "section": "2.6 Beyond the Proportional Odds Logit Model",
    "text": "2.6 Beyond the Proportional Odds Logit Model\nThe Proportional Odds Model using the logit link function is the most common and often the default choice for analyzing ordinal data due to its interpretability (log-odds) and computational stability. However, the Proportional Odds Model is just one member of a broader family of models for ordinal outcomes. The specific choice of model can influence the interpretation and the fit to the data. Several alternatives exist.\n\n2.6.1 Alternative Link Functions for Cumulative Models\nIn a Proportional Odds Model, the “link function” transforms the cumulative probabilities to a linear scale, where they are modeled by your predictors.\nThe general form of a cumulative model is:\n\\[\ng[P(R \\leq r_j)] = \\alpha_j - \\boldsymbol{\\beta}^T\\boldsymbol{X}\n\\] where - \\(P(R \\leq r_j)\\) is the cumulative probability of being in category \\(j\\) or lower - \\(g(\\cdot)\\) is the link function - \\(\\alpha_j\\) are the category-specific intercepts - \\(\\boldsymbol{\\beta}\\) is the vector of regression coefficients for the predictors - \\(\\boldsymbol{X}\\) is the vector of predictor variables\nThe link function can be a logit function, as shown previously, but it may be another function.\nProbit Link\nThe probit link uses the inverse of the standard normal cumulative distribution function (\\(\\Phi^{-1}\\)) and it models the cumulative probabilities on a scale that corresponds to the normal distribution\n\\[\n\\text{probit}[P(R\\leq r_j)] = \\Phi^{-1}[P(R\\leq r_j)] =  \\alpha_j - \\boldsymbol{\\beta}^T\\boldsymbol{X}\n\\] The coefficients are interpreted in terms of standard deviation units of the underlying latent normal variable. A one-unit increase in \\(X_k\\) leads to a \\(\\beta_k\\) standard deviation change in the latent variable. They don’t have the direct odds ratio interpretation of the logit model.\nThis link function is often preferred when there’s a theoretical belief that the underlying continuous variable driving the ordinal outcome is normally distributed.\nLog-log Link\nThe Log-Log link is defined as follows:\n\\[\n\\text{loglog}[P(R\\leq r_j)] =  \\log(-\\log(P(R\\leq r_j))) =  \\alpha_j - \\boldsymbol{\\beta}^T\\boldsymbol{X}\n\\]\nThis link is used when the probability of the lowest category is expected to decrease very quickly, or when the process leading to higher categories accelerates rapidly.\nThe interpretation is less straightforward than logit. It’s more sensitive to changes in the upper tail of the probability distribution. It implies that the probability of being in a lower category decreases rapidly."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#alternative-model-structures",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#alternative-model-structures",
    "title": "Ordinal Data Analysis in R",
    "section": "2.7 Alternative Model Structures",
    "text": "2.7 Alternative Model Structures\nBeyond altering the link function for cumulative probabilities, it is possible to change what probabilities the model is trying to explain.\nAdjacent Categories Logit Model\nInstead of modeling cumulative probabilities, the adjacent categories model focuses on the log-odds of being in category \\(r_j\\) versus the next adjacent category \\(r_{j+1}\\):\n\\[\n\\log \\Bigg(\\frac{P(R = r_j)}{P(R = r_{j+1})}\\Bigg) = \\alpha_j - \\boldsymbol{\\beta}^T\\boldsymbol{X} \\qquad \\text{for} \\qquad j = 1,\\dots,m-1\n\\] In this model, the coefficients are not constrained to be the same across all adjacent log-odds comparisons. This means it does not assume the proportional odds assumption. Each \\(\\alpha_j\\) is a separate intercept for that specific adjacent comparison, and each \\(\\beta_j\\) is a separate vector of coefficients. So, for each pair of adjacent categories \\(j\\) and \\(j+1\\), \\(\\text{exp}(\\beta_k)\\) is the odds ratio of being in category \\(j\\) versus \\(j+1\\) for a one-unit change in \\(X_k\\).\nThe advantage of this model is that it does not impose the proportional odds assumption, so it is more flexible compared to the Proportional Odds Model. However, it leads to the estimation of \\(m-1\\) sets of predictors which are many more parameters compared to the Proportional Odds Model; this can lead ot larger standard errors and require larger sample sizes.\nContinuation Ratio Logit Model\nThis model focuses on the log-odds of being in category \\(j\\) versus being in a higher category, given that the outcome is at least \\(j\\). It’s a sequential modeling approach. The model is expressed as: \\[\n\\log\\left(\\frac{P(R = r_j \\mid R \\ge r_j)}{P(R &gt; r_j \\mid R \\ge r_j)}\\right) = \\alpha_j - \\boldsymbol{\\beta}_j^T \\mathbf{X} \\quad \\text{for } j=1, \\dots, m-1\n\\] Where:\n\n\\(P(Y = j \\mid Y \\ge j)\\) is the conditional probability of observing outcome category \\(j\\), given that the outcome is in category \\(j\\) or higher.\n\\(P(Y &gt; j \\mid Y \\ge j)\\) is the conditional probability of observing an outcome category higher than \\(j\\), given that the outcome is in category \\(j\\) or higher.\n\\(\\alpha_j\\) is the category-specific intercept for the \\(j\\)-th comparison.\n\\(\\boldsymbol{\\beta}_j\\) is the vector of regression coefficients for the predictors \\(\\mathbf{x}\\) for the \\(j\\)-th comparison.\n\nThe exponential of a coefficient, \\(\\exp(\\beta_{jk})\\), represents the odds ratio of observing category \\(j\\) versus observing a category higher than \\(j\\), given that the outcome is at least \\(j\\), for a one-unit increase in predictor \\(x_k\\). Crucially, the coefficients \\(\\boldsymbol{\\beta}_j\\) can vary across these sequential comparisons (i.e., for different values of \\(j\\)). This means the effect of a predictor might differ depending on which step of the ordinal scale is examined.\nSimilar to the adjacent categories model, the continuation ratio model typically does not assume proportional odds. This means it does not constrain the \\(\\boldsymbol{\\beta}\\) coefficients to be constant across all sequential comparisons. However, a “proportional odds” variant can be imposed on the continuation ratios by forcing \\(\\boldsymbol{\\beta}\\) to be constant across all \\(j\\).\nThis model is especially suited for situations where the ordinal categories represent a natural progression or a series of choices. Common applications include:\n\nEducational Attainment: Modeling the odds of graduating from high school versus continuing to college, then the odds of completing college versus pursuing graduate studies, and so on.\nDisease Progression: Analyzing the odds of a patient staying at their current disease stage versus progressing to the next, more severe stage.\nConsumer Behavior: Understanding the odds of a customer making a basic purchase versus upgrading to a premium version.\n\nReferences\n\nAgresti, A. (2010). Analysis of ordinal categorical data. John Wiley & Sons.\nMcNulty, K. (2021). Handbook of regression modeling in people analytics: with examples in R and Python. Chapman and Hall/CRC."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#the-psychological-reasons-behind-the-cub-model",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#the-psychological-reasons-behind-the-cub-model",
    "title": "Ordinal Data Analysis in R",
    "section": "3.1 The Psychological Reasons Behind the CUB Model",
    "text": "3.1 The Psychological Reasons Behind the CUB Model\nThe innovation of the CUB model lies in its attempt to statistically represent two fundamental psychological components that influence a respondent’s choice on an ordinal scale. The model posits that an observed rating \\(R\\) is a probabilistic outcome of a decision process that weighs two components.\n1. Feeling (or Attraction/Preference)\nThis component represents the conscious, rational, and deliberative aspect of the decision-making process. It reflects the respondent’s genuine evaluation, opinion, or perception regarding the item being rated. The “feeling” directs the respondent towards a specific category on the scale that best aligns with their internal assessment. This is the component that captures the respondent’s true position or stance on the matter at hand.\nDepending on the specific context of the rating, “feeling” can be interpreted as:\n\nAgreement/Disagreement: How much a person agrees or disagrees with a statement.\nSatisfaction/Dissatisfaction: The level of contentment or discontent with a product, service, or experience.\nLiking/Disliking: The degree of preference or aversion towards an item.\nPerceived quality, importance, risk, etc.: The subjective assessment of various attributes.\n\nIn essence, the feeling component drives the respondent towards a particular region of the ordinal scale, reflecting their underlying preference or “attraction” to certain categories.\n2. Uncertainty (or Indecision/Fuzziness)\nThis component captures the hesitation, randomness, or lack of decisiveness that can accompany the choice process. It acknowledges a crucial psychological reality: respondents may not always have a perfectly clear and precise mapping of their internal feeling onto the provided scale categories. This can introduce a degree of randomness or “noise” into the selection process.\nSources of this uncertainty can be varied and include:\n\nLack of Information or Knowledge: The respondent might not have sufficient information to form a strong opinion about the item being rated.\nAmbiguity: The question wording or the definition of the scale categories might be unclear, leading to confusion.\nPersonal Tendencies: Some individuals may inherently be more indecisive or tend to use certain parts of a scale (e.g., sticking to the middle categories).\nCognitive Effort/Satisficing: In long surveys or under time pressure, respondents might engage in “satisficing” behavior. Instead of expending full mental energy to find the optimal category, they might pick a plausible but not necessarily precise one to save cognitive effort.\nTime Pressure or Fatigue: Being rushed or tired can reduce the ability to make a precise decision.\nEmotional State or Mood: A respondent’s transient emotional state can also introduce variability into their choices.\n\nThe uncertainty component effectively describes the probability that the respondent’s choice is influenced by random factors rather than a specific preference."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#the-cub-model-statistical-formulation",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#the-cub-model-statistical-formulation",
    "title": "Ordinal Data Analysis in R",
    "section": "3.2 The CUB Model: Statistical Formulation",
    "text": "3.2 The CUB Model: Statistical Formulation\nThe basic CUB model, designed for m ordered categories (typically \\(r=1,2,\\dots,m\\)), is a finite mixture of two discrete probability distributions:\n\nA shifted Binomial distribution to model the feeling (or attraction/preference) component.\nA discrete Uniform distribution to model the uncertainty (or indecision/fuzziness) component.\n\nThe Probability Mass Function (PMF) for an observed rating \\(R=r\\) is given by:\n\\[\nP(R = r \\mid \\pi,\\xi) = \\pi B(r \\mid\\xi) + (1-\\pi)U(r)\n\\] where the elements of the mixture are:\n\n\\(m\\): This is the number of ordered categories on the scale. For instance, if a scale ranges from “1 = Strongly Disagree” to “5 = Strongly Agree,” then \\(m=5\\).\n\\(r\\): This denotes the selected category by a respondent, where \\(r\\in \\{ 1,2,…,m\\}\\).\n\\(\\pi \\in (0,1]\\), acts as a mixture weight. \\(\\pi\\) represents the probability that the observed choice is driven by the feeling component (i.e., the shifted Binomial distribution).\nConsequently, \\((1−\\pi)\\) is called Uncertainty parameter and represents the probability that the observed choice is driven by the uncertainty component (i.e., the discrete Uniform distribution). A higher value of \\((1−\\pi)\\) means greater indecision or randomness in the response, indicating that the uniform component has a stronger influence.\n\\(\\xi \\in [0,1]\\), is directly related to the shifted Binomial distribution and determines the location of the “feeling”.\nMore intuitively, \\((1−\\xi)\\) is often considered the direct measure of “feeling”, indeed it is called Feeling parameter.\n\nIf \\((1−\\xi)\\) is high (e.g., close to 1, meaning \\(\\xi\\) is close to 0), there’s a strong underlying feeling towards the higher end of the scale.\nIf \\((1−\\xi)\\) is low (e.g., close to 0, meaning \\(\\xi\\) is close to 1), there’s a strong underlying feeling towards the lower end of the scale.\nIf \\((1−\\xi) = 0.5\\) (meaning \\(\\xi=0.5\\)), the feeling component is neutral or centered, implying a symmetric preference if not for the influence of uncertainty.\n\n\\(U(r)\\): This is the probability of choosing category \\(r\\) according to a discrete Uniform distribution.\nFor any category \\(r\\in\\{1,2,…,m\\}\\), \\(U(r)= \\frac{1}{m}\\).\nThis component represents complete randomness or a lack of specific preference among the \\(m\\) categories. If a respondent is entirely uncertain or indecisive, their response is essentially a random pick from the available options.\n\\(B(r∣\\xi)\\): This is the probability of choosing category \\(r\\) according to a shifted Binomial distribution.\nSpecifically, this refers to \\(P(X=r−1)\\) where \\(X\\sim Bin(m−1,1−\\xi)\\). The “shifted” aspect arises because the rating scale typically starts from 1, while a standard Binomial distribution’s trials start from 0. Thus, to model a choice of \\(r\\) on a scale \\(1,…,m\\), we consider \\(r−1\\) successes out of \\(m−1\\) trials.\nSo, the PMF is:\n\n\\[\nB(r\\mid\\xi)=\\binom{m-1}{r-1}(1-\\xi)^{r-1}\\xi^{m-r}\n\\]\nThis component models the feeling towards a particular category, allowing for various shapes (unimodal, skewed left/right, or even U-shaped if \\((1−\\xi)\\) is extremely close to 0 or 1, though the latter is less common in direct “feeling” interpretation). It captures where the respondent’s underlying preference lies on the scale.\nFinally, the CUB model can be written as: \\[\nP(R = r \\mid \\pi,\\xi) = \\pi \\binom{m-1}{r-1}(1-\\xi)^{r-1}\\xi^{m-r} + (1-\\pi) \\frac{1}{m}\n\\]\n\n3.2.1 Interpretation of Parameters\nThe feeling \\((1-\\xi)\\) and the uncertainty \\((1-\\pi)\\) parameters are not merely statistical quantities; they have direct psychological meanings related to the respondent’s decision process.\nThe Uncertainty Parameter\nIt directly quantifies the level of uncertainty or indecision in the respondent’s choice.\n\nIf \\((1-\\pi = 0)\\) (i.e., \\(\\pi = 1\\)) it means that the observed choice is entirely determined by the feeling, modeled by the binomial component. There is no uncertainty. The distribution of responses will reflect the shape of the Binomial.\nIf \\((1−\\pi)=1\\) (i.e., \\(\\pi=0\\)) it means that the observed choice is entirely determined by “uncertainty” (Uniform component). The respondent’s feeling plays no role. The distribution of responses will be perfectly flat, meaning each category is chosen with equal probability.\nValues between 0 and 1 indicate a mix of the two components. A higher value of \\((1−\\pi)\\) effectively “flattens” the observed distribution of responses towards a uniform shape, as the random component gains more influence.\n\nThe Feeling Parameter\nThe quantity \\((1−\\xi)\\) measures the underlying “feeling”, “preference”, or “attraction” of the respondent. It dictates the skewness and location of the Binomial component.\n\nIf \\((1−\\xi)\\) is high (close to 1, so \\(\\xi\\) is close to 0), it means that there’s a strong attraction towards higher-valued categories. The Binomial component will be skewed to the right, with its mode (most frequent value) located at or near the maximum category \\(m\\).\nIf \\((1−\\xi)\\) is low (close to 0, so \\(\\xi\\) is close to 1) it means that there’s a strong attraction towards lower-valued categories. The Binomial component will be skewed to the left, with its mode located at or near the minimum category 1.\nIf \\((1−\\xi)=0.5\\) (so \\(\\xi=0.5\\)), the Binomial component is symmetric around the center of the scale, indicating a neutral or balanced feeling.\n\n\n\n\nProbability distribution functions of the CUB model, for \\(m = 7\\) and 9 combinations of feeling and uncertainty.\n\n\n\n\n3.2.2 Model identifiability and Estimation\nThe CUB model is identifiable (i.e., different sets of parameter values lead to different probability distributions for the observed data) if the number of categories \\(m &gt; 3\\).\nThe parameters \\((\\pi, \\xi)\\) of the CUB model are typically estimated using the Maximum Likelihood Estimation (MLE) method. MLE search for the parameter values that maximize the likelihood of observing the given data.\nSince the CUB model is a mixture model, direct maximization of this log-likelihood function can be complex due to its non-linear nature. Therefore, the Expectation-Maximization (EM) algorithm is a common and robust iterative method for finding the MLEs.\n\n\n3.2.3 Assessing the Goodness of Fit\nA particularly common and intuitive measure for assessing how well a CUB model fits the observed data is the Dissimilarity (\\(Diss\\)) Index. It quantifies the absolute difference between the observed and fitted proportions for each category. The \\(Diss\\) index is defined as follows:\n\\[\nDiss = \\frac{1}{2}\\sum_{r = 1}^{m} \\mid f_r - p_r(\\hat{\\boldsymbol{\\theta}})\\mid\n\\] where \\(f_r\\) are the observed relative frequencies and \\(p_r(\\hat{\\boldsymbol{\\theta}})\\) are the estimated probabilities for the response categories.\n\nValues of \\(Diss\\) closer to 0 indicate a better fit, with a perfect fit yielding \\(Diss = 0\\).\nThe maximum value of \\(Diss\\) is 1 when there is no overlap between observed and fitted distributions.\n\nThe Dissimilarity Index is popular because it provides a direct and easily interpretable measure of the overall discrepancy between the observed data distribution and the distribution predicted by the CUB model. In other words, it measures the proportion of responses to be changed to achieve a perfect fit.\nIt is less sensitive to low expected frequencies than the Pearson Chi-squared test and gives a clear indication of prediction accuracy.\n\n\n3.2.4 Parameter Space Visualization\nCUB models offer a highly intuitive way to visualize the joint interpretation of their parameters: the parameter space visualization. This is typically represented on a unit square where:\nThe x-axis represents \\((1−\\pi)\\), the uncertainty level. The y-axis represents \\((1−\\xi)\\), the feeling or attraction level. This “CUB plot” is incredibly helpful for comparing different datasets, subgroups, or even changes within a single group over time. Key regions of this plot offer immediate insights:\n\nBottom-left corner (\\((1−\\pi)\\approx0,(1−\\xi)\\approx0\\)): This indicates very low uncertainty (respondents are very certain in their choices) and a strong feeling towards low scores (e.g., strong disagreement, high dissatisfaction).\nTop-left corner (\\((1−\\pi)\\approx0,(1−\\xi)\\approx1\\)): This signifies very low uncertainty and a strong feeling towards high scores (e.g., strong agreement, high satisfaction).\nRight side (\\((1−\\pi)\\approx1\\)): This region represents very high uncertainty. In this scenario, the feeling component becomes less influential, and responses tend to be closer to a uniform distribution, indicating a significant degree of randomness in the choices.\n\nThis graphical representation provides a powerful diagnostic tool for understanding the underlying psychological processes at play in rating data.\n\n\n\nGraphical representation of the parameter space of the CUB model"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#extensions-of-the-cub-model-incorporating-covariates",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#extensions-of-the-cub-model-incorporating-covariates",
    "title": "Ordinal Data Analysis in R",
    "section": "3.3 Extensions of the CUB Model: Incorporating Covariates",
    "text": "3.3 Extensions of the CUB Model: Incorporating Covariates\nThe basic CUB model, as discussed in Chapter 2, provides a powerful framework for understanding the underlying feeling and uncertainty in rating data. However, the basic formulation assumes a homogeneity in the population, meaning that the parameters are constant for all individuals. While useful for initial descriptive analysis, this assumption often oversimplifies the complex reality of human responses.\nIn real-world applications, it is almost always more realistic to assume that the psychological components driving responses (the feeling and uncertainty components) vary across different individuals or groups. These variations are often systematically related to observable characteristics of the respondents or the context in which the ratings are collected.\nThe introduction of covariates into the CUB model allows us to move beyond a simple description of the overall population’s feeling and uncertainty, enabling a much more nuanced and insightful analysis. Specifically, introducing covariates allows us to:\n\nExplain heterogeneity in response patterns: Instead of treating differences in responses as random noise, covariates enable us to identify systematic factors that explain why different individuals or groups exhibit different levels of feeling and uncertainty.\nUnderstand how specific characteristics influence feeling and uncertainty: This is perhaps the most powerful advantage. We can quantify the impact of a 10-year age increase on the probability of certainty (π) or the effect of a higher education level on the tendency to rate highly (1−ξ). This moves from mere description to explanation and inference.\n\n\n3.3.1 Statistical Formulation of CUB Model with Covariates\nFormulation of CUB with Covariates In a CUB model with covariates, the core idea is that the parameters \\(\\pi\\) and \\(\\xi\\) are no longer fixed constants across all individuals. Instead, they are modeled as functions of a set of explanatory variables. Let \\(y_i\\) be a vector of covariates for subject \\(i\\) specifically influencing their uncertainty, and \\(w_i\\) be a vector of covariates for subject \\(i\\) influencing their feeling. These covariate vectors can be the same, overlap, or be entirely different.\nTo ensure that the parameters \\(\\pi_i\\) and \\(\\xi_i\\) remain within their valid range of \\([0,1]\\), a link function is employed, typically a logistic (logit) function, which is standard practice for modeling probabilities in generalized linear models.\nModeling the uncertainty\nSince \\(\\pi_i\\in [0,1]\\), we model it using a logistic link. It’s often more intuitive to model \\((1-\\pi_i)\\), which represents the probability that the choice is driven by the uniform component.\nThe log-odds of uncertainty are linked to the covariates \\(y_i\\):\n\\[\n\\text{logit}(1-\\pi_i) = \\log \\Bigg(\\frac{1-\\pi_i}{\\pi_i}\\Bigg) = \\boldsymbol{y}^{T}_i\\boldsymbol{\\beta}\n\\] where \\(\\boldsymbol{\\beta}\\) is a vector of coefficients corresponding to the covariates \\(\\boldsymbol{y}_i\\).\nFrom this, we can derive the direct relationships for \\((1−\\pi_i)\\) and \\(\\pi_i\\)\n\\[\n(1-\\pi_i)=\\frac{\\exp(\\boldsymbol{y}^{T}_i\\boldsymbol{\\beta})}{1+\\exp(\\boldsymbol{y}^{T}_i\\boldsymbol{\\beta})}\n\\]\nAnd consequently, for \\(\\pi_i\\):\n\\[\n\\pi_i = 1-(1-\\pi_i) = 1- \\frac{\\exp(\\boldsymbol{y}^{T}_i\\boldsymbol{\\beta})}{1+\\exp(\\boldsymbol{y}^{T}_i\\boldsymbol{\\beta})} = \\frac{1}{1+\\exp(\\boldsymbol{y}^{T}_i\\boldsymbol{\\beta})}\n\\]\nModeling the feeling\nSimilarly, for \\(\\xi_i\\in [0,1]\\), a logit link is applied. It’s often more insightful to directly model \\((1-\\xi_i)\\), which represents the feeling towards higher scores on the scale.\nThe log-odds of feeling for higher scores are linked to the covariates \\(w_i\\) :\n\\[\n\\text{logit}(1-\\xi_i) = \\log \\Bigg(\\frac{1-\\xi_i}{\\xi_i}\\Bigg) = \\boldsymbol{w}^{T}_i\\boldsymbol{\\gamma}\n\\] where \\(\\boldsymbol{\\gamma}\\) is a vector of coefficients corresponding to the covariates \\(\\boldsymbol{w}_i\\).\nThis implies the following fo \\((1−\\xi_i)\\):\n\\[\n(1-\\xi_i)=\\frac{\\exp(\\boldsymbol{w}^{T}_i\\boldsymbol{\\gamma})}{1+\\exp(\\boldsymbol{w}^{T}_i\\boldsymbol{\\gamma})}\n\\]\nAnd for \\(\\xi_i\\):\n\\[\n\\xi_i = 1-(1-\\xi_i) = 1- \\frac{\\exp(\\boldsymbol{w}^{T}_i\\boldsymbol{\\gamma})}{1+\\exp(\\boldsymbol{w}^{T}_i\\boldsymbol{\\gamma})} = \\frac{1}{1+\\exp(\\boldsymbol{w}^{T}_i\\boldsymbol{\\gamma})}\n\\]\nWith these formulations, the overall probability mass function for an observed rating R=r for subject i becomes:\n\\[\nP(R_i = r \\mid \\pi_i,\\xi_i) = \\pi_i \\binom{m-1}{r-1}(1-\\xi_i)^{r-1}\\xi_i^{m-r} + (1-\\pi_i) \\frac{1}{m}\n\\]\nwhere \\(\\pi_i\\) and \\(\\xi_i\\) are determined by the covariates as defined above.\n\n\n3.3.2 Interpretation of Covariate Effects\nInterpreting the coefficients (\\(\\beta\\) and \\(\\gamma\\)) in a CUB model with covariates is crucial for understanding how specific characteristics influence the psychological components of rating. Remember that these coefficients operate on the log-odds scale due to the logistic link.\nCoefficients \\(\\beta\\)\nThese coefficients describe the impact of covariates on the log-odds of uncertainty.\nA positive coefficient \\(\\beta_k\\) implies that an increase in the \\(k\\)-th covariate \\(y_{ik}\\), holding other covariates constant, increases the log-odds of uncertainty. Therefore \\((1−\\pi_i)\\) increases and makes the respondent’s choice more driven by the uniform component. This suggests that as \\(y_{ik}\\) increases, the individual is more indecisive or random in their rating.\nA negative \\(\\beta_k\\) implies that an increase in \\(y_{ik}\\) decreases the log-odds of uncertainty, thus decreasing \\((1−\\pi_i)\\) and making the respondent’s choice more driven by their underlying feeling. This suggests that as \\(y_{ik}\\) increases, the individual becomes more certain in their rating.\nCoefficients \\(\\gamma\\)\nThese coefficients describe the impact of covariates \\(w_i\\) on the log-odds of feeling towards higher scores.\nA positive \\(\\gamma_k\\) implies that an increase in the \\(k\\)-th covariate \\(w_{ik}\\), holding other covariates constant, increases the log-odds of feeling for higher scores, thus increasing \\((1-\\xi_i)\\). This means that as \\(w_{ik}\\) increases, the individual’s underlying preference shifts towards the higher end of the rating scale.\nA negative \\(\\gamma_k\\) implies that an increase in \\(w_{ik}\\) decreases the log-odds of feeling for higher scores, thus decreasing \\((1-\\xi_i)\\). This means that as \\(w_{ik}\\) increases, the individual’s underlying preference shifts towards the lower end of the rating scale.\nImportant Note: CUB models with covariates are not Generalized Linear Models (GLMs) in the strict sense. While the parameters \\(\\pi_i\\) and \\(\\xi_i\\) are linked to covariates using GLM-like structures (specifically, logistic regressions), the response variable itself (the mixture of Binomial and Uniform distributions) does not belong to the exponential family.\n\n\n3.3.3 Case Study: University’s students satisfaction\n\n\n\n  R Script \n\nThis case study demonstrates the application of CUB (Combination of Uniform and a shifted Binomial) models for analyzing ordinal data, using the CUB package in R. The univer dataset comes from a survey administered to students at the University of Naples Federico II to evaluate orientation services.\nDataset Description\nThe univer dataset from 2002 contains 2,179 observations, including responses to evaluation questions on a 7-point Likert scale (from 1 = “very unsatisfied” to 7 = “extremely satisfied”). The ordinal variables of interest are:\n\ninformat: Satisfaction level with the information received.\nwillingn: Satisfaction level with the staff’s willingness.\nofficeho: Satisfaction level with office opening hours.\ncompeten: Satisfaction level with staff competence.\nglobal: Overall satisfaction level.\n\nThe dataset also includes covariates:\n\nFaculty: A factor variable, with levels ranging from 1 to 13 indicating the coding for the different university faculties\nFreqserv: A factor with levels: 0 = for not regular users, 1 = for regular users\nAge: Variable indicating the age of the respondent in years\nGender: A factor with levels: 0 = man, 1 = woman\nDiploma: A factor with levels: 1 = classic studies, 2 = scientific studies, 3 = linguistic, 4 = Professional, 5 = Technical/Accountancy, 6 = others\nResidence: A factor with levels: 1 = city NA, 2 = district NA, 3 = others\nChangeFa: A factor with levels: 1 = changed faculty, 2 = not changed faculty\n\nExploratory Analysis\nAs a first step, it is helpful to visualize the estimated CUB models for all ordinal variables simultaneously. The multicub() function fits a CUB model to each variable and plots the estimated parameters for uncertainty (1 - \\(\\hat{\\pi}\\)) and feeling (1 - \\(\\hat{\\xi}\\)) in the parameter space. The “feeling” (1 - \\(\\xi\\)) represents the preference for higher scores on the rating scale.\nNote that multicub() is especially useful for a comparative visual analysis of rating data vectors, even with different scale lengths. It requires the variables to be stored in a list.\n\n\nCode\nlibrary(CUB)\ndata(univer)\n\n# Select only the columns with ordinal variables\nlistord &lt;- univer[, 8:12]\n# Assign labels for the plot\nlabels &lt;- names(univer)[8:12]\n\n# Visualize CUB models for each ordinal variable\nmulticub(listord, labels = labels,\n         caption = \"CUB models on Univer data set\", pch = 19,\n         pos = c(1, rep(3, ncol(listord) - 1)), ylim = c(0.75, 1), xlim = c(0, 0.4))\n\n\n\n\n\nFrom the visual analysis, it appears that the officeho item (satisfaction with opening hours) has the highest uncertainty. For this reason, the subsequent analysis will focus on this variable.\nCUB Model without Covariates for officeho\nFirst, a basic CUB model is estimated for the officeho variable without including any covariates. This serves as a baseline to understand the variable’s distribution.\n\n\nCode\n## CUB model without covariates for \"officeho\"\ncub_00 &lt;- GEM(Formula(officeho ~ 0 | 0 | 0), family = \"cub\", data = univer)\n\n# Display a detailed summary of the estimated model\nsummary(cub_00, digits = 5)\n\n\n======================================================================= \n=====&gt;&gt;&gt; CUB  model    &lt;&lt;&lt;=====   ML-estimates via E-M algorithm   \n======================================================================= \n m= 7  Sample size: n= 2179  Iterations= 28  Maxiter= 500 \n======================================================================= \nUncertainty                                            \n    Estimates   StdErr   Wald\npai   0.68019 0.019349 35.153\n======================================================================= \nFeeling                                            \n    Estimates    StdErr   Wald\ncsi   0.19714 0.0058822 33.515\n======================================================================= \nLog-lik            = -3759.917 \nMean Log-likelihood= -1.72552 \nLog-lik(UNIFORM)   = -4240.138 \nLog-lik(saturated) = -3706.922 \nDeviance           = 105.9914 \n----------------------------------------------------------------------- \nAIC       = 7523.834 \nBIC       = 7535.208 \nICOMP     = 7521.135 \n======================================================================= \nElapsed time= 0.02 seconds =====&gt;&gt;&gt; Wed Jul  2 20:48:01 2025 \n======================================================================= \n\n\nCode\n# Extract and print the estimated parameters\nparam &lt;- coef(cub_00, digits = 3)\nparam\n\n\n         \npai 0.680\ncsi 0.197\n\n\nCode\n# Calculate and print the uncertainty and feeling estimates\nuncertainty &lt;- 1 - param[1]\nuncertainty\n\n\n[1] 0.32\n\n\nCode\nfeeling &lt;- 1 - param[2]\nfeeling\n\n\n[1] 0.803\n\n\nThe makeplot() function can be used to compare the observed relative frequencies with the theoretical probabilities predicted by the estimated model.\n\n\nCode\n## Create a comparison plot\nmakeplot(cub_00)\n\n\n\n\n\nCUB Models with Covariates To improve the model’s fit and interpretation, covariates can be introduced. In this example, we consider the dichotomous covariate freqserv (0 = non-regular users, 1 = regular users), which indicates the service usage frequency.\nA model is estimated where the feeling component depends on the freqserv covariate, while the uncertainty remains constant. The model formula is officeho ~ 0 | freqserv | 0, indicating that the feeling component is modeled as a function of freqserv (~ freqserv), while the other components are constant (~ 0).\n\n\nCode\n# Estimate the CUB model with freqserv as a covariate on feeling\ncub_csi &lt;- GEM(Formula(officeho ~ 0 | freqserv | 0), family = \"cub\", data = univer)\n\n# Display the model summary\nsummary(cub_csi, digits = 3)\n\n\n======================================================================= \n=====&gt;&gt;&gt; CUB  model    &lt;&lt;&lt;=====   ML-estimates via E-M algorithm   \n======================================================================= \n m= 7  Sample size: n= 2179  Iterations= 24  Maxiter= 500 \n======================================================================= \nUncertainty                                            \n    Estimates StdErr Wald\npai     0.687 0.0192 35.9\n======================================================================= \nFeeling                                            \n         Estimates StdErr   Wald\nconstant    -1.152 0.0403 -28.58\nfreqserv    -0.811 0.0850  -9.55\n======================================================================= \nLog-lik            = -3704.357 \nMean Log-likelihood= -1.7 \n----------------------------------------------------------------------- \nAIC       = 7414.714 \nBIC       = 7431.773 \nICOMP     = 7410.96 \n======================================================================= \nElapsed time= 0.57 seconds =====&gt;&gt;&gt; Wed Jul  2 20:48:02 2025 \n======================================================================= \n\n\nThe regression coefficients for the logistic link of the “feeling” can be extracted and analyzed.\n\n\nCode\n# Extract the regression coefficients\ngama &lt;- coef(cub_csi)[2:3]\ngama\n\n\n[1] -1.151942 -0.811228\n\n\nCode\n# Separate the intercept from the covariate's coefficient\ngama0 &lt;- gama[1] # Intercept term\ngama1 &lt;- gama[2] # Coefficient for freqserv\n\n\nUsing these coefficients, you can calculate the feeling parameter for each group of users (regular and non-regular). The logis() function calculates the inverse logistic transformation.\n\n\nCode\n# Calculate the csi parameter for non-regular users (freqserv = 0)\ncsi_nru &lt;- logis(0, gama)\ncsi_nru\n\n\n[1] 0.2401346\n\n\nCode\n# Calculate the csi parameter for regular users (freqserv = 1)\ncsi_ru &lt;- logis(1, gama)\ncsi_ru\n\n\n[1] 0.1231244\n\n\nIn this model, the feeling component is specified by:\n\\[\nlogit(1-\\xi_i) = - \\gamma_0 - \\gamma_1 freqserv_i, \\quad i = 1,\\dots,n\n\\] A plot can be generated to compare the fitted probability distributions for the two groups.\n\n\nCode\n# Compare the fitted probability distributions for the two user groups\nmakeplot(cub_csi)\n\n\n\n\n\n\n\nCode\nBIC(cub_00); BIC(cub_csi)\n\n\n[1] 7535.208\n\n\n[1] 7431.773\n\n\nthe reduction in BIC index from BIC(cub_00)= 7535.208 to BIC(cub_csi)= 7431.773 strongly supports the inclusion of the covariate freqserv in the model.\nCUB Model with Covariates on Both Uncertainty and Feeling\nIn this more complex example, two covariates are considered jointly: lage (the deviation from the mean of the logarithmic transform of age) and gender. The variable lage is used to explain both uncertainty and feeling, while gender is used for uncertainty and freqserv for feeling.\n\n\nCode\ndata(univer)\n# Calculate the deviation from the mean of log(age)\nage &lt;- univer$age\nlage &lt;- log(age) - mean(log(age))\n\n# Estimate the CUB model with covariates on both parameters\ncub_pai_csi &lt;- GEM(Formula(officeho ~ lage + gender | lage + freqserv | 0), family = \"cub\", data = univer)\n\n# Display the summary with coefficient correlations\nsummary(cub_pai_csi, correlation = TRUE, digits = 3)\n\n\n======================================================================= \n=====&gt;&gt;&gt; CUB  model    &lt;&lt;&lt;=====   ML-estimates via E-M algorithm   \n======================================================================= \n m= 7  Sample size: n= 2179  Iterations= 23  Maxiter= 500 \n======================================================================= \nUncertainty                                            \n         Estimates StdErr Wald\nconstant     0.563  0.118 4.78\nlage         1.240  0.613 2.02\ngender       0.495  0.169 2.94\n======================================================================= \nFeeling                                            \n         Estimates StdErr   Wald\nconstant    -1.147 0.0404 -28.41\nlage        -0.590 0.2410  -2.45\nfreqserv    -0.824 0.0851  -9.68\n======================================================================= \nParameters Correlation matrix \n           constant       lage     gender   constant       lage   freqserv\nconstant  1.0000000 -0.0506498 -0.6428539  0.1733713  0.0186039  0.1264982\nlage     -0.0506498  1.0000000  0.0915196 -0.0064633  0.2750023 -0.0028018\ngender   -0.6428539  0.0915196  1.0000000 -0.0158130 -0.0255944  0.0369170\nconstant  0.1733713 -0.0064633 -0.0158130  1.0000000 -0.0992397 -0.4153292\nlage      0.0186039  0.2750023 -0.0255944 -0.0992397  1.0000000  0.0802712\nfreqserv  0.1264982 -0.0028018  0.0369170 -0.4153292  0.0802712  1.0000000\n======================================================================= \nLog-lik            = -3693.888 \nMean Log-likelihood= -1.695 \n----------------------------------------------------------------------- \nAIC       = 7399.776 \nBIC       = 7433.895 \nICOMP     = 7396.68 \n======================================================================= \nElapsed time= 2 seconds =====&gt;&gt;&gt; Wed Jul  2 20:48:04 2025 \n======================================================================= \n\n\nThe estimated coefficients correspond to the logistic regression coefficients for the uncertainty and feeling components.\n\n\nCode\n# Print the estimated coefficients for both logistic models\ncoef(cub_pai_csi, digits = 3)\n\n\n               \nconstant  0.563\nlage      1.240\ngender    0.495\nconstant -1.147\nlage     -0.590\nfreqserv -0.824\n\n\nThen, the resulting cub model can be summarized by:\n\\[\n\\text{logit}(1 - \\pi_i) = -\\beta_0 - \\beta_1 \\cdot \\text{lage}_i - \\beta_2 \\cdot \\text{gender}_i\n\\]\n\\[\n\\text{logit}(1 - \\xi_i) = -\\gamma_0 - \\gamma_1 \\cdot \\text{lage}_i - \\gamma_2 \\cdot \\text{freqserv}_i\n\\]\nSince no plot is directly provided as output, the performance of the cub model with significant covariates on feeling and uncertainty parameters may be summarized as in the following figure, obtained by the following code:\n\n\nCode\ndata(univer)\nage&lt;-univer$age\naverage&lt;-mean(log(age))\nageseq&lt;-log(seq(17, 51, by = 0.1))-average\nparam&lt;-coef(cub_pai_csi)\n####################\npaicov0&lt;-logis(cbind(ageseq, 0), param[1:3])\npaicov1&lt;-logis(cbind(ageseq, 1), param[1:3])\ncsicov0&lt;-logis(cbind(ageseq, 0), param[4:6])\ncsicov1&lt;-logis(cbind(ageseq, 1), param[4:6])\n####################\nplot(1-paicov0, 1-csicov0, type = \"n\", col = \"blue\", cex = 1,\nxlim = c(0, 0.6), ylim = c(0.4, 0.9), font.main = 4, las = 1,\nmain = \"CUB models with covariates\",\nxlab = expression(paste(\"Uncertainty \", (1-pi))),\nylab = expression(paste(\"Feeling \", (1-xi))), cex.main = 0.9,\ncex.lab = 0.9)\nlines(1-paicov1, 1-csicov1, lty = 1, lwd = 4, col = \"red\")\nlines(1-paicov0, 1-csicov0, lty = 1, lwd = 4, col = \"blue\")\nlines(1-paicov0, 1-csicov1, lty = 1, lwd = 4, col = \"black\")\nlines(1-paicov1, 1-csicov0, lty = 1, lwd = 4, col = \"green\")\nlegend(\"bottomleft\", legend = c(\"Man-User\", \"Man-Not User\",\n\"Woman-User\", \"Woman-Not User\"), col = c(\"black\", \"blue\", \"red\", \"green\"),\nlty = 1, text.col = c(\"black\", \"blue\", \"red\", \"green\"), cex = 0.6)\ntext(0.1, 0.85, labels = \"Young\", offset = 0.3, cex = 0.8, font = 4)\ntext(0.5, 0.5, labels = \"Elderly\", offset = 0.3, cex = 0.8, font = 4)\n\n\n\n\n\nSummarizing, the sketch of analysis so pursued indicates that satisfaction increase with age whereas indecision decreases, and that men are more satisfied than women across all profiles."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#cub-model-with-shelter-option",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#cub-model-with-shelter-option",
    "title": "Ordinal Data Analysis in R",
    "section": "3.4 CUB Model with Shelter Option",
    "text": "3.4 CUB Model with Shelter Option\nWhile the basic CUB model effectively captures the feeling and uncertainty in ordinal responses, real-world rating data often exhibit additional complexities.\nOne such common phenomenon is the tendency for respondents to disproportionately select a specific category, not entirely explained by their true underlying feeling or by simple random guessing. This over-selection of a particular category suggests it acts as a “shelter” or “refuge” for some respondents.\nThe shelter effect describes a situation where a specific category on an ordinal scale receives an “extra” probability mass, beyond what would be predicted solely by the respondent’s underlying feeling or by pure random uncertainty. This happens because some respondents might gravitate towards this category for reasons unrelated to their precise preference or indecision.\nReasons for this “shelter-seeking” behavior have been extensively discussed in the literature (e.g., Iannario, 2012; Piccolo & Simone, 2019):\n\nCognitive Simplification: Respondents may choose an easy, or cognitively less demanding option to reduce mental effort. This could be the middle category (e.g., “Neutral”), a neutral option specifically provided, or even the first/last option if it serves as an easy default.\nFatigue or Boredom: Especially prevalent in long questionnaires or surveys, fatigue can lead respondents to disengage and simply pick a convenient category rather than carefully considering their true response.\nSocial Desirability or Privacy Concerns: Individuals might select a “safe”, non-committal, or socially acceptable answer to avoid expressing a strong or controversial opinion, or to protect their privacy. The neutral option often serves this purpose.\nQuestionnaire Design: Poorly worded questions, ambiguous scale anchors, or an overwhelming number of options might inadvertently guide respondents towards a default or ambiguous middle ground.\nSatisficing: This refers to the tendency to select a minimally acceptable response rather than the optimal one, often to save cognitive resources. The shelter option becomes the “good enough” answer.\n\nThe critical implication of the shelter effect is that the chosen category \\(c\\) has an “extra” probability mass. This means the observed frequency for category \\(c\\) is higher than what a standard two-component CUB model would predict. Effectively, a subset of respondents might be selecting \\(c\\) directly, bypassing the usual feeling-uncertainty decision process.\n\n3.4.1 Defining the shelter category\nThe shelter category, denoted by \\(c\\) (where \\(c \\in \\{1,2,…,m\\}\\)), is a specific category on the ordinal scale that exhibits this inflated probability mass. Identifying this category is a crucial preliminary step. It can be:\n\nHypothesized a priori: In many cases, the shelter category can be hypothesized beforehand based on the scale’s design. For instance, on a 5-point Likert scale (1=Strongly Disagree, 5=Strongly Agree), the central category \\(c=3\\) (“Neutral” or “Neither agree nor disagree”) is a very common candidate for a shelter option. Other possibilities could be the minimum (\\(c=1\\)) or maximum (\\(c=m\\)) category if they function as easy “opt-out” or default choices.\nIdentified Empirically: If no a priori hypothesis exists, the shelter category can be identified empirically. This involves observing an unusually high frequency for a particular category that is not well explained by a simple CUB model. A large positive residual for a specific category from a basic CUB fit can point towards a potential shelter effect.\n\nOnce identified, the shelter category \\(c\\) is treated as fixed in the model estimation.\n\n\n3.4.2 Statistical Formulation of CUB Model with Shelter Option\nTo account for the shelter effect, the basic CUB model is extended into a three-component mixture model. A common and highly interpretable approach is the CUB with Shelter (CUSH) model. This model introduces a third component: a degenerate distribution that assigns all probability mass exclusively to the shelter category \\(c\\).\nThe probability mass function (PMF) for the CUSH model for a rating \\(R=r\\) is given by:\n\\[\nP(R = r\\mid \\pi, \\xi, \\delta) = \\delta \\Big[D_c(r)\\Big] + (1-\\delta)\\Big[\\pi B(r \\mid \\xi) + (1-\\pi)U(r)\\Big]\n\\]\nwhere there is an additional component, compared to the basic CUB model: the degenerate distribution fir the shelter category \\(c\\), \\(D_c(r)\\):\n\\[\nD_c(r) =\n\\begin{cases}\n    1, & \\text{if } r = s; \\\\\n    0, & \\text{otherwise;}\n\\end{cases}\n\\qquad r = 1, \\dots, m.\n\\]\nThe parameter \\(\\delta \\in [0,1]\\) represents the probability of choosing the shelter category \\(c\\) directly. This is the weight assigned to the degenerate distribution. A higher \\(\\delta\\) indicates a stronger tendency for respondents to opt for the designated shelter category, irrespective of their true feeling or general uncertainty.\nModel selection\nModel selection is crucial when deciding whether a CUSH model offers a significantly better fit than a simpler CUB model. Standard statistical criteria can be used:\nInformation criteria like Akaike Information Criterion and Bayesian information Criterion are widely used to balance model fit with model complexity.\nLower values for AIC and BIC indicate a better-fitting model. BIC is often preferred in model selection for CUB models as it penalizes model complexity more heavily, which can help in choosing more parsimonious models.\nAfter fitting a basic CUB model, it’s good practice to examine the residuals, which are the differences between observed and fitted probabilities/frequencies. If a simple CUB model shows a large positive residual for a specific category, it strongly suggests the presence of a shelter effect for that category. The Dissimilarity Index (\\(Diss\\)) can also be used to compare the overall fit improvement when a shelter component is added.\n\n\n3.4.3 Case study: the reelgoods dataset\n\n\n\n  R Script \n\nThe CUB model with shelter option is shown with the relgoods dataset, a survey designed to assess the importance of relational goods and involvement in leisure time activities.\nThis dataset contains the results of a survey conducted in December 2014 in the metropolitan area of Naples, Italy. The survey’s primary goal was to measure people’s evaluation of relational goods (e.g., time spent with friends and family) and leisure time activities. The dataset includes both demographic information about the respondents and their evaluations on various topics.\nThe survey used two main types of scales to collect data:\n\n10-Point Ordinal Scale (Likert-type): Respondents rated several items on a 10-point scale (from 1 to 10). The interpretation of the scale endpoints varied slightly:\n\n\nFor relational goods: 1 = “never, at all” to 10 = “always, a lot”.\nFor leisure time activities: 1 = “At all, nothing, never” to 10 = “Totally, extremely important, always”. This scale captures the frequency or importance of a given activity.\n\n\nContinuous Happiness Scale: To measure happiness, respondents marked a sign on a 110mm horizontal line (left = “extremely unhappy”, right = “extremely happy”). This is treated as a continuous variable.\n\nThe dataset includes the following types of variables:\n\nSubject Covariates (Demographics): These provide background information about each respondent (e.g., ID, Gender, BirthMonth, EducationDegree, MaritalStatus, job, Smoking, Pets).\nRelational Goods and General Evaluations (10-Point Scale): These variables capture aspects of social life and the environment (e.g., WalkOut, RelFriends, Safety).\nLeisure Time Activities (10-Point Scale): These measure involvement or enjoyment of various activities (e.g., Reading, Cinema, Sport, Internet, MusicInstr).\nContinuous Happiness Score: The self-reported happiness score (Happiness).\n\nLet’s inspect the distribution of the variable Writing, which asks respondents to rate their involvement in writing on a 10-point scale.\n\n\nCode\nlibrary(CUB)\ndata(relgoods)\n\n# Plot the frequency distribution of the 'MusicInstr' variable\nplot(table(as.factor(relgoods$Writing)))\n\n\n\n\n\nLet’s apply the CUB model to the Writing variable to illustrate the shelter effect. This variable asks about involvement in writing on a 10-point scale.\nFirst, we fit a standard CUB model without the shelter option to the Writing variable. This serves as a baseline for comparison.\n\n\nCode\n# Fit a standard CUB model to the 'Writing' variable\ncub_writing &lt;- GEM(Formula(Writing ~ 0 | 0 | 0), family = \"cub\",\n                  maxiter = 500, toler = 1e-3, data = relgoods)\nsummary(cub_writing)\n\n\n======================================================================= \n=====&gt;&gt;&gt; CUB  model    &lt;&lt;&lt;=====   ML-estimates via E-M algorithm   \n======================================================================= \n m= 10  Sample size: n= 2449  Iterations= 18  Maxiter= 500 \n======================================================================= \nUncertainty                                            \n    Estimates     StdErr     Wald\npai   0.37887 0.01354622 27.96869\n======================================================================= \nFeeling                                            \n    Estimates     StdErr    Wald\ncsi   0.98052 0.00275681 355.672\n======================================================================= \nLog-lik            = -4920.633 \nMean Log-likelihood= -2.009242 \nLog-lik(UNIFORM)   = -5639.031 \nLog-lik(saturated) = -4859.017 \nDeviance           = 123.2317 \n----------------------------------------------------------------------- \nAIC       = 9845.266 \nBIC       = 9856.873 \nICOMP     = 9843.336 \n======================================================================= \nElapsed time= 0.01 seconds =====&gt;&gt;&gt; Wed Jul  2 20:48:05 2025 \n======================================================================= \n\n\nCode\nmakeplot(cub_writing)\n\n\n\n\n\nNow, we introduce the shelter parameter, which models the probability of selecting a specific category as a refuge. In this case, we specify shelter = 1, which indicates that the first category (score of 1) is the shelter category.\n\n\nCode\n# Fit a CUB model with a shelter option on category 1\ncub_she &lt;- GEM(Formula(Writing ~ 0 | 0 | 0), family = \"cub\", shelter = 1,\n               maxiter = 500, toler = 1e-3, data = relgoods)\nsummary(cub_she)\n\n\n======================================================================= \n=====&gt;&gt;&gt; CUB  model    &lt;&lt;&lt;=====   ML-estimates via E-M algorithm   \n======================================================================= \n m= 10  Sample size: n= 2449  Iterations= 77  Maxiter= 500 \n======================================================================= \n======================================================================= \n     Estimates     StdErr     Wald\npai1 0.1922306 0.01652574 11.63219\npai2 0.4902216 0.01907354 25.70166\ncsi  0.7518936 0.01433876 52.43784\n======================================================================= \nAlternative parameterization \n        Estimates     StdErr     Wald\npaistar 0.2816762 0.02422031 11.62975\ncsi     0.7518936 0.01433876 52.43784\ndelta   0.3175478 0.01121160 28.32315\n======================================================================= \nLog-lik            = -4887.843 \nMean Log-likelihood= -1.995853 \nLog-lik(UNIFORM)   = -5639.031 \nLog-lik(saturated) = -4859.017 \nDeviance           = 57.65222 \n----------------------------------------------------------------------- \nAIC       = 9781.686 \nBIC       = 9799.096 \nICOMP     = 9776.957 \n======================================================================= \nElapsed time= 0.02 seconds =====&gt;&gt;&gt; Wed Jul  2 20:48:05 2025 \n======================================================================= \n\n\nCode\nmakeplot(cub_she)\n\n\n\n\n\nIf we look at the value of \\(\\delta\\), it indicates that there is a medium tendency to opt for the shelter category."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#treatment-of-dont-know-dk-options-within-the-cub-framework",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#treatment-of-dont-know-dk-options-within-the-cub-framework",
    "title": "Ordinal Data Analysis in R",
    "section": "3.5 Treatment of “Don’t Know” (DK) Options within the CUB Framework",
    "text": "3.5 Treatment of “Don’t Know” (DK) Options within the CUB Framework\nBeyond the core rating scale, surveys often include an option for respondents to indicate “Don’t Know” (DK), “No Opinion”, “Not Applicable”, or similar non-substantive responses. While seemingly innocuous, the handling of these responses presents a significant challenge for traditional statistical modeling, as they often don’t fit neatly into standard analytical frameworks.\nThe CUB framework, however, offers a theoretically robust and psychologically insightful approach to incorporating information from DK responses.\n“Don’t Know” responses are common in surveys, and their treatment is critical for valid statistical inference. They are not merely missing data points; rather, they represent an active choice that reflects a specific cognitive or attitudinal state of the respondent.\nDK responses are problematic for several reasons:\n\nNot Simply Missing Data: Treating DKs as simple missing values and discarding them (listwise deletion) can lead to biased samples and results. If respondents who choose DK differ systematically from those who provide substantive ratings, removing them can distort the representativeness of the remaining sample, affecting parameter estimates and generalizability.\nImputation Issues: Imputing a value for a DK response is inherently difficult and often relies on strong, untestable assumptions. Assigning a central value (e.g., the mean or median of the substantive responses) might mask genuine uncertainty, while more complex imputation methods (e.g., multiple imputation) can be challenging to implement and interpret accurately in the context of ordinal data.\nAdding DK as a Category: A seemingly straightforward approach is to include DK as just another category on the ordinal scale. However, this fundamentally breaks the ordinal nature of the scale. For example, a sequence like “Strongly Disagree, Disagree, DK, Agree, Strongly Agree” is conceptually problematic. The DK option is not naturally ordered between “Disagree” and “Agree”; it represents a different type of response altogether. Its inclusion disrupts the monotonic relationship implied by ordinal categories.\n\nTo properly handle DK responses, it is essential to understand their underlying meanings. Research suggests that “Don’t Know” can have several psychological interpretations:\n\nTrue Lack of Knowledge/Opinion: The respondent genuinely has no information, experience, or hasn’t formed an opinion on the topic. Unwillingness to Answer: The respondent might have an opinion but chooses not to express it due to a sensitive topic, privacy concerns, or wanting to appear socially desirable (e.g., not wanting to seem extreme or uninformed).\nInability to Map Opinion to Scale: The respondent might have an opinion but feels the provided scale categories are inadequate, too vague, or don’t quite fit their nuanced view. They might feel their true sentiment falls “between” categories or doesn’t align with any.\nQuestion Ambiguity: The respondent simply might not understand the question, its underlying assumptions, or the terms used, leading them to pick DK as a way out.\n\nThe varied meanings of DK emphasize that it’s a rich source of information, not just data to be thrown away or arbitrarily filled in.\nThe approach for handling DK responses in the CUB framework is to think of the total population as having two unobserved (latent) groups:\n\nGroup A=0: Those who can give a substantive rating on the m-point scale. These people have a genuine underlying feeling or are able to make a choice, even if that choice has some general uncertainty.\nGroup A=1: Those who cannot (or would not) and would genuinely choose DK if it were an option. This group essentially represents the “true” non-responders when it comes to having a substantive opinion.\n\nWe’ll use \\(p_{DK}\\) to represent the proportion of individuals in the population who would choose DK.\n\n3.5.1 Modeling Assumptions for Latent Groups\nThis approach makes specific assumptions about how each latent group generates responses:\nFor those who can answer (Group A=0, proportion \\((1-p_{DK})\\)): When these individuals provide a substantive rating (\\(R=r\\)), their responses are assumed to follow a standard CUB model. This CUB model has its own parameters, \\(\\pi_0\\) (for uncertainty within this group) and \\(\\xi_0\\) (for feeling within this group), which describe the feeling and uncertainty among those capable of providing an opinion.\n\\[\nP(R = r \\mid A = 0, \\pi_0, \\xi_0) = \\pi_0B(r\\mid\\xi_0) + (1-\\pi_0)U(r)\n\\]\nFor those who would choose DK (Group A=1, proportion \\(p_{DK}\\)), if these individuals are forced to pick a category from the m-point scale (e.g., if the DK option isn’t available, or if their preference for DK is part of their general uncertainty), their choice isn’t based on a genuine “feeling”. Instead, their responses are driven purely by randomness or uncertainty across the available options. So, their responses are modeled by a discrete Uniform distribution.\n\\[\nP(R = r\\mid A =1) =U(r)= \\frac{1}{m}\n\\]\nThe overall observed distribution of ratings, if all respondents are forced to choose from the m-point scale (meaning no explicit DK option is given, or if we consider the hypothetical responses of those who would pick DK if it were there), is a mix of the CUB model for “knowers” and the Uniform distribution for those “forced” to choose.\n\n\n3.5.2 Adjusting CUB Parameters using Observed DKs\nThe method proposed in this framework uses the presence of DKs to adjust the fundamental uncertainty in the main CUB model.\nWhen DK responses are explicitly allowed in a survey and are present in the data the approach proceeds as follows:\n\nEstimate \\(p_{DK}\\): This is simply the observed proportion of DK responses in the sample.\n\n\\[\n\\hat{p}_{DK} = \\frac{Total\\, number\\, of\\, DK\\, responses}{Total\\, number\\, of\\, responses}\n\\]\n\nFocus on Substantive Responders: The remaining \\((1- \\hat{p}_{DK})\\) proportion of the sample consists of individuals who gave a rating on the m-point ordinal scale (i.e., they picked a category from \\(1,…,m\\)). Let \\(N_{sub}\\) be the number of these substantive responders.\nModel for Substantive Responses: A standard CUB model is then fitted only to these \\(N_{sub}\\) substantive responses. This fitting process gives us estimates for their underlying parameters, called \\(\\pi_S\\) and \\(\\xi_S\\). This model describes the probability distribution of ratings given that a substantive response was provided: \\[\nP(R = r \\mid Substantive) = \\pi_SB(r\\mid\\xi_S) + (1-\\pi_S)U(r)\n\\]\nRelating to the Overall Population Parameters: The crucial step is to link these parameters \\((\\pi_S, \\xi_S)\\) back to the overall population’s true feeling and uncertainty, taking into account the proportion of DKs.\n\nThe feeling parameter for the overall population, \\((1-\\xi)\\), is considered to be best represented by the feeling of those who actually gave a substantive rating. This is based on the idea that people who genuinely say “Don’t Know” don’t contribute to the “feeling” aspect of the scale. So: \\[\n(1-\\xi) = (1-\\xi_S)\n\\]\nThe overall uncertainty parameter for the population, \\((1-\\pi)\\), comes from two sources:\n\nThe inherent uncertainty among those who could answer (captured by \\(1-\\pi_S\\)).\nThe complete uncertainty of those who chose DK (who are considered 100% uncertain regarding the m-point scale). The overall \\(\\pi\\) for the population (the probability that a response is driven by feeling) is then calculated as:\n\n\n\n\\[\n\\pi = (1-\\hat{p}_{DK})\\cdot\\pi_S\n\\]\nThis means the overall probability that a response is based on feeling (\\(\\pi\\)) is the probability that a respondent isn’t a DK type \\((1-\\hat{p}_{DK})\\) multiplied by the probability that, given they aren’t a DK type, they respond based on feeling \\((\\pi_S)\\).\nConsequently, the overall uncertainty for the population is:\n\\[\n(1-\\pi) = 1-(1-\\hat{p}_{DK})\\cdot\\pi_S\n\\]\nThis can be rewritten as:\n\\[\n(1-\\pi) = \\hat{p}_{DK} + (1-\\hat{p}_{DK})\\cdot(1-\\pi_S)\n\\]\nThis last equation shows that the overall uncertainty in the population is the sum of the proportion of individuals who selected DK \\((\\hat{p}_{DK})\\) and the proportion of uncertainty among those who provided a substantive response \\((1-\\pi_S)\\), weighted by their share of the total population \\((1-\\hat{p}_{DK})\\).\n\n\n3.5.3 Case study Eurobarometer’s data\n\n\n\n  R Script \n\n\n  R Data \n\nIn this section, we examine data from Standard Eurobarometer 81, a cross-national public opinion survey carried out in 2014 among citizens aged 15 and over across the 28 member states of the European Union (EU).\nThe goal is to investigate how European citizens perceive the functioning, role, and future of the EU, with a particular focus on the treatment of Don’t Know (DK) responses in the analysis of ordinal data. Our attention is centered on a block of six attitudinal items (QA19.1–QA19.6), in which respondents rated their agreement on a four-point ordinal scale (totally disagree, tend to disagree, tend to agree, totally agree), with the possibility to select a DK option indicating uncertainty or lack of opinion. These items are intended to capture both cognitive understanding and emotional evaluation of the EU and related issues, such as globalization and European integration. By explicitly modeling DK responses, we gain insight not only into the direction of public opinion but also into the levels of uncertainty and ambivalence that shape attitudes toward complex political institutions like the EU.\nThe attitudinal items are:\n\nQA19.1: I understand how the EU works\nQA19.2: Globalisation is an opportunity for economic growth\nQA19.3: (OUR COUNTRY) could better face the future outside the EU\nQA19.4: The EU should develop further into a federation of nation states\nQA19.5: More decisions should be taken at EU level\nQA19.6: We need a united Europe in today’s world\n\nLet’s start with the fitting of a CUB model for each variable by excluding the DK option.\n\n\nCode\nlibrary(ggplot2)\nlibrary(CUB)\n# Load the data\nload(\"materials//microdata_eurobarometer.Rdata\")\n\n\n\n\nCode\nstr(eurobarometer)\n\n\n'data.frame':   2800 obs. of  7 variables:\n $ Country: chr  \"BE\" \"BE\" \"BE\" \"BE\" ...\n $ D19-1  : chr  \"2\" \"2\" \"3\" \"4\" ...\n $ D19-2  : chr  \"dk\" \"2\" \"dk\" \"3\" ...\n $ D19-3  : chr  \"4\" \"3\" \"3\" \"4\" ...\n $ D19-4  : chr  \"3\" \"dk\" \"2\" \"4\" ...\n $ D19-5  : chr  \"2\" \"1\" \"3\" \"2\" ...\n $ D19-6  : chr  \"4\" \"1\" \"1\" \"1\" ...\n\n\nCode\n # Estimate a CUB model for each item\nCUB_list &lt;- list ()\npi &lt;- c()\nxi &lt;- c()\nfor (i in 2:7){\n  ordinal &lt;- eurobarometer[,i]\n  ordinal &lt;- as.numeric(ordinal[-which(ordinal == \"dk\")])\n  CUB_list[[i-1]] &lt;- GEM(Formula(ordinal ~ 0 | 0 | 0), family = \"cub\")\n  pi[i-1] &lt;- CUB_list[[i-1]]$estimates[1]\n  xi[i-1] &lt;- CUB_list[[i-1]]$estimates[2]\n}\n\nparam &lt;- data.frame(cbind(c(\"QA19.1\", \"QA19.2\",\n                            \"QA19.3\", \"QA19.4\",\n                            \"QA19.5\", \"QA19.6\"), pi, xi))\n\ncolnames(param) &lt;- c(\"label\", \"x\", \"y\")\n\n\nparam$x &lt;- as.numeric(param$x)\nparam$y &lt;- as.numeric(param$y)\n\n\nplot &lt;- ggplot(param, aes(x = (1-x), y = (1-y), label = label)) +\n  geom_point(size = 3) +\n  geom_text(vjust = -1, hjust = 0.5) +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\") +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_bw() +\n  labs(x = expression((1-pi)), y = expression((1-xi))) +\n  theme(panel.grid = element_blank())\nplot\n\n\n\n\n\nNow, let’s see how the estimates of \\(\\pi\\) change by considering the DK option. First of all we have to compute the following quantity for each item:\n\\[\n\\hat{p}_{DK} = \\frac{Total\\, number\\, of\\, DK\\, responses}{Total\\, number\\, of\\, responses}\n\\]\n\n\nCode\np_DK &lt;- c()\n\n# Loop over the 6 items\nfor (i in 2:7) {\n  item_responses &lt;- eurobarometer[, i]\n  total_responses &lt;- length(item_responses)\n  dk_count &lt;- sum(item_responses == \"dk\", na.rm = TRUE)\n  p_DK[i - 1] &lt;- dk_count / total_responses\n}\n\n# Assign labels to the vector\nnames(p_DK) &lt;- paste0(\"QA19.\", 1:6)\n\n# Display the result\np_DK\n\n\n    QA19.1     QA19.2     QA19.3     QA19.4     QA19.5     QA19.6 \n0.03821429 0.14964286 0.10500000 0.21928571 0.10107143 0.07000000 \n\n\nCode\nparam$p_DK &lt;- as.numeric(p_DK)\n\n\nThe “corrected” \\(\\pi\\) is computed as \\(\\pi = (1-\\hat{p}_{DK})\\cdot\\pi_S\\).\n\n\nCode\nparam$x_cor &lt;- (1- param$p_DK)*param$x\n\n\nThen we can plot the estimated models by considering the DK option:\n\n\nCode\nparam$x_old &lt;- 1 - param$x\nparam$x_new &lt;- 1 - param$x_cor\nparam$y_val &lt;- 1 - param$y\n\nggplot(param) +\n  geom_segment(aes(x = x_old, y = y_val, xend = x_new-0.015, yend = y_val),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = \"darkgray\") +\n  geom_point(aes(x = x_old, y = y_val), color = \"gray30\", size = 2) +\n  geom_point(aes(x = x_new, y = y_val), color = \"black\", size = 3) +\n  geom_text(aes(x = x_new, y = y_val, label = label),\n            vjust = -1, hjust = 0.5, size = 3.5) +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\") +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +\n  theme_bw() +\n  labs(x = expression(1 - pi), y = expression(1 - xi)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\nThis plot shows that globally Europeans have quite negative feelings against Europe, since by looking at the results related to question QA19.6 people have a low feeling and low uncertainty, meaning that they generally don’t think that we need a united europe today and they are almost sure about that. A similar opinion is confirmed by the high feeling shown for item QA19.3 which suggest that people globally think that their own country could better face the future outside the EU.\nThe other items show a medium level of feeling, especially QA19.2 and QA19.4, the consideration of the DK option shifts relevantly the level of uncertainty, meaning that the interviewees are not always able to say whether globalization is perceived as an opportunity for economic growth and are not able to express their perception about the possible development of the EU into a federation of nation states.\nReferences and additional information\nFor those interested in learning more about CUB models, I suggest this paper, which provides a good overview of the main developments within the class of CUB models. Additional contributions and developments from my research group can be found on this page.\nMain References\nThe main references used for this module are:\n\nD’Elia, A., & Piccolo, D. (2005). A mixture model for preferences data analysis. Computational Statistics & Data Analysis, 49(3), 917-934.\nIannario, M. (2012). Modelling shelter choices in a class of mixture models for ordinal responses. Statistical Methods & Applications, 21(1), 1-22.\nManisera, M., & Zuccolotto, P. (2014). Modeling “don’t know” responses in rating scales. Pattern Recognition Letters, 45, 226-234.\nPiccolo, D., & Simone, R. (2019). The class of CUB models: statistical foundations, inferential issues and empirical evidence. Statistical Methods & Applications, 28, 389-435.\n\nR Packages\nThe models and datasets (univer, relgoods) used in this module are provided in the R package CUB"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nOrdinal Data Analysis in R\n\n\nJun 9, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/lectures.html",
    "href": "docs/lectures.html",
    "title": "Courses",
    "section": "",
    "text": "Title\n\n\nDate\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#case-study-analyzing-disciplinary-decisions-in-professional-soccer",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#case-study-analyzing-disciplinary-decisions-in-professional-soccer",
    "title": "Ordinal Data Analysis in R",
    "section": "2.5 Case Study: Analyzing Disciplinary Decisions in Professional Soccer",
    "text": "2.5 Case Study: Analyzing Disciplinary Decisions in Professional Soccer\nYou are working as a data analyst for a sports broadcaster preparing an in-depth segment on referee decisions and player discipline in professional soccer. Your task is to investigate which factors may influence the likelihood and severity of disciplinary actions taken against players during matches.\nTo support this analysis, you have access to a dataset containing information on over 2000 player-match observations. The key variable of interest is the level of disciplinary action taken by the referee, which is recorded as an ordinal outcome with three ordered categories:\n\nNone: no disciplinary action,\nYellow: the player received a yellow card (warning),\nRed: the player received a red card (ejection from the game).\n\nIn addition to the outcome, the dataset includes the following explanatory variables:\n\nn_yellow_25: number of yellow cards received by the player in the 25 games prior to the current match.\nn_red_25: number of red cards received by the player in the previous 25 games.\nposition: player’s position in the game — ‘D’ for defender or goalkeeper, ‘M’ for midfielder, ‘S’ for striker.\nlevel: competitive level of the match — 1 for higher-tier games, 2 for lower-tier games.\ncountry: country where the match was played — England or Germany.\nresult: result of the game for the player’s team — ‘W’ (win), ‘L’ (loss), or ‘D’ (draw).\n\nYour objective is to model the disciplinary outcome and identify whether past player behavior, position, game context, and other covariates are associated with the referee’s decision, using statistical techniques suitable for ordinal response data.\n\n\nCode\n# download data\nurl &lt;- \"http://peopleanalytics-regression-book.org/data/soccer.csv\"\nsoccer &lt;- read.csv(url)\nhead(soccer)\n\n\n\n\n  \n\n\n\nCode\nstr(soccer)\n\n\n'data.frame':   2291 obs. of  7 variables:\n $ discipline : chr  \"None\" \"None\" \"None\" \"None\" ...\n $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n $ position   : chr  \"S\" \"D\" \"M\" \"M\" ...\n $ result     : chr  \"D\" \"W\" \"D\" \"L\" ...\n $ country    : chr  \"England\" \"England\" \"England\" \"Germany\" ...\n $ level      : int  1 2 1 1 1 1 2 1 1 1 ...\n\n\nBefore proceeding with the modeling phase, we need to ensure that the variables in our dataset are correctly specified in terms of data type. In particular, since our outcome variable discipline represents ordered categories—ranging from no punishment to a red card—we need to explicitly define it as an ordered factor in R. This is essential because many models for ordinal data (e.g., proportional odds models) rely on the natural ordering of the response variable.\nWe can perform this conversion using the ordered() function, specifying the levels in increasing order of severity:\n\n\nCode\n# convert discipline to ordered factor\nsoccer$discipline &lt;- ordered(soccer$discipline, \n                             levels = c(\"None\", \"Yellow\", \"Red\"))\n\n# check conversion\nstr(soccer)\n\n\n'data.frame':   2291 obs. of  7 variables:\n $ discipline : Ord.factor w/ 3 levels \"None\"&lt;\"Yellow\"&lt;..: 1 1 1 1 1 1 1 1 1 1 ...\n $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n $ position   : chr  \"S\" \"D\" \"M\" \"M\" ...\n $ result     : chr  \"D\" \"W\" \"D\" \"L\" ...\n $ country    : chr  \"England\" \"England\" \"England\" \"Germany\" ...\n $ level      : int  1 2 1 1 1 1 2 1 1 1 ...\n\n\nIn addition to the response variable, several explanatory variables in our dataset are also categorical in nature. Specifically, the variables position, country, result, and level represent categories rather than continuous measurements, and should therefore be converted to factors in R.\nWhile some of these variables could conceptually be treated as ordered—for example, result might be interpreted as “win &gt; draw &gt; loss”, and level as “level 1 &gt; level 2” in terms of competition quality—this is not strictly necessary when using them as predictor variables in a model. In practice, treating them as nominal (unordered) factors often leads to more interpretable model outputs.\nWe can convert them as follows:\n\n\nCode\n# Convert categorical predictors to factors\nsoccer$position &lt;- factor(soccer$position)\nsoccer$result   &lt;- factor(soccer$result)\nsoccer$country  &lt;- factor(soccer$country)\nsoccer$level    &lt;- factor(soccer$level)\n\n\nWe are now ready to fit the model using all available predictors. The clm() function within the ordinal package will automatically handle the creation of dummy variables.\n\n\nCode\nlibrary(ordinal)\n\n# Fit the full model\nmodel_clm &lt;- clm(\n  discipline ~ n_yellow_25 + n_red_25 + position + \n               result + country + level,\n  data = soccer\n)\n\n# View summary\nsummary(model_clm)\n\n\nformula: \ndiscipline ~ n_yellow_25 + n_red_25 + position + result + country + level\ndata:    soccer\n\n link  threshold nobs logLik   AIC     niter max.grad cond.H \n logit flexible  2291 -1722.27 3464.53 5(0)  4.43e-12 7.5e+02\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \nn_yellow_25     0.32236    0.03308   9.746  &lt; 2e-16 ***\nn_red_25        0.38324    0.04051   9.462  &lt; 2e-16 ***\npositionM       0.19685    0.11649   1.690   0.0911 .  \npositionS      -0.68534    0.15011  -4.566 4.98e-06 ***\nresultL         0.48303    0.11195   4.315 1.60e-05 ***\nresultW        -0.73947    0.12129  -6.097 1.08e-09 ***\ncountryGermany  0.13297    0.09360   1.421   0.1554    \nlevel2          0.09097    0.09355   0.972   0.3308    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n            Estimate Std. Error z value\nNone|Yellow   2.5085     0.1918   13.08\nYellow|Red    3.9257     0.2057   19.08\n\n\nAs expected, the summary displays:\n\nA single set of regression coefficients for the predictor variables, assuming the proportional odds assumption holds (i.e., the effects are constant across the thresholds of the ordinal response).\nTwo intercepts (also called cutpoints or thresholds) corresponding to the boundaries between the three ordered categories of the response variable (None, Yellow, Red).\n\nWhile the intercepts are required for model estimation, they are typically not the focus of interpretation. Instead, we concentrate on the coefficients of the predictors, which quantify the change in the (log) odds of receiving a higher level of discipline.\n\n\nCode\n# Odds ratios\nexp(coef(model_clm))\n\n\n   None|Yellow     Yellow|Red    n_yellow_25       n_red_25      positionM \n    12.2865823     50.6896260      1.3803820      1.4670350      1.2175573 \n     positionS        resultL        resultW countryGermany         level2 \n     0.5039204      1.6209823      0.4773655      1.1422177      1.0952321 \n\n\nBased on the p-values, we can now interpret the estimated coefficients for the significant predictors. In each case, we interpret the effect on the odds of receiving a more severe disciplinary action, holding all other variables constant.\n\nn_yellow_25: Each additional yellow card received in the previous 25 games is associated with a 38% increase in the odds of receiving more severe discipline from the referee. (Odds ratio ≈ 1.38)\nn_red_25: Each additional red card in the last 25 games corresponds to a 47% increase in the odds of greater disciplinary action. (Odds ratio ≈ 1.47)\nposition: Players in the Striker (S) role have about 50% lower odds of receiving greater disciplinary action compared to Defenders (D), the reference category. (Odds ratio ≈ 0.50)\n\nWe can conclude that:\n\nPlayers on teams that lost have 62% higher odds of greater disciplinary action than those whose team drew. (Odds ratio ≈ 1.62)\nPlayers on teams that won have 52% lower odds compared to those on teams that drew. (Odds ratio ≈ 0.48)\n\n\nSimplifying the Model\nWe may choose to simplify the model by removing predictors that are not statistically significant and do not substantially improve model fit. In this case, both level and country show little impact, either in terms of coefficient size or statistical significance.\nWe can refit the model without these variables and compare the AIC (Akaike Information Criterion) values. If the simpler model yields a comparable AIC, we may prefer it for interpretability and parsimony:\n\n\nCode\n# Reduced model (without level and country)\nmodel_clm_reduced &lt;- clm(\n  discipline ~ n_yellow_25 + n_red_25 + position + result,\n  data = soccer\n)\n\n# Compare AICs\nAIC(model_clm, model_clm_reduced)\n\n\n\n\n  \n\n\n\nSince the reduced model shows no meaningful degradation in fit, we proceed with it for the remainder of our analysis.\n\n\nGoodness of Fit\nWe can assess the fit and parsimony of a proportional odds model using pseudo-\\(R^2\\) measures and AIC.\n\n\nCode\n# Log-likelihoods\nll_full &lt;- logLik(model_clm_reduced)\nll_null &lt;- logLik(update(model_clm_reduced, . ~ 1))\n\n# Sample size\nn &lt;- nobs(model_clm_reduced)\n\n# McFadden's R2\nr2_mcfadden &lt;- 1 - (as.numeric(ll_full) / as.numeric(ll_null))\n\n# Cox-Snell's R2\nr2_coxsnell &lt;- 1 - exp((2 / n) * (as.numeric(ll_null) - as.numeric(ll_full)))\n\n# Report\ndata.frame(\n  McFadden = r2_mcfadden,\n  CoxSnell = r2_coxsnell,\n  AIC = AIC(model_clm_reduced)\n)\n\n\n\n\n  \n\n\n\n\nMcFadden’s \\(R^2\\) is conceptually similar to \\(R^2\\) in linear models but usually much smaller in value.\nCox–Snell and Nagelkerke are rescaled to be more interpretable on a 0–1 scale.\nAIC helps compare models: lower values indicate better parsimony.\n\n\n\nTesting the proportional odds assumption\nAs we discussed earlier, the suitability of a proportional odds logistic regression model depends on the assumption that each input variable has a similar effect on the different levels of the ordinal outcome variable. It is very important to check that this assumption is not violated before proceeding to declare the results of a proportional odds model valid. There are two common approaches to validating the proportional odds assumption, and we will go through each of them here.\n\nNominal Test: use nominal_test():\n\n\n\nCode\nnominal_test(model_clm_reduced)\n\n\n\n\n  \n\n\n\n\nNull hypothesis (\\(H_0\\)): PO assumption holds\nA small p-value (e.g. &lt; 0.05) indicates violation of the assumption for that variable.\nGraphical Inspection\n\n\n\nCode\nlibrary(ordinal)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Supponiamo che model_clm sia già il tuo modello \"semplice\"\nmodel_clm &lt;- clm(discipline ~ n_yellow_25 + n_red_25 + position + result, data = soccer)\n\n# Estrai coefficienti\nthresholds &lt;- model_clm$Theta              # soglie (intercette cumulative)\nbeta_n_yellow &lt;- coef(model_clm)[\"n_yellow_25\"]  # coefficiente principale\n\n# Simula una sequenza di valori del predittore continuo\nx_vals &lt;- seq(min(soccer$n_yellow_25), max(soccer$n_yellow_25), length.out = 100)\n\n# Calcola le log-odds cumulative per ogni soglia\nplot_data &lt;- data.frame(\n  x = x_vals,\n  Threshold_1 = thresholds[1] - beta_n_yellow * x_vals,\n  Threshold_2 = thresholds[2] - beta_n_yellow * x_vals\n)\n\n# Porta in formato long\nplot_data_long &lt;- plot_data %&gt;%\n  pivot_longer(\n    cols = starts_with(\"Threshold_\"),\n    names_to = \"Cumulative_Log_Odds_Curve\",\n    values_to = \"Log_Odds\"\n  )\n\n# Grafico\nggplot(plot_data_long, aes(x = x, y = Log_Odds, color = Cumulative_Log_Odds_Curve)) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Verifica grafica dell’assunzione di proportional odds\",\n    subtitle = \"Linee parallele delle log-odds cumulative → assunzione plausibile\",\n    x = \"Numero di cartellini gialli nelle 25 partite precedenti\",\n    y = \"Log-odds cumulative\",\n    color = \"Soglia cumulativa\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"P(Disciplina ≤ Yellow)\", \"P(Disciplina ≤ Red)\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nCode\nlibrary(ordinal)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Prepara i dati con valori medi per le variabili continue, e livelli per position\nnew_data &lt;- expand.grid(\n  position = levels(soccer$position),\n  n_yellow_25 = mean(soccer$n_yellow_25, na.rm = TRUE),\n  n_red_25 = mean(soccer$n_red_25, na.rm = TRUE),\n  result = levels(soccer$result)[1]  # primo livello di result, ad es. \"D\"\n)\n\n# Predici le probabilità cumulative (attenzione al tipo \"cum.prob\")\npred &lt;- predict(model_clm_reduced, newdata = new_data, type = \"cum.prob\")\n\n# Trasforma in data frame, aggiungi la posizione\nplot_data &lt;- as.data.frame(pred)\nplot_data$position &lt;- new_data$position\n\n# Metti in formato long per ggplot\nplot_data_long &lt;- pivot_longer(plot_data, cols = -position,\n                              names_to = \"Threshold\", values_to = \"Cumulative_Prob\")\n\n# Ordina posizione come fattore\nplot_data_long$position &lt;- factor(plot_data_long$position, levels = levels(soccer$position))\n\n# Calcola log-odds\nplot_data_long &lt;- plot_data_long %&gt;%\n  mutate(Log_Odds = log(Cumulative_Prob / (1 - Cumulative_Prob)))\n\n# Grafico\nggplot(plot_data_long, aes(x = position, y = Log_Odds, group = Threshold, color = Threshold)) +\n  geom_line(aes(linetype = Threshold), size = 1.2) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Log-odds cumulative per soglia in funzione di 'position'\",\n    subtitle = \"Verifica grafica dell’assunzione di proportional odds\",\n    x = \"Posizione\",\n    y = \"Log-odds cumulative\",\n    color = \"Soglia cumulativa\"\n  ) +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nCode\nconfint(model_clm_reduced, type = \"Wald\")\n\n\n                  2.5 %     97.5 %\nNone|Yellow  2.04607487  2.7520921\nYellow|Red   3.43369100  4.1959994\nn_yellow_25  0.25732251  0.3870545\nn_red_25     0.30204387  0.4606422\npositionM   -0.02744177  0.4290013\npositionS   -0.97101327 -0.3829654\nresultL      0.26472851  0.7029830\nresultW     -0.97178750 -0.4967697"
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#why-classical-multivariate-methods-fall-short",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#why-classical-multivariate-methods-fall-short",
    "title": "Ordinal Data Analysis in R",
    "section": "4.1 Why Classical Multivariate Methods Fall Short",
    "text": "4.1 Why Classical Multivariate Methods Fall Short\nThe inadequacy of classical multivariate methods for ordinal data stems from their underlying assumptions, which are typically violated by the properties of ordinal scales.\n\n4.1.1 Principal Component Analysis (PCA)\nClassical PCA is a dimensionality reduction technique built upon the covariance or correlation matrix of the variables. Its objective is to identify a set of orthogonal linear combinations of the original variables (principal components) that capture the maximum variance in the data. The method inherently assumes that the data are numerical and that the distances between values are meaningful and consistent.\nWhen PCA is applied directly to ordinal data (even after assigning numerical labels like 1, 2, 3, etc.), it treats these labels as if they represent continuous, interval-scaled quantities. This creates a critical misrepresentation. For example, PCA would treat the numerical “distance” between “agree” (e.g., coded as 4) and “strongly agree” (e.g., coded as 5) as identical to the “distance” between “neutral” (e.g., coded as 3) and “agree” (e.g., coded as 4). As previously discussed, these perceived “distances” on an ordinal scale are rarely equal in their true conceptual or psychological magnitude. By treating these non-uniform intervals as equal, PCA can generate principal components that distort the true underlying structure of the ordinal relationships, leading to potentially misleading interpretations of the latent dimensions.\n\n\n4.1.2 K-Means Clustering\nSimilarly, standard clustering algorithms like K-means rely heavily on Euclidean distances to quantify similarity or dissimilarity between data points. Euclidean distance, by its very definition, assumes interval-scaled variables where differences between values are direct and comparable.\nApplying K-means directly to numerically coded ordinal data leads to similar problems as with PCA. The calculated Euclidean distances between response profiles will be based on the arbitrary numerical assignments, not on the true, often unequal, conceptual distances between ordered categories. This can result in the formation of artificial clusters that do not genuinely reflect meaningful patterns or homogeneous groups within the ordinal data. Consequently, relevant patterns might be masked, or spurious groupings might emerge, hindering accurate segmentation and interpretation.\nTherefore, when confronted with multivariate ordinal data, it becomes important to use analytical tools that account for their ordered nature."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#techniques-for-dimensionality-reduction-and-visualization-of-ordinal-data",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#techniques-for-dimensionality-reduction-and-visualization-of-ordinal-data",
    "title": "Ordinal Data Analysis in R",
    "section": "4.2 Techniques for Dimensionality Reduction and Visualization of Ordinal Data",
    "text": "4.2 Techniques for Dimensionality Reduction and Visualization of Ordinal Data\nThe methods discussed in this section—Nonlinear PCA, Correspondence Analysis, and Multiple Correspondence Analysis—all share a common overarching goal: to reduce the dimensionality of complex multivariate data and to provide insightful graphical representations of the relationships within the data. While they achieve this through different mechanisms and for slightly different data structures, they collectively aim to uncover latent structures and visualize patterns in a lower-dimensional space.\n\n4.2.1 Nonlinear Principal Component Analysis (NLPCA)\nNonlinear Principal Component Analysis (NLPCA) represents a significant advancement over classical PCA, extending its framework to accommodate diverse data types, including ordinal variables. When applied to ordinal data, NLPCA addresses the core challenge by transforming the original categorical responses into optimal scores.\nThe fundamental idea behind NLPCA is to “translate” ordinal data into a numerical scale in a way that maximizes the variance explained by the resulting principal components, much like traditional PCA. However, unlike classical PCA, this “translation” is not fixed or arbitrary. Instead, NLPCA iteratively determines the most appropriate numerical values (optimal scores) for each category. These optimal scores are derived such that they best preserve the order of the categories and maximize the explained variance in the transformed data, effectively capturing potentially nonlinear relationships.\nBy employing optimal scaling, NLPCA can effectively handle data that are a mix of continuous, nominal, and ordinal variables, offering a flexible and powerful dimensionality reduction technique for heterogeneous datasets. The output of NLPCA is a set of principal components that reflect the major sources of variability, but crucially, they do so while respecting the intrinsic nature of the ordinal data. NLPCA is particularly valuable when one suspects that the latent constructs driving the data exhibit nonlinear relationships or are multidimensional in a non-Euclidean sense.\n\n\n4.2.2 Correspondence Analysis (CA)\nCorrespondence Analysis (CA) is a statistical technique primarily designed for exploring the association between two categorical variables, typically presented in a contingency table (a cross-tabulation). CA’s strength lies in its ability to graphically represent the relationships between the row and column categories in a common low-dimensional space, usually two dimensions, where their proximity visually reflects their association.\nFor example, if we have a contingency table cross-tabulating “Education Level” (an ordinal variable) with “Preferred News Source” (a nominal or ordinal variable), CA can visually reveal if specific education levels are strongly associated with particular news sources. Points representing categories that are close together on the CA map indicate a strong tendency to co-occur.\nWhile CA is most commonly associated with nominal data, it can also be applied to ordinal variables. However, it’s important to note that in its standard form, CA does not explicitly leverage or enforce the order information of ordinal variables. Consequently, while CA can be highly useful for detecting general patterns and visualizing broad relationships in the data, it should be applied with caution when the precise ordinal nature and monotonicity of the data are central to the research question. Nevertheless, CA remains a valuable exploratory tool, offering useful insights into co-occurrence structures and showing which categories of different variables are more likely to appear together, particularly in the initial phases of data exploration.\n\n\n4.2.3 Multiple Correspondence Analysis (MCA)\nMultiple Correspondence Analysis (MCA) extends the principles of CA to analyze the relationships among more than two categorical variables simultaneously. This makes MCA particularly well-suited for large survey datasets where respondents provide answers across numerous items, often using Likert-type scales.\nSimilar to CA, MCA represents both the categories of the variables and the individual respondents in a low-dimensional space (e.g., a 2D scatter plot). This visual representation greatly facilitates the detection of latent structures and the visualization of similarities among individuals and variables. For instance, MCA can help identify clusters of individuals who exhibit similar response profiles across multiple survey questions. It can also reveal the underlying dimensions or “themes” that explain the relationships among the survey items themselves.\nA key point to remember about standard MCA is that it traditionally treats all variables as nominal, thereby disregarding the inherent ordered nature of ordinal variables. To mitigate this limitation, several extensions of MCA have been proposed. These include methods like ordinal MCA or nonlinear MCA, which incorporate specific coding and weighting schemes for responses that explicitly account for their ordering. These advanced versions strive to preserve the monotonicity of ordinal scales and to produce more meaningful and interpretable dimensions that are faithful to the data’s original structure.\nIn practice, MCA is widely employed in the social sciences, marketing research, and psychology due to its powerful ability to simplify complex datasets into interpretable patterns. It is an invaluable tool for dimensionality reduction and visualization, often preceding or complementing other analyses by providing a simplified, meaningful representation of the data."
  },
  {
    "objectID": "courses/ordinal_data/Ordinal_data_an_R.html#clustering-with-ordinal-and-mixed-data",
    "href": "courses/ordinal_data/Ordinal_data_an_R.html#clustering-with-ordinal-and-mixed-data",
    "title": "Ordinal Data Analysis in R",
    "section": "4.3 Clustering with Ordinal and Mixed Data",
    "text": "4.3 Clustering with Ordinal and Mixed Data\nClustering is a milestone of unsupervised learning since it allows to discover natural groupings or segments within a dataset. However, as anticipated, standard clustering algorithms like K-means are poorly suited for ordinal or mixed-type data due to their reliance on distance metrics that assume continuous, interval-scaled variables.\n\n4.3.1 Distance-Based Clustering\nTo perform clustering on ordinal data, the critical first step is to define distance measures that genuinely respect the ordinal nature of the variables. One of the most widely adopted and versatile approaches is to use Gower’s distance (or Gower’s coefficient of similarity). Gower’s distance can handle mixed data types, including nominal, ordinal, and continuous variables, within a single dissimilarity metric. For ordinal variables specifically, Gower’s metric calculates distances in a way that preserves the order and accounts for the number of categories separating two responses, without assuming equal intervals.\nAnother option involves using other ordinal-specific dissimilarity measures, such as those based on ranks, monotonic transformations, or agreement/disagreement counts. These measures are designed to avoid the pitfalls of assuming equal spacing between categories.\nOnce an appropriate dissimilarity matrix is computed (e.g., using Gower’s distance), clustering algorithms that do not rely on Euclidean geometry or mean centroids can be effectively employed.\nSome examples are:\n\nHierarchical clustering: Builds a hierarchy of clusters from individual data points, based on their dissimilarities.\nPartitioning Around Medoids (PAM): A robust alternative to K-means that uses actual data points (medoids) as cluster centers, minimizing the sum of dissimilarities rather than squared Euclidean distances.\n\n\n\n4.3.2 Model-Based Clustering\nBeyond distance-based approaches, model-based clustering offers a probabilistic framework for identifying clusters. Instead of relying only on distance metrics, model-based clustering assumes that the data points within each cluster are generated from a specific probability distribution, and the entire dataset is a mixture of these distributions. The goal is to estimate the parameters of these component distributions and assign observations to the clusters that they are most likely to belong to.\nFor ordinal data, model-based clustering often employs specialized distributions that are designed for ordered categories. For example, some approaches might use latent class models, where each latent class represents a cluster, and within each cluster, the ordinal responses follow a specific discrete probability distribution (e.g., a multinomial distribution or a specific ordinal-friendly distribution like the Binary Ordinal Search (BOS) model). This allows for a more rigorous statistical inference on the number of clusters and the characteristics of each cluster.\nAn important advantage of model-based clustering is its ability to:\n\nProvide a statistical basis for clustering: This allows for formal tests and information criteria (like BIC or AIC) to select the optimal number of clusters.\nHandle uncertainty in cluster membership: Instead of hard assignments, observations are often assigned to clusters probabilistically.\n\nThis approach allows researchers to segment individuals into genuinely homogeneous groups, revealing hidden patterns in the data that might not be evident through classical analyses while also providing a framework for statistical inference about the clustering solution.\n\n\nFinal notes\n\nThis course has offered a comprehensive introduction to the analysis of ordinal data, a type of data that lies between nominal and continuous variables and is frequently encountered in social sciences, psychology, marketing, and many applied domains.\nWe have covered the following main topics:\n\nDefinition and characteristics of ordinal data, and the importance of treating them appropriately rather than assuming interval or continuous scale properties.\nSurvey and scale design.\nStatistical models for ordinal data, such as the Proportional Odds Model and CUB models..\nMultivariate methods for ordinal and mixed data, including nonlinear PCA, Correspondence Analysis, Multiple Correspondence Analysis, and clustering approaches based on ordinal or mixed-type dissimilarities.\n\nThroughout the course, a common thread has been the need to respect the nature of ordinal data, choosing analytical methods that reflect their structure and meaning. Applying models designed for other types of data may lead to invalid interpretations, while appropriate methods can provide valuable insights into individual preferences, attitudes, and behaviors."
  }
]